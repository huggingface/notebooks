{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï»¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion ëª¨ë¸ì„ í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unconditional ì´ë¯¸ì§€ ìƒì„±ì€ í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ê³¼ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” diffusion ëª¨ë¸ì—ì„œ ì¸ê¸° ìˆëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ, ê°€ì¥ ì¢‹ì€ ê²°ê³¼ëŠ” íŠ¹ì • ë°ì´í„°ì…‹ì— ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ê²ƒìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ [í—ˆë¸Œ](https://huggingface.co/search/full-text?q=unconditional-image-generation&type=model)ì—ì„œ ì´ëŸ¬í•œ ë§ì€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆì§€ë§Œ, ë§Œì•½ ë§ˆìŒì— ë“œëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´, ì–¸ì œë“ ì§€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì€ ë‚˜ë§Œì˜ ğŸ¦‹ ë‚˜ë¹„ ğŸ¦‹ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ [Smithsonian Butterflies](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) ë°ì´í„°ì…‹ì˜ í•˜ìœ„ ì§‘í•©ì—ì„œ `UNet2DModel` ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ê°€ë¥´ì³ì¤„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "> [!TIP]\n",
    "> ğŸ’¡ ì´ í•™ìŠµ íŠœí† ë¦¬ì–¼ì€ [Training with ğŸ§¨ Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb) ë…¸íŠ¸ë¶ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. Diffusion ëª¨ë¸ì˜ ì‘ë™ ë°©ì‹ ë° ìì„¸í•œ ë‚´ìš©ì€ ë…¸íŠ¸ë¶ì„ í™•ì¸í•˜ì„¸ìš”!\n",
    "\n",
    "ì‹œì‘ ì „ì—, ğŸ¤— Datasetsì„ ë¶ˆëŸ¬ì˜¤ê³  ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë°ì´í„°ì…‹ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ ë‹¤ìˆ˜ GPUì—ì„œ í•™ìŠµì„ ê°„ì†Œí™”í•˜ê¸° ìœ„í•´ ğŸ¤— Accelerate ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ê·¸ í›„ í•™ìŠµ ë©”íŠ¸ë¦­ì„ ì‹œê°í™”í•˜ê¸° ìœ„í•´ [TensorBoard](https://www.tensorflow.org/tensorboard)ë¥¼ ë˜í•œ ì„¤ì¹˜í•˜ì„¸ìš”. (ë˜í•œ í•™ìŠµ ì¶”ì ì„ ìœ„í•´ [Weights & Biases](https://docs.wandb.ai/)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n",
    "\n",
    "```bash\n",
    "!pip install diffusers[training]\n",
    "```\n",
    "\n",
    "ì»¤ë®¤ë‹ˆí‹°ì— ëª¨ë¸ì„ ê³µìœ í•  ê²ƒì„ ê¶Œì¥í•˜ë©°, ì´ë¥¼ ìœ„í•´ì„œ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸ì„ í•´ì•¼ í•©ë‹ˆë‹¤. (ê³„ì •ì´ ì—†ë‹¤ë©´ [ì—¬ê¸°](https://hf.co/join)ì—ì„œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.) ë…¸íŠ¸ë¶ì—ì„œ ë¡œê·¸ì¸í•  ìˆ˜ ìˆìœ¼ë©° ë©”ì‹œì§€ê°€ í‘œì‹œë˜ë©´ í† í°ì„ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë˜ëŠ” í„°ë¯¸ë„ë¡œ ë¡œê·¸ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "```bash\n",
    "hf auth login\n",
    "```\n",
    "\n",
    "ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ê°€ ìƒë‹¹íˆ í¬ê¸° ë•Œë¬¸ì— [Git-LFS](https://git-lfs.com/)ì—ì„œ ëŒ€ìš©ëŸ‰ íŒŒì¼ì˜ ë²„ì „ ê´€ë¦¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "```bash\n",
    "!sudo apt -qq install git-lfs\n",
    "!git config --global credential.helper store\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í¸ì˜ë¥¼ ìœ„í•´ í•™ìŠµ íŒŒë¼ë¯¸í„°ë“¤ì„ í¬í•¨í•œ `TrainingConfig` í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ììœ ë¡­ê²Œ ì¡°ì • ê°€ëŠ¥):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # ìƒì„±ë˜ëŠ” ì´ë¯¸ì§€ í•´ìƒë„\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # í‰ê°€ ë™ì•ˆì— ìƒ˜í”Œë§í•  ì´ë¯¸ì§€ ìˆ˜\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no`ëŠ” float32, ìë™ í˜¼í•© ì •ë°€ë„ë¥¼ ìœ„í•œ `fp16`\n",
    "    output_dir = \"ddpm-butterflies-128\"  # ë¡œì»¬ ë° HF Hubì— ì €ì¥ë˜ëŠ” ëª¨ë¸ëª…\n",
    "\n",
    "    push_to_hub = True  # ì €ì¥ëœ ëª¨ë¸ì„ HF Hubì— ì—…ë¡œë“œí• ì§€ ì—¬ë¶€\n",
    "    hub_private_repo = None\n",
    "    overwrite_output_dir = True  # ë…¸íŠ¸ë¶ì„ ë‹¤ì‹œ ì‹¤í–‰í•  ë•Œ ì´ì „ ëª¨ë¸ì— ë®ì–´ì”Œìš¸ì§€\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ [Smithsonian Butterflies](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "config.dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
    "dataset = load_dataset(config.dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡[HugGan Community Event](https://huggingface.co/huggan) ì—ì„œ ì¶”ê°€ì˜ ë°ì´í„°ì…‹ì„ ì°¾ê±°ë‚˜ ë¡œì»¬ì˜ [`ImageFolder`](https://huggingface.co/docs/datasets/image_dataset#imagefolder)ë¥¼ ë§Œë“¦ìœ¼ë¡œì¨ ë‚˜ë§Œì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. HugGan Community Event ì— ê°€ì ¸ì˜¨ ë°ì´í„°ì…‹ì˜ ê²½ìš° ë¦¬í¬ì§€í† ë¦¬ì˜ idë¡œ `config.dataset_name` ì„ ì„¤ì •í•˜ê³ , ë‚˜ë§Œì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° `imagefolder` ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ¤— Datasetsì€ `Image` ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë””ì½”ë”©í•˜ê³  [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html)ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì´ë¥¼ ì‹œê°í™” í•´ë³´ë©´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, image in enumerate(dataset[:4][\"image\"]):\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/butterflies_ds.png)\n",
    "\n",
    "ì´ë¯¸ì§€ëŠ” ëª¨ë‘ ë‹¤ë¥¸ ì‚¬ì´ì¦ˆì´ê¸° ë•Œë¬¸ì—, ìš°ì„  ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤:\n",
    "\n",
    "-   `Resize` ëŠ” `config.image_size` ì— ì •ì˜ëœ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë¡œ ë³€ê²½í•©ë‹ˆë‹¤.\n",
    "-   `RandomHorizontalFlip` ì€ ëœë¤ì ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë¯¸ëŸ¬ë§í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë³´ê°•í•©ë‹ˆë‹¤.\n",
    "-   `Normalize` ëŠ” ëª¨ë¸ì´ ì˜ˆìƒí•˜ëŠ” [-1, 1] ë²”ìœ„ë¡œ í”½ì…€ ê°’ì„ ì¬ì¡°ì • í•˜ëŠ”ë° ì¤‘ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµ ë„ì¤‘ì— `preprocess` í•¨ìˆ˜ë¥¼ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasetsì˜ `set_transform` ë°©ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ ì¡°ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ì‹œê°í™”í•´ë³´ì„¸ìš”. ì´ì œ [DataLoader](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader)ì— ë°ì´í„°ì…‹ì„ í¬í•¨í•´ í•™ìŠµí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet2DModel ìƒì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§¨ Diffusersì— ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ë“¤ì€ ëª¨ë¸ í´ë˜ìŠ¤ì—ì„œ ì›í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `UNet2DModel`ë¥¼ ìƒì„±í•˜ë ¤ë©´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # íƒ€ê²Ÿ ì´ë¯¸ì§€ í•´ìƒë„\n",
    "    in_channels=3,  # ì…ë ¥ ì±„ë„ ìˆ˜, RGB ì´ë¯¸ì§€ì—ì„œ 3\n",
    "    out_channels=3,  # ì¶œë ¥ ì±„ë„ ìˆ˜\n",
    "    layers_per_block=2,  # UNet ë¸”ëŸ­ë‹¹ ëª‡ ê°œì˜ ResNet ë ˆì´ì–´ê°€ ì‚¬ìš©ë˜ëŠ”ì§€\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # ê° UNet ë¸”ëŸ­ì„ ìœ„í•œ ì¶œë ¥ ì±„ë„ ìˆ˜\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # ì¼ë°˜ì ì¸ ResNet ë‹¤ìš´ìƒ˜í”Œë§ ë¸”ëŸ­\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # spatial self-attentionì´ í¬í•¨ëœ ì¼ë°˜ì ì¸ ResNet ë‹¤ìš´ìƒ˜í”Œë§ ë¸”ëŸ­\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # ì¼ë°˜ì ì¸ ResNet ì—…ìƒ˜í”Œë§ ë¸”ëŸ­\n",
    "        \"AttnUpBlock2D\",  # spatial self-attentionì´ í¬í•¨ëœ ì¼ë°˜ì ì¸ ResNet ì—…ìƒ˜í”Œë§ ë¸”ëŸ­\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìƒ˜í”Œì˜ ì´ë¯¸ì§€ í¬ê¸°ì™€ ëª¨ë¸ ì¶œë ¥ í¬ê¸°ê°€ ë§ëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê¸° ìœ„í•œ ì¢‹ì€ ì•„ì´ë””ì–´ê°€ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Input shape: torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
    "print(\"Input shape:\", sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output shape: torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í›Œë¥­í•´ìš”! ë‹¤ìŒ, ì´ë¯¸ì§€ì— ì•½ê°„ì˜ ë…¸ì´ì¦ˆë¥¼ ë”í•˜ê¸° ìœ„í•´ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ëª¨ë¸ì„ í•™ìŠµ ë˜ëŠ” ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ”ì§€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤. ì¶”ë¡ ì‹œì—, ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ë…¸ì´ì¦ˆë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í•™ìŠµì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” diffusion ê³¼ì •ì—ì„œì˜ íŠ¹ì • í¬ì¸íŠ¸ë¡œë¶€í„° ëª¨ë¸ì˜ ì¶œë ¥ ë˜ëŠ” ìƒ˜í”Œì„ ê°€ì ¸ì™€ *ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„* ê³¼ *ì—…ë°ì´íŠ¸ ê·œì¹™*ì— ë”°ë¼ ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "`DDPMScheduler`ë¥¼ ë³´ë©´ ì´ì „ìœ¼ë¡œë¶€í„° `sample_image`ì— ëœë¤í•œ ë…¸ì´ì¦ˆë¥¼ ë”í•˜ëŠ” `add_noise` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "noise = torch.randn(sample_image.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/noisy_butterfly.png)\n",
    "\n",
    "ëª¨ë¸ì˜ í•™ìŠµ ëª©ì ì€ ì´ë¯¸ì§€ì— ë”í•´ì§„ ë…¸ì´ì¦ˆë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ ì†ì‹¤ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "noise_pred = model(noisy_image, timesteps).sample\n",
    "loss = F.mse_loss(noise_pred, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì§€ê¸ˆê¹Œì§€, ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•˜ê¸° ìœ„í•´ ë§ì€ ë¶€ë¶„ì„ ê°–ì¶”ì—ˆìœ¼ë©° ì´ì œ ë‚¨ì€ ê²ƒì€ ëª¨ë“  ê²ƒì„ ì¡°í•©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ìš°ì„  ì˜µí‹°ë§ˆì´ì €(optimizer)ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬(learning rate scheduler)ê°€ í•„ìš”í•  ê²ƒì…ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ í›„, ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. í‰ê°€ë¥¼ ìœ„í•´, `DDPMPipeline`ì„ ì‚¬ìš©í•´ ë°°ì¹˜ì˜ ì´ë¯¸ì§€ ìƒ˜í”Œë“¤ì„ ìƒì„±í•˜ê³  ê·¸ë¦¬ë“œ í˜•íƒœë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # ëœë¤í•œ ë…¸ì´ì¦ˆë¡œ ë¶€í„° ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.(ì´ëŠ” ì—­ì „íŒŒ diffusion ê³¼ì •ì…ë‹ˆë‹¤.)\n",
    "    # ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì¶œë ¥ í˜•íƒœëŠ” `List[PIL.Image]` ì…ë‹ˆë‹¤.\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    # ì´ë¯¸ì§€ë“¤ì„ ê·¸ë¦¬ë“œë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # ì´ë¯¸ì§€ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoardì— ë¡œê¹…, ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë° í˜¼í•© ì •ë°€ë„ í•™ìŠµì„ ì‰½ê²Œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ğŸ¤— Accelerateë¥¼ í•™ìŠµ ë£¨í”„ì— í•¨ê»˜ ì•ì„œ ë§í•œ ëª¨ë“  êµ¬ì„± ì •ë³´ë“¤ì„ ë¬¶ì–´ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í—ˆë¸Œì— ëª¨ë¸ì„ ì—…ë¡œë“œ í•˜ê¸° ìœ„í•´ ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ ë° ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ê³  í—ˆë¸Œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ğŸ’¡ì•„ë˜ì˜ í•™ìŠµ ë£¨í”„ëŠ” ì–´ë µê³  ê¸¸ì–´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ë‚˜ì¤‘ì— í•œ ì¤„ì˜ ì½”ë“œë¡œ í•™ìŠµì„ í•œë‹¤ë©´ ê·¸ë§Œí•œ ê°€ì¹˜ê°€ ìˆì„ ê²ƒì…ë‹ˆë‹¤! ë§Œì•½ ê¸°ë‹¤ë¦¬ì§€ ëª»í•˜ê³  ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ì‹¶ë‹¤ë©´, ì•„ë˜ ì½”ë“œë¥¼ ììœ ë¡­ê²Œ ë¶™ì—¬ë„£ê³  ì‘ë™ì‹œí‚¤ë©´ ë©ë‹ˆë‹¤. ğŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        if config.push_to_hub:\n",
    "            repo_id = create_repo(\n",
    "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
    "            ).repo_id\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # ëª¨ë“  ê²ƒì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "    # ê¸°ì–µí•´ì•¼ í•  íŠ¹ì •í•œ ìˆœì„œëŠ” ì—†ìœ¼ë©° ì¤€ë¹„í•œ ë°©ë²•ì— ì œê³µí•œ ê²ƒê³¼ ë™ì¼í•œ ìˆœì„œë¡œ ê°ì²´ì˜ ì••ì¶•ì„ í’€ë©´ ë©ë‹ˆë‹¤.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # ì´ì œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[\"images\"]\n",
    "            # ì´ë¯¸ì§€ì— ë”í•  ë…¸ì´ì¦ˆë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # ê° ì´ë¯¸ì§€ë¥¼ ìœ„í•œ ëœë¤í•œ íƒ€ì„ìŠ¤í…(timestep)ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # ê° íƒ€ì„ìŠ¤í…ì˜ ë…¸ì´ì¦ˆ í¬ê¸°ì— ë”°ë¼ ê¹¨ë—í•œ ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "            # (ì´ëŠ” foward diffusion ê³¼ì •ì…ë‹ˆë‹¤.)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # ë…¸ì´ì¦ˆë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # ê° ì—í¬í¬ê°€ ëë‚œ í›„ evaluate()ì™€ ëª‡ ê°€ì§€ ë°ëª¨ ì´ë¯¸ì§€ë¥¼ ì„ íƒì ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ê³  ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    upload_folder(\n",
    "                        repo_id=repo_id,\n",
    "                        folder_path=config.output_dir,\n",
    "                        commit_message=f\"Epoch {epoch}\",\n",
    "                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "                    )\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "íœ´, ì½”ë“œê°€ ê½¤ ë§ì•˜ë„¤ìš”! í•˜ì§€ë§Œ ğŸ¤— Accelerateì˜ `notebook_launcher` í•¨ìˆ˜ì™€ í•™ìŠµì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ì— í•™ìŠµ ë£¨í”„, ëª¨ë“  í•™ìŠµ ì¸ìˆ˜, í•™ìŠµì— ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜(ì‚¬ìš© ê°€ëŠ¥í•œ GPUì˜ ìˆ˜ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŒ)ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•œë²ˆ í•™ìŠµì´ ì™„ë£Œë˜ë©´, diffusion ëª¨ë¸ë¡œ ìƒì„±ëœ ìµœì¢… ğŸ¦‹ì´ë¯¸ì§€ğŸ¦‹ë¥¼ í™•ì¸í•´ë³´ê¸¸ ë°”ëë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "sample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
    "Image.open(sample_images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/butterflies_final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë‹¤ìŒ ë‹¨ê³„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unconditional ì´ë¯¸ì§€ ìƒì„±ì€ í•™ìŠµë  ìˆ˜ ìˆëŠ” ì‘ì—… ì¤‘ í•˜ë‚˜ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì‘ì—…ê³¼ í•™ìŠµ ë°©ë²•ì€ [ğŸ§¨ Diffusers í•™ìŠµ ì˜ˆì‹œ](https://huggingface.co/docs/diffusers/main/ko/tutorials/../training/overview) í˜ì´ì§€ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì˜ˆì‹œì…ë‹ˆë‹¤:\n",
    "\n",
    "-   [Textual Inversion](https://huggingface.co/docs/diffusers/main/ko/tutorials/../training/text_inversion), íŠ¹ì • ì‹œê°ì  ê°œë…ì„ í•™ìŠµì‹œì¼œ ìƒì„±ëœ ì´ë¯¸ì§€ì— í†µí•©ì‹œí‚¤ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\n",
    "-   [DreamBooth](https://huggingface.co/docs/diffusers/main/ko/tutorials/../training/dreambooth), ì£¼ì œì— ëŒ€í•œ ëª‡ ê°€ì§€ ì…ë ¥ ì´ë¯¸ì§€ë“¤ì´ ì£¼ì–´ì§€ë©´ ì£¼ì œì— ëŒ€í•œ ê°œì¸í™”ëœ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "-   [Guide](https://huggingface.co/docs/diffusers/main/ko/tutorials/../training/text2image) ë°ì´í„°ì…‹ì— Stable Diffusion ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "-   [Guide](https://huggingface.co/docs/diffusers/main/ko/tutorials/../training/lora)  LoRAë¥¼ ì‚¬ìš©í•´ ë§¤ìš° í° ëª¨ë¸ì„ ë¹ ë¥´ê²Œ íŒŒì¸íŠœë‹í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
