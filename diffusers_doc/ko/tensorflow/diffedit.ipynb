{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffEdit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ í¸ì§‘ì„ í•˜ë ¤ë©´ ì¼ë°˜ì ìœ¼ë¡œ í¸ì§‘í•  ì˜ì—­ì˜ ë§ˆìŠ¤í¬ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. DiffEditëŠ” í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆìŠ¤í¬ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ë¯€ë¡œ ì´ë¯¸ì§€ í¸ì§‘ ì†Œí”„íŠ¸ì›¨ì–´ ì—†ì´ë„ ë§ˆìŠ¤í¬ë¥¼ ë§Œë“¤ê¸°ê°€ ì „ë°˜ì ìœ¼ë¡œ ë” ì‰¬ì›Œì§‘ë‹ˆë‹¤. DiffEdit ì•Œê³ ë¦¬ì¦˜ì€ ì„¸ ë‹¨ê³„ë¡œ ì‘ë™í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. Diffusion ëª¨ë¸ì´ ì¼ë¶€ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° í…ìŠ¤íŠ¸ë¥¼ ì¡°ê±´ë¶€ë¡œ ì´ë¯¸ì§€ì˜ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ ì´ë¯¸ì§€ì˜ ì—¬ëŸ¬ ì˜ì—­ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ë…¸ì´ì¦ˆ ì¶”ì •ì¹˜ë¥¼ ìƒì„±í•˜ê³ , ê·¸ ì°¨ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ë„ë¡ ì´ë¯¸ì§€ì˜ ì–´ëŠ ì˜ì—­ì„ ë³€ê²½í•´ì•¼ í•˜ëŠ”ì§€ ì‹ë³„í•˜ê¸° ìœ„í•œ ë§ˆìŠ¤í¬ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.\n",
    "2. ì…ë ¥ ì´ë¯¸ì§€ê°€ DDIMì„ ì‚¬ìš©í•˜ì—¬ ì ì¬ ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©ë©ë‹ˆë‹¤.\n",
    "3. ë§ˆìŠ¤í¬ ì™¸ë¶€ì˜ í”½ì…€ì´ ì…ë ¥ ì´ë¯¸ì§€ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€ë˜ë„ë¡ ë§ˆìŠ¤í¬ë¥¼ ê°€ì´ë“œë¡œ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ì— ì¡°ê±´ì´ ì§€ì •ëœ diffusion ëª¨ë¸ë¡œ latentsë¥¼ ë””ì½”ë”©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê°€ì´ë“œì—ì„œëŠ” ë§ˆìŠ¤í¬ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë§Œë“¤ì§€ ì•Šê³  DiffEditë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í¸ì§‘í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colabì—ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê¸° ìœ„í•´ ì£¼ì„ì„ ì œì™¸í•˜ì„¸ìš”\n",
    "#!pip install -q diffusers transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StableDiffusionDiffEditPipeline`ì—ëŠ” ì´ë¯¸ì§€ ë§ˆìŠ¤í¬ì™€ ë¶€ë¶„ì ìœ¼ë¡œ ë°˜ì „ëœ latents ì§‘í•©ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ë§ˆìŠ¤í¬ëŠ” `generate_mask()` í•¨ìˆ˜ì—ì„œ ìƒì„±ë˜ë©°, ë‘ ê°œì˜ íŒŒë¼ë¯¸í„°ì¸ `source_prompt`ì™€ `target_prompt`ê°€ í¬í•¨ë©ë‹ˆë‹¤. ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì„ í¸ì§‘í• ì§€ ê²°ì •í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, *ê³¼ì¼* í•œ ê·¸ë¦‡ì„ *ë°°* í•œ ê·¸ë¦‡ìœ¼ë¡œ ë³€ê²½í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_prompt = \"a bowl of fruits\"\n",
    "target_prompt = \"a bowl of pears\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¶€ë¶„ì ìœ¼ë¡œ ë°˜ì „ëœ latentsëŠ” `invert()` í•¨ìˆ˜ì—ì„œ ìƒì„±ë˜ë©°, ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” `prompt` ë˜ëŠ” *ìº¡ì…˜*ì„ í¬í•¨í•˜ëŠ” ê²ƒì´ inverse latent sampling í”„ë¡œì„¸ìŠ¤ë¥¼ ê°€ì´ë“œí•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ìº¡ì…˜ì€ ì¢…ì¢… `source_prompt`ê°€ ë  ìˆ˜ ìˆì§€ë§Œ, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ììœ ë¡­ê²Œ ì‹¤í—˜í•´ ë³´ì„¸ìš”!\n",
    "\n",
    "íŒŒì´í”„ë¼ì¸, ìŠ¤ì¼€ì¤„ëŸ¬, ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ìµœì í™”ë¥¼ í™œì„±í™”í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline\n",
    "\n",
    "pipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_vae_slicing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìˆ˜ì •í•˜ê¸° ìœ„í•œ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n",
    "raw_image = load_image(img_url).resize((768, 768))\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ `generate_mask()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì—ì„œ í¸ì§‘í•  ë‚´ìš©ì„ ì§€ì •í•˜ê¸° ìœ„í•´ `source_prompt`ì™€ `target_prompt`ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "source_prompt = \"a bowl of fruits\"\n",
    "target_prompt = \"a basket of pears\"\n",
    "mask_image = pipeline.generate_mask(\n",
    "    image=raw_image,\n",
    "    source_prompt=source_prompt,\n",
    "    target_prompt=target_prompt,\n",
    ")\n",
    "Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\").resize((768, 768))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ, ë°˜ì „ëœ latentsë¥¼ ìƒì„±í•˜ê³  ì´ë¯¸ì§€ë¥¼ ë¬˜ì‚¬í•˜ëŠ” ìº¡ì…˜ì— ì „ë‹¬í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_latents = pipeline.invert(prompt=source_prompt, image=raw_image).latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ë¯¸ì§€ ë§ˆìŠ¤í¬ì™€ ë°˜ì „ëœ latentsë¥¼ íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬í•©ë‹ˆë‹¤. `target_prompt`ëŠ” ì´ì œ `prompt`ê°€ ë˜ë©°, `source_prompt`ëŠ” `negative_prompt`ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = pipeline(\n",
    "    prompt=target_prompt,\n",
    "    mask_image=mask_image,\n",
    "    image_latents=inv_latents,\n",
    "    negative_prompt=source_prompt,\n",
    ").images[0]\n",
    "mask_image = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\").resize((768, 768))\n",
    "make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex gap-4\">\n",
    "  <div>\n",
    "    <img class=\"rounded-xl\" src=\"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"/>\n",
    "    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">original image</figcaption>\n",
    "  </div>\n",
    "  <div>\n",
    "    <img class=\"rounded-xl\" src=\"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/blob/main/assets/target.png?raw=true\"/>\n",
    "    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">edited image</figcaption>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sourceì™€ target ì„ë² ë”© ìƒì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sourceì™€ target ì„ë² ë”©ì€ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ëŒ€ì‹  [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Flan-T5 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸í•  sourceì™€ target í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì´ˆê¸° í…ìŠ¤íŠ¸ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_concept = \"bowl\"\n",
    "target_concept = \"basket\"\n",
    "\n",
    "source_text = f\"Provide a caption for images containing a {source_concept}. \"\n",
    "\"The captions should be in English and should be no longer than 150 characters.\"\n",
    "\n",
    "target_text = f\"Provide a caption for images containing a {target_concept}. \"\n",
    "\"The captions should be in English and should be no longer than 150 characters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ, í”„ë¡¬í”„íŠ¸ë“¤ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_prompts(input_prompt):\n",
    "    input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=0.8, num_return_sequences=16, do_sample=True, max_new_tokens=128, top_k=10\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "source_prompts = generate_prompts(source_text)\n",
    "target_prompts = generate_prompts(target_text)\n",
    "print(source_prompts)\n",
    "print(target_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!TIP]\n",
    "> ë‹¤ì–‘í•œ í’ˆì§ˆì˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì „ëµì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ìƒì„± ì „ëµ](https://huggingface.co/docs/transformers/main/en/generation_strategies) ê°€ì´ë“œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
    "\n",
    "í…ìŠ¤íŠ¸ ì¸ì½”ë”©ì„ ìœ„í•´ `StableDiffusionDiffEditPipeline`ì—ì„œ ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ì¸ì½”ë” ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionDiffEditPipeline\n",
    "\n",
    "pipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_vae_slicing()\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_prompts(sentences, tokenizer, text_encoder, device=\"cuda\"):\n",
    "    embeddings = []\n",
    "    for sent in sentences:\n",
    "        text_inputs = tokenizer(\n",
    "            sent,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=None)[0]\n",
    "        embeddings.append(prompt_embeds)\n",
    "    return torch.concatenate(embeddings, dim=0).mean(dim=0).unsqueeze(0)\n",
    "\n",
    "source_embeds = embed_prompts(source_prompts, pipeline.tokenizer, pipeline.text_encoder)\n",
    "target_embeds = embed_prompts(target_prompts, pipeline.tokenizer, pipeline.text_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ, ì„ë² ë”©ì„ `generate_mask()` ë° `invert()` í•¨ìˆ˜ì™€ íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:\n",
    "\n",
    "```diff\n",
    "  from diffusers import DDIMInverseScheduler, DDIMScheduler\n",
    "  from diffusers.utils import load_image, make_image_grid\n",
    "  from PIL import Image\n",
    "\n",
    "  pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "  pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "  img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n",
    "  raw_image = load_image(img_url).resize((768, 768))\n",
    "\n",
    "  mask_image = pipeline.generate_mask(\n",
    "      image=raw_image,\n",
    "-     source_prompt=source_prompt,\n",
    "-     target_prompt=target_prompt,\n",
    "+     source_prompt_embeds=source_embeds,\n",
    "+     target_prompt_embeds=target_embeds,\n",
    "  )\n",
    "\n",
    "  inv_latents = pipeline.invert(\n",
    "-     prompt=source_prompt,\n",
    "+     prompt_embeds=source_embeds,\n",
    "      image=raw_image,\n",
    "  ).latents\n",
    "\n",
    "  output_image = pipeline(\n",
    "      mask_image=mask_image,\n",
    "      image_latents=inv_latents,\n",
    "-     prompt=target_prompt,\n",
    "-     negative_prompt=source_prompt,\n",
    "+     prompt_embeds=target_embeds,\n",
    "+     negative_prompt_embeds=source_embeds,\n",
    "  ).images[0]\n",
    "  mask_image = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\")\n",
    "  make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°˜ì „ì„ ìœ„í•œ ìº¡ì…˜ ìƒì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`source_prompt`ë¥¼ ìº¡ì…˜ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë¶€ë¶„ì ìœ¼ë¡œ ë°˜ì „ëœ latentsë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, [BLIP](https://huggingface.co/docs/transformers/model_doc/blip) ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìº¡ì…˜ì„ ìë™ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ BLIP ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ ìº¡ì…˜ì„ ìƒì„±í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption(images, caption_generator, caption_processor):\n",
    "    text = \"a photograph of\"\n",
    "\n",
    "    inputs = caption_processor(images, text, return_tensors=\"pt\").to(device=\"cuda\", dtype=caption_generator.dtype)\n",
    "    caption_generator.to(\"cuda\")\n",
    "    outputs = caption_generator.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # ìº¡ì…˜ generator ì˜¤í”„ë¡œë“œ\n",
    "    caption_generator.to(\"cpu\")\n",
    "\n",
    "    caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  `generate_caption` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ì´ë¯¸ì§€ì— ëŒ€í•œ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "\n",
    "img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n",
    "raw_image = load_image(img_url).resize((768, 768))\n",
    "caption = generate_caption(raw_image, model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <figure>\n",
    "        <img class=\"rounded-xl\" src=\"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"/>\n",
    "        <figcaption class=\"text-center\">generated caption: \"a photograph of a bowl of fruit on a table\"</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "ì´ì œ ìº¡ì…˜ì„ `invert()` í•¨ìˆ˜ì— ë†“ì•„ ë¶€ë¶„ì ìœ¼ë¡œ ë°˜ì „ëœ latentsë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
