{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Consistency Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Latent Consistency Models (LCMs)](https://hf.co/papers/2310.04378) enable fast high-quality image generation by directly predicting the reverse diffusion process in the latent rather than pixel space. In other words, LCMs try to predict the noiseless image from the noisy image in contrast to typical diffusion models that iteratively remove noise from the noisy image. By avoiding the iterative sampling process, LCMs are able to generate high-quality images in 2-4 steps instead of 20-30 steps.\n",
    "\n",
    "LCMs are distilled from pretrained models which requires ~32 hours of A100 compute. To speed this up, [LCM-LoRAs](https://hf.co/papers/2311.05556) train a [LoRA adapter](https://huggingface.co/docs/peft/conceptual_guides/adapter#low-rank-adaptation-lora) which have much fewer parameters to train compared to the full model. The LCM-LoRA can be plugged into a diffusion model once it has been trained.\n",
    "\n",
    "This guide will show you how to use LCMs and LCM-LoRAs for fast inference on tasks and how to use them with other adapters like ControlNet or T2I-Adapter.\n",
    "\n",
    "> [!TIP]\n",
    "> LCMs and LCM-LoRAs are available for Stable Diffusion v1.5, Stable Diffusion XL, and the SSD-1B model. You can find their checkpoints on the [Latent Consistency](https://hf.co/collections/latent-consistency/latent-consistency-models-weights-654ce61a95edd6dffccef6a8) Collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hfoptions id=\"lcm-text2img\">\n",
    "<hfoption id=\"LCM\">\n",
    "\n",
    "To use LCMs, you need to load the LCM checkpoint for your supported model into [UNet2DConditionModel](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel) and replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler). Then you can use the pipeline as usual, and pass a text prompt to generate an image in just 4 steps.\n",
    "\n",
    "A couple of notes to keep in mind when using LCMs are:\n",
    "\n",
    "* Typically, batch size is doubled inside the pipeline for classifier-free guidance. But LCM applies guidance with guidance embeddings and doesn't need to double the batch size, which leads to faster inference. The downside is that negative prompts don't work with LCM because they don't have any effect on the denoising process.\n",
    "* The ideal range for `guidance_scale` is [3., 13.] because that is what the UNet was trained with. However, disabling `guidance_scale` with a value of 1.0 is also effective in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\n",
    "import torch\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"latent-consistency/lcm-sdxl\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=8.0\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_full_sdxl_t2i.png\"/>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "<hfoption id=\"LCM-LoRA\">\n",
    "\n",
    "To use LCM-LoRAs, you need to replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler) and load the LCM-LoRA weights with the [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights) method. Then you can use the pipeline as usual, and pass a text prompt to generate an image in just 4 steps.\n",
    "\n",
    "A couple of notes to keep in mind when using LCM-LoRAs are:\n",
    "\n",
    "* Typically, batch size is doubled inside the pipeline for classifier-free guidance. But LCM applies guidance with guidance embeddings and doesn't need to double the batch size, which leads to faster inference. The downside is that negative prompts don't work with LCM because they don't have any effect on the denoising process.\n",
    "* You could use guidance with LCM-LoRAs, but it is very sensitive to high `guidance_scale` values and can lead to artifacts in the generated image. The best values we've found are between [1.0, 2.0].\n",
    "* Replace [stabilityai/stable-diffusion-xl-base-1.0](https://hf.co/stabilityai/stable-diffusion-xl-base-1.0) with any finetuned model. For example, try using the [animagine-xl](https://huggingface.co/Linaqruf/animagine-xl) checkpoint to generate anime images with SDXL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, LCMScheduler\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n",
    "\n",
    "prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n",
    "generator = torch.manual_seed(42)\n",
    "image = pipe(\n",
    "    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2i.png\"/>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "</hfoptions>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-to-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hfoptions id=\"lcm-img2img\">\n",
    "<hfoption id=\"LCM\">\n",
    "\n",
    "To use LCMs for image-to-image, you need to load the LCM checkpoint for your supported model into [UNet2DConditionModel](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel) and replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler). Then you can use the pipeline as usual, and pass a text prompt and initial image to generate an image in just 4 steps.\n",
    "\n",
    "> [!TIP]\n",
    "> Experiment with different values for `num_inference_steps`, `strength`, and `guidance_scale` to get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image, UNet2DConditionModel, LCMScheduler\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"SimianLuo/LCM_Dreamshaper_v7\",\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"Lykon/dreamshaper-7\",\n",
    "    unet=unet,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\")\n",
    "prompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=7.5,\n",
    "    strength=0.5,\n",
    "    generator=generator\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex gap-4\">\n",
    "  <div>\n",
    "    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"/>\n",
    "    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">initial image</figcaption>\n",
    "  </div>\n",
    "  <div>\n",
    "    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm-img2img.png\"/>\n",
    "    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">generated image</figcaption>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "<hfoption id=\"LCM-LoRA\">\n",
    "\n",
    "To use LCM-LoRAs for image-to-image, you need to replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler) and load the LCM-LoRA weights with the [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights) method. Then you can use the pipeline as usual, and pass a text prompt and initial image to generate an image in just 4 steps.\n",
    "\n",
    "> [!TIP]\n",
    "> Experiment with different values for `num_inference_steps`, `strength`, and `guidance_scale` to get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image, LCMScheduler\n",
    "from diffusers.utils import make_image_grid, load_image\n",
    "\n",
    "pipe = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"Lykon/dreamshaper-7\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\")\n",
    "prompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=1,\n",
    "    strength=0.6,\n",
    "    generator=generator\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex gap-4\">\n",
    "  <div>\n",
    "    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"/>\n",
    "    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">initial image</figcaption>\n",
    "  </div>\n",
    "  <div>\n",
    "    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm-lora-img2img.png\"/>\n",
    "    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">generated image</figcaption>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "</hfoptions>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LCM-LoRAs for inpainting, you need to replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler) and load the LCM-LoRA weights with the [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights) method. Then you can use the pipeline as usual, and pass a text prompt, initial image, and mask image to generate an image in just 4 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForInpainting, LCMScheduler\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipe = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-inpainting\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\n",
    "mask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n",
    "\n",
    "prompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    image=init_image,\n",
    "    mask_image=mask_image,\n",
    "    generator=generator,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=4,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex gap-4\">\n",
    "  <div>\n",
    "    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\"/>\n",
    "    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">initial image</figcaption>\n",
    "  </div>\n",
    "  <div>\n",
    "    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm-lora-inpaint.png\"/>\n",
    "    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">generated image</figcaption>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCMs are compatible with adapters like LoRA, ControlNet, T2I-Adapter, and AnimateDiff. You can bring the speed of LCMs to these adapters to generate images in a certain style or condition the model on another input like a canny image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LoRA](https://huggingface.co/docs/diffusers/main/en/using-diffusers/../tutorials/using_peft_for_inference) adapters can be rapidly finetuned to learn a new style from just a few images and plugged into a pretrained model to generate images in that style.\n",
    "\n",
    "<hfoptions id=\"lcm-lora\">\n",
    "<hfoption id=\"LCM\">\n",
    "\n",
    "Load the LCM checkpoint for your supported model into [UNet2DConditionModel](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel) and replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler). Then you can use the [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights) method to load the LoRA weights into the LCM and generate a styled image in a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\n",
    "import torch\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"latent-consistency/lcm-sdxl\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\n",
    "\n",
    "prompt = \"papercut, a cute fox\"\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=8.0\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_full_sdx_lora_mix.png\"/>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "<hfoption id=\"LCM-LoRA\">\n",
    "\n",
    "Replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler). Then you can use the [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights) method to load the LCM-LoRA weights and the style LoRA you want to use. Combine both LoRA adapters with the `~loaders.UNet2DConditionLoadersMixin.set_adapters` method and generate a styled image in a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, LCMScheduler\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\", adapter_name=\"lcm\")\n",
    "pipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\n",
    "\n",
    "pipe.set_adapters([\"lcm\", \"papercut\"], adapter_weights=[1.0, 0.8])\n",
    "\n",
    "prompt = \"papercut, a cute fox\"\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdx_lora_mix.png\"/>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "</hfoptions>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ControlNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ControlNet](https://huggingface.co/docs/diffusers/main/en/using-diffusers/./controlnet) are adapters that can be trained on a variety of inputs like canny edge, pose estimation, or depth. The ControlNet can be inserted into the pipeline to provide additional conditioning and control to the model for more accurate generation.\n",
    "\n",
    "You can find additional ControlNet models trained on other inputs in [lllyasviel's](https://hf.co/lllyasviel) repository.\n",
    "\n",
    "<hfoptions id=\"lcm-controlnet\">\n",
    "<hfoption id=\"LCM\">\n",
    "\n",
    "Load a ControlNet model trained on canny images and pass it to the [ControlNetModel](https://huggingface.co/docs/diffusers/main/en/api/models/controlnet#diffusers.ControlNetModel). Then you can load a LCM model into [StableDiffusionControlNetPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline) and replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler). Now pass the canny image to the pipeline and generate an image.\n",
    "\n",
    "> [!TIP]\n",
    "> Experiment with different values for `num_inference_steps`, `controlnet_conditioning_scale`, `cross_attention_kwargs`, and `guidance_scale` to get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ").resize((512, 512))\n",
    "\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"SimianLuo/LCM_Dreamshaper_v7\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    \"the mona lisa\",\n",
    "    image=canny_image,\n",
    "    num_inference_steps=4,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "make_image_grid([canny_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_full_sdv1-5_controlnet.png\"/>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "<hfoption id=\"LCM-LoRA\">\n",
    "\n",
    "Load a ControlNet model trained on canny images and pass it to the [ControlNetModel](https://huggingface.co/docs/diffusers/main/en/api/models/controlnet#diffusers.ControlNetModel). Then you can load a Stable Diffusion v1.5 model into [StableDiffusionControlNetPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline) and replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler). Use the [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights) method to load the LCM-LoRA weights, and pass the canny image to the pipeline and generate an image.\n",
    "\n",
    "> [!TIP]\n",
    "> Experiment with different values for `num_inference_steps`, `controlnet_conditioning_scale`, `cross_attention_kwargs`, and `guidance_scale` to get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ").resize((512, 512))\n",
    "\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    \"the mona lisa\",\n",
    "    image=canny_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=1.5,\n",
    "    controlnet_conditioning_scale=0.8,\n",
    "    cross_attention_kwargs={\"scale\": 1},\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_controlnet.png\"/>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "</hfoptions>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2I-Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[T2I-Adapter](https://huggingface.co/docs/diffusers/main/en/using-diffusers/./t2i_adapter) is an even more lightweight adapter than ControlNet, that provides an additional input to condition a pretrained model with. It is faster than ControlNet but the results may be slightly worse.\n",
    "\n",
    "You can find additional T2I-Adapter checkpoints trained on other inputs in [TencentArc's](https://hf.co/TencentARC) repository.\n",
    "\n",
    "<hfoptions id=\"lcm-t2i\">\n",
    "<hfoption id=\"LCM\">\n",
    "\n",
    "Load a T2IAdapter trained on canny images and pass it to the [StableDiffusionXLAdapterPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/adapter#diffusers.StableDiffusionXLAdapterPipeline). Then load a LCM checkpoint into [UNet2DConditionModel](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel) and replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler). Now pass the canny image to the pipeline and generate an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableDiffusionXLAdapterPipeline, UNet2DConditionModel, T2IAdapter, LCMScheduler\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "# detect the canny map in low resolution to avoid high-frequency details\n",
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ").resize((384, 384))\n",
    "\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image).resize((1024, 1216))\n",
    "\n",
    "adapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"latent-consistency/lcm-sdxl\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    unet=unet,\n",
    "    adapter=adapter,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"the mona lisa, 4k picture, high quality\"\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=canny_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=5,\n",
    "    adapter_conditioning_scale=0.8,\n",
    "    adapter_conditioning_factor=1,\n",
    "    generator=generator,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm-t2i.png\"/>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "<hfoption id=\"LCM-LoRA\">\n",
    "\n",
    "Load a T2IAdapter trained on canny images and pass it to the [StableDiffusionXLAdapterPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/adapter#diffusers.StableDiffusionXLAdapterPipeline). Replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler), and use the [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights) method to load the LCM-LoRA weights. Pass the canny image to the pipeline and generate an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableDiffusionXLAdapterPipeline, UNet2DConditionModel, T2IAdapter, LCMScheduler\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "# detect the canny map in low resolution to avoid high-frequency details\n",
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ").resize((384, 384))\n",
    "\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image).resize((1024, 1024))\n",
    "\n",
    "adapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    adapter=adapter,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n",
    "\n",
    "prompt = \"the mona lisa, 4k picture, high quality\"\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=canny_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=1.5,\n",
    "    adapter_conditioning_scale=0.8,\n",
    "    adapter_conditioning_factor=1,\n",
    "    generator=generator,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm-lora-t2i.png\"/>\n",
    "</div>\n",
    "\n",
    "</hfoption>\n",
    "</hfoptions>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnimateDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AnimateDiff](https://huggingface.co/docs/diffusers/main/en/using-diffusers/../api/pipelines/animatediff) is an adapter that adds motion to an image. It can be used with most Stable Diffusion models, effectively turning them into \"video generation\" models. Generating good results with a video model usually requires generating multiple frames (16-24), which can be very slow with a regular Stable Diffusion model. LCM-LoRA can speed up this process by only taking 4-8 steps for each frame.\n",
    "\n",
    "Load a [AnimateDiffPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/animatediff#diffusers.AnimateDiffPipeline) and pass a `MotionAdapter` to it. Then replace the scheduler with the [LCMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lcm#diffusers.LCMScheduler), and combine both LoRA adapters with the `~loaders.UNet2DConditionLoadersMixin.set_adapters` method. Now you can pass a prompt to the pipeline and generate an animated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "\n",
    "adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5\")\n",
    "pipe = AnimateDiffPipeline.from_pretrained(\n",
    "    \"frankjoshua/toonyou_beta6\",\n",
    "    motion_adapter=adapter,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# set scheduler\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# load LCM-LoRA\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\", adapter_name=\"lcm\")\n",
    "pipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", weight_name=\"diffusion_pytorch_model.safetensors\", adapter_name=\"motion-lora\")\n",
    "\n",
    "pipe.set_adapters([\"lcm\", \"motion-lora\"], adapter_weights=[0.55, 1.2])\n",
    "\n",
    "prompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\n",
    "generator = torch.manual_seed(0)\n",
    "frames = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=5,\n",
    "    guidance_scale=1.25,\n",
    "    cross_attention_kwargs={\"scale\": 1},\n",
    "    num_frames=24,\n",
    "    generator=generator\n",
    ").frames[0]\n",
    "export_to_gif(frames, \"animation.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm-lora-animatediff.gif\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
