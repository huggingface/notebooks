{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisim/notebooks/blob/autoformer-and-dlinear/examples/autoformer-transformers-are-effective.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73b1232b",
      "metadata": {
        "id": "73b1232b"
      },
      "source": [
        "# Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "A few months ago, we introduced the [Informer](https://huggingface.co/blog/informer) model ([Zhou, Haoyi, et al., 2021](https://arxiv.org/abs/2012.07436)), which is a Time Series Transformer that won the AAAI 2021 best paper award. We also provided an example for multivariate probabilistic forecasting with Informer. In this post, we discuss the question: [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504) (AAAI 2023). As we will see, they are.\n",
        "\n",
        "Firstly, we will provide empirical evidence that **Transformers are indeed Effective for Time Series Forecasting**. Our comparison shows that the simple linear model, known as _DLinear_, is not better than Transformers as claimed. When compared against equivalent sized models in the same setting as the linear models, the Transformer-based models perform better on the test set metrics we consider.\n",
        "Afterwards, we will introduce the _Autoformer_ model ([Wu, Haixu, et al., 2021](https://arxiv.org/abs/2106.13008)), which was published in NeurIPS 2021 after the Informer model. The Autoformer model is [now available](https://huggingface.co/docs/transformers/main/en/model_doc/autoformer) in ğŸ¤— Transformers. Finally, we will discuss the _DLinear_ model, which is a simple feedforward network that uses the decomposition layer from Autoformer. The DLinear model was first introduced in [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504) and claimed to outperform Transformer-based models in time-series forecasting.\n",
        "\n",
        "Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking - Transformers vs. DLinear\n",
        "In the paper [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504), published recently in AAAI 2023,\n",
        "the authors claim that Transformers are not effective for time series forecasting. They compare the Transformer-based models against a simple linear model, which they call _DLinear_.\n",
        "The DLinear model uses the decomposition layer from the Autoformer model, which we will introduce later in this post. The authors claim that the DLinear model outperforms the Transformer-based models in time-series forecasting.\n",
        "Is that so? Let's find out.\n",
        "\n",
        "|      Dataset      | Autoformer (uni.) MASE | DLinear  MASE |\n",
        "|:-----------------:|:----------------------:|:-------------:|\n",
        "|    `Traffic` \t    |         0.910          |     0.969     |\n",
        "| `Exchange-Rate` \t |         1.087          |     1.690     |\n",
        "|  `Electricity` \t  |         0.751          |     0.831     |\n",
        "\n",
        "The table above shows the results of the comparison between the Autoformer and DLinear models on the three datasets used in the paper.\n",
        "The results show that the Autoformer model outperforms the DLinear model on all three datasets.\n",
        "\n",
        "Next, we will present the new Autoformer model along with the DLinear model. We will showcase how to compare them on the Traffic dataset from the table above, and provide explanations for the results we obtained.\n",
        "\n",
        "**TL;DR:** A simple linear model, while advantageous in certain cases, has no capacity to incorporate covariates compared to more complex models like transformers in the univariate setting.\n"
      ],
      "metadata": {
        "id": "o9KmCEu7OJmf"
      },
      "id": "o9KmCEu7OJmf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoformer - Under The Hood\n",
        "\n",
        "Autoformer builds upon the traditional method of decomposing time series into seasonality and trend-cycle components. This is achieved through the incorporation of a _Decomposition Layer_, which enhances the model's ability to capture these components accurately. Moreover, Autoformer introduces an innovative auto-correlation mechanism that replaces the standard self-attention used in the vanilla transformer. This mechanism enables the model to utilize period-based dependencies in the attention, thus improving the overall performance.\n",
        "\n",
        "In the upcoming sections, we will delve into the two key contributions of Autoformer: the _Decomposition Layer_ and the _Attention (Autocorrelation) Mechanism_. We will also provide code examples to illustrate how these components function within the Autoformer architecture.\n",
        "\n",
        "### Decomposition Layer\n",
        "Decomposition has long been a popular method in time series analysis, but it had not been extensively incorporated into deep learning models until the introduction of the Autoformer paper. Following a brief explanation of the concept, we will demonstrate how the idea is applied in Autoformer using PyTorch code.\n",
        "\n",
        "####  Decomposition of Time Series\n",
        "In time series analysis, [decomposition](https://en.wikipedia.org/wiki/Decomposition_of_time_series) is a method of breaking down a time series into three systematic components: trend-cycle, seasonal variation, and random fluctuations.\n",
        "The trend component represents the long-term direction of the time series, which can be increasing, decreasing, or stable over time. The seasonal component represents the recurring patterns that occur within the time series, such as yearly or quarterly cycles. Finally, the random (sometimes called \"irregular\") component represents the random noise in the data that cannot be explained by the trend or seasonal components.\n",
        "\n",
        "Two main types of decomposition are additive and multiplicative decomposition, which are implemented in the [great statsmodels library](https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html). By decomposing a time series into these components, we can better understand and model the underlying patterns in the data.\n",
        "\n",
        "But how can we incorporate decomposition into the Transformer architecture? Let's see how Autoformer does it.\n",
        "\n",
        "#### Decomposition in Autoformer\n",
        "\n",
        "| ![autoformer_architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/148_autoformer/autoformer_architecture.png) |\n",
        "|:--:|\n",
        "| Autoformer architecture from [the paper](https://arxiv.org/abs/2106.13008) |\n",
        "\n",
        "Autoformer incorporates a decomposition block as an inner operation of the model, as presented in the Autoformer's architecture above. As can be seen, the encoder and decoder use a decomposition block to aggregate the trend-cyclical part and extract the seasonal part from the series progressively. The concept of inner decomposition has demonstrated its usefulness since the publication of Autoformer. Subsequently, it has been adopted in several other time series papers, such as FEDformer ([Zhou, Tian, et al., ICML 2022](https://arxiv.org/abs/2201.12740)) and DLinear [(Zeng, Ailing, et al., AAAI 2023)](https://arxiv.org/abs/2205.13504), highlighting its significance in time series modeling.\n",
        "\n",
        "Now, let's define the decomposition layer formally:\n",
        "\n",
        "For an input series \\\\(\\mathcal{X} \\in \\mathbb{R}^{L \\times d}\\\\) with length \\\\(L\\\\), the decomposition layer returns \\\\(\\mathcal{X}_\\textrm{trend}, \\mathcal{X}_\\textrm{seasonal}\\\\) defined as:\n",
        "\n",
        "$$\n",
        "\\mathcal{X}_\\textrm{trend} = \\textrm{AvgPool(Padding(} \\mathcal{X} \\textrm{))} \\\\\n",
        "\\mathcal{X}_\\textrm{seasonal} = \\mathcal{X} - \\mathcal{X}_\\textrm{trend}\n",
        "$$\n",
        "\n",
        "And the implementation in PyTorch:\n",
        "```python\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class DecompositionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Returns the trend and the seasonal parts of the time series.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=0) # moving average\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Input shape: Batch x Time x EMBED_DIM\"\"\"\n",
        "        # padding on the both ends of time series\n",
        "        num_of_pads = (self.kernel_size - 1) // 2\n",
        "        front = x[:, 0:1, :].repeat(1, num_of_pads, 1)\n",
        "        end = x[:, -1:, :].repeat(1, num_of_pads, 1)\n",
        "        x_padded = torch.cat([front, x, end], dim=1)\n",
        "\n",
        "        # calculate the trend and seasonal part of the series\n",
        "        x_trend = self.avg(x_padded.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "        x_seasonal = x - x_trend\n",
        "        return x_seasonal, x_trend\n",
        "```\n",
        "\n",
        "As you can see, the implementation is quite simple and can be used in other models, as we will see with DLinear. Now, let's explain the second contribution - _Attention (Autocorrelation) Mechanism_.\n",
        "\n",
        "### Attention (Autocorrelation) Mechanism\n",
        "\n",
        "| ![autoformer_autocorrelation_vs_full_attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/148_autoformer/autoformer_autocorrelation_vs_full_attention.png) |\n",
        "|:--:|\n",
        "|  Vanilla self attention vs Autocorrelation mechanism, from [the paper](https://arxiv.org/abs/2106.13008) |\n",
        "\n",
        "In addition to the decomposition layer, Autoformer employs a novel auto-correlation mechanism which replaces the self-attention seamlessly. In the [vanilla Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer), attention weights are computed in the time domain and point-wise aggregated. On the other hand, as can be seen in the figure above, Autoformer computes them in the frequency domain (using [fast fourier transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform)) and aggregates them by time delay.\n",
        "\n",
        "In the following sections, we will dive into these topics in detail and explain them with code examples.\n",
        "\n",
        "####  Frequency Domain Attention\n",
        "\n",
        "| ![autoformer_autocorrelation_only_attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/148_autoformer/autoformer_autocorrelation_only_attention.png) |\n",
        "|:--:|\n",
        "| Attention weights computation in frequency domain using FFT, from [the paper](https://arxiv.org/abs/2106.13008) |\n",
        "\n",
        "In theory, given a time lag \\\\(\\tau\\\\), _autocorrelation_ for a single discrete variable \\\\(y\\\\) is used to measure the \"relationship\" (pearson correlation) between the variable's current value at time \\\\(t\\\\) to its past value at time \\\\(t-\\tau\\\\):\n",
        "\n",
        "$$\n",
        "\\textrm{Autocorrelation}(\\tau) = \\textrm{Corr}(y_t, y_{t-\\tau})\n",
        "$$\n",
        "\n",
        "Using autocorrelation, Autoformer extracts frequency-based dependencies from the queries and keys, instead of the standard dot-product between them. You can think about it as a replacement for the \\\\(QK^T\\\\) term in the self-attention.\n",
        "\n",
        "In practice, autocorrelation of the queries and keys for **all lags** is calculated at once by FFT. By doing so, the autocorrelation mechanism achieves \\\\(O(L \\log L)\\\\) time complexity (\\\\(L\\\\) is the input time length), similar to [Informer's ProbSparse attention](https://huggingface.co/blog/informer#probsparse-attention). Note that the theory behind computing autocorrelation using FFT is based on the [Wienerâ€“Khinchin theorem](https://en.wikipedia.org/wiki/Wiener%E2%80%93Khinchin_theorem), which is outside the scope of this blog post.\n",
        "\n",
        "Now, we are ready to see the code in PyTorch:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "def autocorrelation(query_states, key_states):\n",
        "    \"\"\"\n",
        "    Computes autocorrelation(Q,K) using `torch.fft`.\n",
        "    Think about it as a replacement for the QK^T in the self-attention.\n",
        "\n",
        "    Assumption: states are resized to same shape of [batch_size, time_length, embedding_dim].\n",
        "    \"\"\"\n",
        "    query_states_fft = torch.fft.rfft(query_states, dim=1)\n",
        "    key_states_fft = torch.fft.rfft(key_states, dim=1)\n",
        "    attn_weights = query_states_fft * torch.conj(key_states_fft)\n",
        "    attn_weights = torch.fft.irfft(attn_weights, dim=1)\n",
        "\n",
        "    return attn_weights\n",
        "```\n",
        "\n",
        "Quite simple! ğŸ˜ Please be aware that this is only a partial implementation of `autocorrelation(Q,K)`, and the full implementation can be found in ğŸ¤— Transformers.\n",
        "\n",
        "Next, we will see how to aggregate our `attn_weights` with the values by time delay, process which is termed as _Time Delay Aggregation_.\n",
        "\n",
        "#### Time Delay Aggregation\n",
        "| ![autoformer_autocorrelation_only_aggregation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/148_autoformer/autoformer_autocorrelation_only_aggregation.png) |\n",
        "|:--:|\n",
        "| Aggregation by time delay, from [the Autoformer paper](https://arxiv.org/abs/2106.13008) |\n",
        "\n",
        "Let's consider the autocorrelations (referred to as `attn_weights`) as \\\\(\\mathcal{R_{Q,K}}\\\\). The question arises: how do we aggregate these \\\\(\\mathcal{R_{Q,K}}(\\tau_1), \\mathcal{R_{Q,K}}(\\tau_2), ..., \\mathcal{R_{Q,K}}(\\tau_k)\\\\) with \\\\(\\mathcal{V}\\\\)? In the standard self-attention mechanism, this aggregation is accomplished through dot-product. However, in Autoformer, we employ a different approach. Firstly, we align \\\\(\\mathcal{V}\\\\) by calculating its value for each time delay \\\\(\\tau_1, \\tau_2, ... \\tau_k\\\\), which is also known as _Rolling_. Subsequently, we conduct element-wise multiplication between the aligned \\\\(\\mathcal{V}\\\\) and the autocorrelations. In the provided figure, you can observe the left side showcasing the rolling of \\\\(\\mathcal{V}\\\\) by time delay, while the right side illustrates the element-wise multiplication with the autocorrelations.\n",
        "\n",
        "It can be summarized with the following equations:\n",
        "\n",
        "$$\n",
        "\\tau_1, \\tau_2, ... \\tau_k = \\textrm{arg Top-k}(\\mathcal{R_{Q,K}}(\\tau)) \\\\\n",
        "\\hat{\\mathcal{R}}\\mathcal{_{Q,K}}(\\tau _1), \\hat{\\mathcal{R}}\\mathcal{_{Q,K}}(\\tau _2), ..., \\hat{\\mathcal{R}}\\mathcal{_{Q,K}}(\\tau _k) = \\textrm{Softmax}(\\mathcal{R_{Q,K}}(\\tau _1), \\mathcal{R_{Q,K}}(\\tau_2), ..., \\mathcal{R_{Q,K}}(\\tau_k)) \\\\\n",
        "\\textrm{Autocorrelation-Attention} = \\sum_{i=1}^k \\textrm{Roll}(\\mathcal{V}, \\tau_i) \\cdot \\hat{\\mathcal{R}}\\mathcal{_{Q,K}}(\\tau _i)\n",
        "$$\n",
        "\n",
        "And that's it! Note that \\\\(k\\\\) is controlled by a hyperparameter called `autocorrelation_factor` (similar to `sampling_factor` in [Informer](https://huggingface.co/blog/informer)), and softmax is applied to the autocorrelations before the multiplication.\n",
        "\n",
        "Now, we are ready to see the final code:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def time_delay_aggregation(attn_weights, value_states, autocorrelation_factor=2):\n",
        "    \"\"\"\n",
        "    Computes aggregation as value_states.roll(delay) * top_k_autocorrelations(delay).\n",
        "    The final result is the autocorrelation-attention output.\n",
        "    Think about it as a replacement of the dot-product between attn_weights and value states.\n",
        "\n",
        "    The autocorrelation_factor is used to find top k autocorrelations delays.\n",
        "    Assumption: value_states and attn_weights shape: [batch_size, time_length, embedding_dim]\n",
        "    \"\"\"\n",
        "    bsz, num_heads, tgt_len, channel = ...\n",
        "    time_length = value_states.size(1)\n",
        "    autocorrelations = attn_weights.view(bsz, num_heads, tgt_len, channel)\n",
        "\n",
        "    # find top k autocorrelations delays\n",
        "    top_k = int(autocorrelation_factor * math.log(time_length))\n",
        "    autocorrelations_mean = torch.mean(autocorrelations, dim=(1, -1))  # bsz x tgt_len\n",
        "    top_k_autocorrelations, top_k_delays = torch.topk(autocorrelations_mean, top_k, dim=1)\n",
        "\n",
        "    # apply softmax on the channel dim\n",
        "    top_k_autocorrelations = torch.softmax(top_k_autocorrelations, dim=-1)  # bsz x top_k\n",
        "\n",
        "    # compute aggregation: value_states.roll(delay) * top_k_autocorrelations(delay)\n",
        "    delays_agg = torch.zeros_like(value_states).float()  # bsz x time_length x channel\n",
        "    for i in range(top_k):\n",
        "        value_states_roll_delay = value_states.roll(shifts=-int(top_k_delays[i]), dims=1)\n",
        "        top_k_at_delay = top_k_autocorrelations[:, i]\n",
        "        # aggregation\n",
        "        top_k_resized = top_k_at_delay.view(-1, 1, 1).repeat(num_heads, tgt_len, channel)\n",
        "        delays_agg += value_states_roll_delay * top_k_resized\n",
        "\n",
        "    attn_output = delays_agg.contiguous()\n",
        "    return attn_output\n",
        "```\n",
        "\n",
        "We did it! The Autoformer model is [now available](https://huggingface.co/docs/transformers/main/en/model_doc/autoformer) in the ğŸ¤— Transformers library, and simply called `AutoformerModel`.\n",
        "\n",
        "Our strategy with this model is to show the performance of the univariate Transformer models in comparison to the DLinear model which is inherently univariate as will shown next. We will also present the results from _two_ multivariate Transformer models trained on the same data.\n"
      ],
      "metadata": {
        "id": "fke9AS-gOMUL"
      },
      "id": "fke9AS-gOMUL"
    },
    {
      "cell_type": "markdown",
      "id": "a4dcdb78",
      "metadata": {
        "id": "a4dcdb78"
      },
      "source": [
        "## DLinear - Under The Hood\n",
        "\n",
        "Actually, DLinear is conceptually simple: it's just a fully connected with the Autoformer's `DecompositionLayer`.\n",
        "It uses the `DecompositionLayer` above to decompose the input time series into the residual (the seasonality) and trend part. In the forward pass each part is passed through its own linear layer, which projects the signal to an appropriate `prediction_length`-sized output. The final output is the sum of the two corresponding outputs in the point-forecasting model:\n",
        "\n",
        "```python\n",
        "def forward(self, context):\n",
        "    seasonal, trend = self.decomposition(context)\n",
        "    seasonal_output = self.linear_seasonal(seasonal)\n",
        "    trend_output = self.linear_trend(trend)\n",
        "    return seasonal_output + trend_output\n",
        "```\n",
        "\n",
        "In the probabilistic setting one can project the context length arrays to  `prediction-length * hidden` dimensions via the `linear_seasonal` and `linear_trend` layers.  The resulting outputs are added and reshaped to `(prediction_length, hidden)`. Finally, a probabilistic head maps the latent representations of size `hidden` to the parameters of some distribution.\n",
        "\n",
        "In our benchmark, we use the implementation of DLinear from [GluonTS](https://github.com/awslabs/gluonts).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2204552b",
      "metadata": {
        "id": "2204552b"
      },
      "source": [
        "## Example: Traffic Dataset\n",
        "\n",
        "We want to show empirically the performance of Transformer-based models in the library, by benchmarking on the `traffic` dataset, a dataset with 862 time series. We will train a shared model on each of the individual time series (i.e. univariate setting).\n",
        "Each time series represents the occupancy value of a sensor and is in the range [0, 1]. We will keep the following hyperparameters fixed for all the models:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traffic prediction_length is 24. Reference:\n",
        "# https://github.com/awslabs/gluonts/blob/6605ab1278b6bf92d5e47343efcf0d22bc50b2ec/src/gluonts/dataset/repository/_lstnet.py#L105\n",
        "\n",
        "prediction_length = 24\n",
        "context_length = prediction_length*2\n",
        "batch_size = 128\n",
        "num_batches_per_epoch = 100\n",
        "epochs = 50\n",
        "scaling = \"std\""
      ],
      "metadata": {
        "id": "8xakUeWWOw9V"
      },
      "id": "8xakUeWWOw9V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformers models are all relatively small with:\n"
      ],
      "metadata": {
        "id": "8ad6GUtSO-eS"
      },
      "id": "8ad6GUtSO-eS"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layers=2\n",
        "decoder_layers=2\n",
        "d_model=16"
      ],
      "metadata": {
        "id": "PFrKFGrlO_jA"
      },
      "id": "PFrKFGrlO_jA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of showing how to train a model using `Autoformer`, one can just replace the model in the previous two blog posts ([TimeSeriesTransformer](https://huggingface.co/blog/time-series-transformers) and [Informer](https://huggingface.co/blog/informer)) with the new `Autoformer` model and train it on the `traffic` dataset. In order to not repeat ourselves, we have already trained the models and pushed them to the HuggingFace Hub. We will use those models for evaluation."
      ],
      "metadata": {
        "id": "6AXubK6rPQPp"
      },
      "id": "6AXubK6rPQPp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset\n",
        "\n",
        "Let's first install the necessary libraries:"
      ],
      "metadata": {
        "id": "pn1U5Pj3PRUU"
      },
      "id": "pn1U5Pj3PRUU"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets evaluate accelerate \"gluonts[torch]\" ujson tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azOR4uCRPXOO",
        "outputId": "d20c5f4c-9cfe-4877-d42c-0aa6b4084a9e"
      },
      "id": "azOR4uCRPXOO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m720.6/720.6 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `traffic` dataset, used by [Lai et al. (2017)](https://arxiv.org/abs/1703.07015), contains the San Francisco Traffic. It contains 862 hourly time series showing the road occupancy rates in the range \\\\([0, 1]\\\\) on the San Francisco Bay Area freeways from 2015 to 2016."
      ],
      "metadata": {
        "id": "GGr6NjN8Pc8g"
      },
      "id": "GGr6NjN8Pc8g"
    },
    {
      "cell_type": "code",
      "source": [
        "from gluonts.dataset.repository.datasets import get_dataset\n",
        "\n",
        "dataset = get_dataset(\"traffic\")\n",
        "freq = dataset.metadata.freq\n",
        "prediction_length = dataset.metadata.prediction_length\n",
        "print(f\"traffic dataset prediction_length: {prediction_length}\")"
      ],
      "metadata": {
        "id": "MKu63zXnPgMt"
      },
      "id": "MKu63zXnPgMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize a time series in the dataset and plot the train/test split:"
      ],
      "metadata": {
        "id": "MlBG39UEPmYY"
      },
      "id": "MlBG39UEPmYY"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_example = next(iter(dataset.train))\n",
        "test_example = next(iter(dataset.test))\n",
        "\n",
        "num_of_samples = 4*prediction_length\n",
        "\n",
        "figure, axes = plt.subplots()\n",
        "axes.plot(train_example[\"target\"][-num_of_samples:], color=\"blue\")\n",
        "axes.plot(\n",
        "    test_example[\"target\"][-num_of_samples - prediction_length :],\n",
        "    color=\"red\",\n",
        "    alpha=0.5,\n",
        ")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vjHeqEjkQGOC"
      },
      "id": "vjHeqEjkQGOC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the train/test splits:\n"
      ],
      "metadata": {
        "id": "Yxon2GHdQJis"
      },
      "id": "Yxon2GHdQJis"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.train\n",
        "test_dataset = dataset.test"
      ],
      "metadata": {
        "id": "pymU2p-FQK_H"
      },
      "id": "pymU2p-FQK_H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Transformations\n",
        "\n",
        "Next, we define the transformations for the data, in particular for the creation of the time features (based on the dataset or universal ones).\n",
        "\n",
        "We define a `Chain` of transformations from GluonTS (which is a bit comparable to `torchvision.transforms.Compose` for images). It allows us to combine several transformations into a single pipeline.\n",
        "\n",
        "The transformations below are annotated with comments to explain what they do. At a high level, we will iterate over the individual time series of our dataset and add/remove fields or features:\n"
      ],
      "metadata": {
        "id": "ugf1jahDQNCa"
      },
      "id": "ugf1jahDQNCa"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig\n",
        "from gluonts.time_feature import time_features_from_frequency_str\n",
        "\n",
        "from gluonts.dataset.field_names import FieldName\n",
        "from gluonts.transform import (\n",
        "    AddAgeFeature,\n",
        "    AddObservedValuesIndicator,\n",
        "    AddTimeFeatures,\n",
        "    AsNumpyArray,\n",
        "    Chain,\n",
        "    ExpectedNumInstanceSampler,\n",
        "    RemoveFields,\n",
        "    SelectFields,\n",
        "    SetField,\n",
        "    TestSplitSampler,\n",
        "    Transformation,\n",
        "    ValidationSplitSampler,\n",
        "    VstackFeatures,\n",
        "    RenameFields,\n",
        ")\n",
        "\n",
        "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
        "    # create a list of fields to remove later\n",
        "    remove_field_names = []\n",
        "    if config.num_static_real_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
        "    if config.num_dynamic_real_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
        "    if config.num_static_categorical_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
        "\n",
        "    return Chain(\n",
        "        # step 1: remove static/dynamic fields if not specified\n",
        "        [RemoveFields(field_names=remove_field_names)]\n",
        "        # step 2: convert the data to NumPy (potentially not needed)\n",
        "        + (\n",
        "            [\n",
        "                AsNumpyArray(\n",
        "                    field=FieldName.FEAT_STATIC_CAT,\n",
        "                    expected_ndim=1,\n",
        "                    dtype=int,\n",
        "                )\n",
        "            ]\n",
        "            if config.num_static_categorical_features > 0\n",
        "            else []\n",
        "        )\n",
        "        + (\n",
        "            [\n",
        "                AsNumpyArray(\n",
        "                    field=FieldName.FEAT_STATIC_REAL,\n",
        "                    expected_ndim=1,\n",
        "                )\n",
        "            ]\n",
        "            if config.num_static_real_features > 0\n",
        "            else []\n",
        "        )\n",
        "        + [\n",
        "            AsNumpyArray(\n",
        "                field=FieldName.TARGET,\n",
        "                # we expect an extra dim for the multivariate case:\n",
        "                expected_ndim=1 if config.input_size == 1 else 2,\n",
        "            ),\n",
        "            # step 3: handle the NaN's by filling in the target with zero\n",
        "            # and return the mask (which is in the observed values)\n",
        "            # true for observed values, false for nan's\n",
        "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
        "            # see loss_weights inside the xxxForPrediction model\n",
        "            AddObservedValuesIndicator(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.OBSERVED_VALUES,\n",
        "            ),\n",
        "            # step 4: add temporal features based on freq of the dataset\n",
        "            # these serve as positional encodings\n",
        "            AddTimeFeatures(\n",
        "                start_field=FieldName.START,\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.FEAT_TIME,\n",
        "                time_features=time_features_from_frequency_str(freq),\n",
        "                pred_length=config.prediction_length,\n",
        "            ),\n",
        "            # step 5: add another temporal feature (just a single number)\n",
        "            # tells the model where in the life the value of the time series is\n",
        "            # sort of running counter\n",
        "            AddAgeFeature(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.FEAT_AGE,\n",
        "                pred_length=config.prediction_length,\n",
        "                log_scale=True,\n",
        "            ),\n",
        "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
        "            VstackFeatures(\n",
        "                output_field=FieldName.FEAT_TIME,\n",
        "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
        "                + (\n",
        "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
        "                    if config.num_dynamic_real_features > 0\n",
        "                    else []\n",
        "                ),\n",
        "            ),\n",
        "            # step 7: rename to match HuggingFace names\n",
        "            RenameFields(\n",
        "                mapping={\n",
        "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
        "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
        "                    FieldName.FEAT_TIME: \"time_features\",\n",
        "                    FieldName.TARGET: \"values\",\n",
        "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
        "                }\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "DaRe5f3oQR9u"
      },
      "id": "DaRe5f3oQR9u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define `InstanceSplitter`\n",
        "\n",
        "For training/validation/testing we next create an `InstanceSplitter` which is used to sample windows from the dataset (as, remember, we can't pass the entire history of values to the model due to time and memory constraints).\n",
        "\n",
        "The instance splitter samples random `context_length` sized and subsequent `prediction_length` sized windows from the data, and appends a `past_` or `future_` key to any temporal keys for the respective windows. This makes sure that the `values` will be split into `past_values` and subsequent `future_values` keys, which will serve as the encoder and decoder inputs respectively. The same happens for any keys in the `time_series_fields` argument:\n"
      ],
      "metadata": {
        "id": "nuVniPDaQS-m"
      },
      "id": "nuVniPDaQS-m"
    },
    {
      "cell_type": "code",
      "source": [
        "from gluonts.transform import InstanceSplitter\n",
        "from gluonts.transform.sampler import InstanceSampler\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "def create_instance_splitter(\n",
        "    config: PretrainedConfig,\n",
        "    mode: str,\n",
        "    train_sampler: Optional[InstanceSampler] = None,\n",
        "    validation_sampler: Optional[InstanceSampler] = None,\n",
        ") -> Transformation:\n",
        "    assert mode in [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "    instance_sampler = {\n",
        "        \"train\": train_sampler\n",
        "        or ExpectedNumInstanceSampler(\n",
        "            num_instances=1.0, min_future=config.prediction_length\n",
        "        ),\n",
        "        \"validation\": validation_sampler\n",
        "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
        "        \"test\": TestSplitSampler(),\n",
        "    }[mode]\n",
        "\n",
        "    return InstanceSplitter(\n",
        "        target_field=\"values\",\n",
        "        is_pad_field=FieldName.IS_PAD,\n",
        "        start_field=FieldName.START,\n",
        "        forecast_start_field=FieldName.FORECAST_START,\n",
        "        instance_sampler=instance_sampler,\n",
        "        past_length=config.context_length + max(config.lags_sequence),\n",
        "        future_length=config.prediction_length,\n",
        "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "yQlvAcXbQYve"
      },
      "id": "yQlvAcXbQYve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create PyTorch DataLoaders\n",
        "\n",
        "Next, it's time to create PyTorch DataLoaders, which allow us to have batches of (input, output) pairs - or in other words (`past_values`, `future_values`).\n"
      ],
      "metadata": {
        "id": "UV0rhLvWQZyu"
      },
      "id": "UV0rhLvWQZyu"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable\n",
        "\n",
        "import torch\n",
        "from gluonts.itertools import Cyclic, Cached\n",
        "from gluonts.dataset.loader import as_stacked_batches\n",
        "\n",
        "\n",
        "def create_train_dataloader(\n",
        "    config: PretrainedConfig,\n",
        "    freq,\n",
        "    data,\n",
        "    batch_size: int,\n",
        "    num_batches_per_epoch: int,\n",
        "    shuffle_buffer_length: Optional[int] = None,\n",
        "    cache_data: bool = True,\n",
        "    **kwargs,\n",
        ") -> Iterable:\n",
        "    PREDICTION_INPUT_NAMES = [\n",
        "        \"past_time_features\",\n",
        "        \"past_values\",\n",
        "        \"past_observed_mask\",\n",
        "        \"future_time_features\",\n",
        "    ]\n",
        "    if config.num_static_categorical_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
        "\n",
        "    if config.num_static_real_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
        "\n",
        "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
        "        \"future_values\",\n",
        "        \"future_observed_mask\",\n",
        "    ]\n",
        "\n",
        "    transformation = create_transformation(freq, config)\n",
        "    transformed_data = transformation.apply(data, is_train=True)\n",
        "    if cache_data:\n",
        "        transformed_data = Cached(transformed_data)\n",
        "\n",
        "    # we initialize a Training instance\n",
        "    instance_splitter = create_instance_splitter(config, \"train\")\n",
        "\n",
        "    # the instance splitter will sample a window of\n",
        "    # context length + lags + prediction length (from the 366 possible transformed time series)\n",
        "    # randomly from within the target time series and return an iterator.\n",
        "    stream = Cyclic(transformed_data).stream()\n",
        "    training_instances = instance_splitter.apply(stream, is_train=True)\n",
        "\n",
        "    return as_stacked_batches(\n",
        "        training_instances,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer_length=shuffle_buffer_length,\n",
        "        field_names=TRAINING_INPUT_NAMES,\n",
        "        output_type=torch.tensor,\n",
        "        num_batches_per_epoch=num_batches_per_epoch,\n",
        "    )"
      ],
      "metadata": {
        "id": "CT-HBLwIQf08"
      },
      "id": "CT-HBLwIQf08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_dataloader(\n",
        "    config: PretrainedConfig,\n",
        "    freq,\n",
        "    data,\n",
        "    batch_size: int,\n",
        "    **kwargs,\n",
        "):\n",
        "    PREDICTION_INPUT_NAMES = [\n",
        "        \"past_time_features\",\n",
        "        \"past_values\",\n",
        "        \"past_observed_mask\",\n",
        "        \"future_time_features\",\n",
        "    ]\n",
        "    if config.num_static_categorical_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
        "\n",
        "    if config.num_static_real_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
        "\n",
        "    transformation = create_transformation(freq, config)\n",
        "    transformed_data = transformation.apply(data, is_train=False)\n",
        "\n",
        "    # we create a Test Instance splitter which will sample the very last\n",
        "    # context window seen during training only for the encoder.\n",
        "    instance_sampler = create_instance_splitter(config, \"test\")\n",
        "\n",
        "    # we apply the transformations in test mode\n",
        "    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\n",
        "\n",
        "    return as_stacked_batches(\n",
        "        testing_instances,\n",
        "        batch_size=batch_size,\n",
        "        output_type=torch.tensor,\n",
        "        field_names=PREDICTION_INPUT_NAMES,\n",
        "    )"
      ],
      "metadata": {
        "id": "mQKA-5YKQgeb"
      },
      "id": "mQKA-5YKQgeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on Autoformer\n",
        "\n",
        "We have already pre-trained an Autoformer model on this dataset, so we can just fetch the model and evaluate it on the test set:\n"
      ],
      "metadata": {
        "id": "kAcFTsVrQjK9"
      },
      "id": "kAcFTsVrQjK9"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoformerConfig, AutoformerForPrediction\n",
        "\n",
        "config = AutoformerConfig.from_pretrained(\"kashif/autoformer-traffic-hourly\")\n",
        "model = AutoformerForPrediction.from_pretrained(\"kashif/autoformer-traffic-hourly\")\n",
        "\n",
        "test_dataloader = create_test_dataloader(\n",
        "    config=config,\n",
        "    freq=freq,\n",
        "    data=test_dataset,\n",
        "    batch_size=64,\n",
        ")"
      ],
      "metadata": {
        "id": "d8fUtwE4Qk0s"
      },
      "id": "d8fUtwE4Qk0s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At inference time, we will use the model's `generate()` method for predicting `prediction_length` steps into the future from the very last context window of each time series in the training set.\n"
      ],
      "metadata": {
        "id": "rIj1HA1cQnAf"
      },
      "id": "rIj1HA1cQnAf"
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "forecasts_ = []\n",
        "for batch in test_dataloader:\n",
        "    outputs = model.generate(\n",
        "        static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
        "        if config.num_static_categorical_features > 0\n",
        "        else None,\n",
        "        static_real_features=batch[\"static_real_features\"].to(device)\n",
        "        if config.num_static_real_features > 0\n",
        "        else None,\n",
        "        past_time_features=batch[\"past_time_features\"].to(device),\n",
        "        past_values=batch[\"past_values\"].to(device),\n",
        "        future_time_features=batch[\"future_time_features\"].to(device),\n",
        "        past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
        "    )\n",
        "    forecasts_.append(outputs.sequences.cpu().numpy())"
      ],
      "metadata": {
        "id": "Mm12jH8bQpZ2"
      },
      "id": "Mm12jH8bQpZ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `input_size`).\n",
        "\n",
        "In this case, we get `100` possible values for the next `24` hours for each of the  time series in the test dataloader batch which if you recall from above is `64`:\n"
      ],
      "metadata": {
        "id": "f3CfJimiQr4n"
      },
      "id": "f3CfJimiQr4n"
    },
    {
      "cell_type": "code",
      "source": [
        "forecasts_[0].shape"
      ],
      "metadata": {
        "id": "EiaDRQpuQtvc"
      },
      "id": "EiaDRQpuQtvc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll stack them vertically, to get forecasts for all time-series in the test dataset: We have `7` rolling windows in the test set which is why we end up with a total of `7 * 862 = 6034` predictions:\n"
      ],
      "metadata": {
        "id": "l8RP9wfHQvrw"
      },
      "id": "l8RP9wfHQvrw"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "forecasts = np.vstack(forecasts_)\n",
        "print(forecasts.shape)"
      ],
      "metadata": {
        "id": "X9sbEmRpQzrK"
      },
      "id": "X9sbEmRpQzrK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate the resulting forecast with respect to the ground truth out of sample values present in the test set. For that, we'll use the ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library, which includes the [MASE](https://huggingface.co/spaces/evaluate-metric/mase) metrics.\n",
        "\n",
        "We calculate the metric for each time series in the dataset and return the average:\n"
      ],
      "metadata": {
        "id": "wtVOIg8aQ0Pa"
      },
      "id": "wtVOIg8aQ0Pa"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "from evaluate import load\n",
        "from gluonts.time_feature import get_seasonality\n",
        "\n",
        "mase_metric = load(\"evaluate-metric/mase\")\n",
        "\n",
        "forecast_median = np.median(forecasts, 1)\n",
        "\n",
        "mase_metrics = []\n",
        "for item_id, ts in enumerate(tqdm(test_dataset)):\n",
        "    training_data = ts[\"target\"][:-prediction_length]\n",
        "    ground_truth = ts[\"target\"][-prediction_length:]\n",
        "    mase = mase_metric.compute(\n",
        "        predictions=forecast_median[item_id],\n",
        "        references=np.array(ground_truth),\n",
        "        training=np.array(training_data),\n",
        "        periodicity=get_seasonality(freq))\n",
        "    mase_metrics.append(mase[\"mase\"])"
      ],
      "metadata": {
        "id": "g2DeG9EsQ2YO"
      },
      "id": "g2DeG9EsQ2YO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the result for the Autoformer model is:\n"
      ],
      "metadata": {
        "id": "3eqRbZDYQ4yQ"
      },
      "id": "3eqRbZDYQ4yQ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Autoformer univariate MASE: {np.mean(mase_metrics):.3f}\")"
      ],
      "metadata": {
        "id": "MlnATIhGQ7VA"
      },
      "id": "MlnATIhGQ7VA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To plot the prediction for any time series with respect to the ground truth test data, we define the following helper:"
      ],
      "metadata": {
        "id": "g_Y-E3ksQ76J"
      },
      "id": "g_Y-E3ksQ76J"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.dates as mdates\n",
        "import pandas as pd\n",
        "\n",
        "test_ds = list(test_dataset)\n",
        "\n",
        "def plot(ts_index):\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    index = pd.period_range(\n",
        "        start=test_ds[ts_index][FieldName.START],\n",
        "        periods=len(test_ds[ts_index][FieldName.TARGET]),\n",
        "        freq=test_ds[ts_index][FieldName.START].freq,\n",
        "    ).to_timestamp()\n",
        "\n",
        "    ax.plot(\n",
        "        index[-5*prediction_length:],\n",
        "        test_ds[ts_index][\"target\"][-5*prediction_length:],\n",
        "        label=\"actual\",\n",
        "    )\n",
        "\n",
        "    plt.plot(\n",
        "        index[-prediction_length:],\n",
        "        np.median(forecasts[ts_index], axis=0),\n",
        "        label=\"median\",\n",
        "    )\n",
        "\n",
        "    plt.gcf().autofmt_xdate()\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jfNGe7JHQ-Hx"
      },
      "id": "jfNGe7JHQ-Hx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, for time-series in the test set with index `4`:"
      ],
      "metadata": {
        "id": "cSbS8tmeRAue"
      },
      "id": "cSbS8tmeRAue"
    },
    {
      "cell_type": "code",
      "source": [
        "plot(4)"
      ],
      "metadata": {
        "id": "3es7ZmasRCQA"
      },
      "id": "3es7ZmasRCQA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on DLinear\n",
        "\n",
        "A probabilistic DLinear is implemented in `gluonts` and thus we can train and evaluate it relatively quickly here:\n"
      ],
      "metadata": {
        "id": "LmHDpw_bRDbz"
      },
      "id": "LmHDpw_bRDbz"
    },
    {
      "cell_type": "code",
      "source": [
        "from gluonts.torch.model.d_linear.estimator import DLinearEstimator\n",
        "\n",
        "# Define the DLinear model with the same parameters as the Autoformer model\n",
        "estimator = DLinearEstimator(\n",
        "    prediction_length=dataset.metadata.prediction_length,\n",
        "    context_length=dataset.metadata.prediction_length*2,\n",
        "    scaling=scaling,\n",
        "    hidden_dimension=2,\n",
        "\n",
        "    batch_size=batch_size,\n",
        "    num_batches_per_epoch=num_batches_per_epoch,\n",
        "    trainer_kwargs=dict(max_epochs=epochs)\n",
        ")"
      ],
      "metadata": {
        "id": "Iknrptv1RKvF"
      },
      "id": "Iknrptv1RKvF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model:\n"
      ],
      "metadata": {
        "id": "e6BYGFrORJa4"
      },
      "id": "e6BYGFrORJa4"
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = estimator.train(\n",
        "    training_data=train_dataset,\n",
        "    cache_data=True,\n",
        "    shuffle_buffer_length=1024\n",
        ")"
      ],
      "metadata": {
        "id": "JC4EjzRdRNqg"
      },
      "id": "JC4EjzRdRNqg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And evaluate it on the test set:"
      ],
      "metadata": {
        "id": "jVIo4koHRPDL"
      },
      "id": "jVIo4koHRPDL"
    },
    {
      "cell_type": "code",
      "source": [
        "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
        "\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=dataset.test,\n",
        "    predictor=predictor,\n",
        ")\n",
        "\n",
        "d_linear_forecasts = list(forecast_it)\n",
        "d_linear_tss = list(ts_it)\n",
        "\n",
        "evaluator = Evaluator()\n",
        "\n",
        "agg_metrics, _ = evaluator(iter(d_linear_tss), iter(d_linear_forecasts))"
      ],
      "metadata": {
        "id": "HOSIs3pbRRGj"
      },
      "id": "HOSIs3pbRRGj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the result for the DLinear model is:"
      ],
      "metadata": {
        "id": "HEWwzvWERRpL"
      },
      "id": "HEWwzvWERRpL"
    },
    {
      "cell_type": "code",
      "source": [
        "dlinear_mase = agg_metrics[\"MASE\"]\n",
        "print(f\"DLinear MASE: {dlinear_mase:.3f}\")"
      ],
      "metadata": {
        "id": "LleL1nj0RVNH"
      },
      "id": "LleL1nj0RVNH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we plot the predictions from our trained DLinear model via this helper:"
      ],
      "metadata": {
        "id": "j-f82ZJwRVmk"
      },
      "id": "j-f82ZJwRVmk"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_gluonts(index):\n",
        "    plt.plot(d_linear_tss[index][-4 * dataset.metadata.prediction_length:].to_timestamp(), label=\"target\")\n",
        "    d_linear_forecasts[index].plot(show_label=True,  color='g')\n",
        "    plt.legend()\n",
        "    plt.gcf().autofmt_xdate()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JCg4Lxs5RX5S"
      },
      "id": "JCg4Lxs5RX5S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_gluonts(4)"
      ],
      "metadata": {
        "id": "jTcIxzldRYWC"
      },
      "id": "jTcIxzldRYWC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `traffic` dataset has a distributional shift in the sensor patterns between weekdays and weekends. So what is going on here? Since the DLinear model has no capacity to incorporate covariates, in particular any date-time features, the context window we give it does not have enough information to figure out if the prediction is for the weekend or weekday. Thus, the model will predict the more common of the patterns, namely the weekdays leading to poorer performance on weekends. Of course, by giving it a larger context window, a linear model will figure out the weekly pattern, but perhaps there is a monthly or quarterly pattern in the data which would require bigger and bigger contexts."
      ],
      "metadata": {
        "id": "2BbAbAxiRZ4S"
      },
      "id": "2BbAbAxiRZ4S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "How do Transformer-based models compare against the above linear baseline? The test set MASE metrics from the different models we have are below:\n",
        "\n",
        "|Dataset | \t Transformer (uni.) |   \t Transformer (mv.)  | Informer (uni.)| Informer (mv.) | Autoformer (uni.) | DLinear |\n",
        "|:--:|:--:| :--:| :--:| :--:|  :--:|  :--:|\n",
        "|`Traffic` \t| **0.876** | 1.046 | 0.924 | 1.131  | 0.910 | 0.969 |\n",
        "\n",
        "As one can observe, the [vanilla Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer) which we introduced last year gets the best results here. Secondly, multivariate models are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations/relationships. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. Recent papers like [CrossFormer](https://openreview.net/forum?id=vSVLM2j9eie) (ICLR 23) and [CARD](https://arxiv.org/abs/2305.12095) try to address this problem in Transformer models.\n",
        "Multivariate models usually perform well when trained on large amounts of data. However, when compared to univariate models, especially on smaller open datasets, the univariate models tend to provide better metrics. By comparing the linear model with equivalent-sized univariate transformers or in fact any other neural univariate model, one will typically get better performance.\n",
        "\n",
        "To summarize, Transformers are definitely far from being outdated when it comes to time-series forcasting!\n",
        "Yet the availability of large-scale datasets is crucial for maximizing their potential.\n",
        "Unlike in CV and NLP, the field of time series lacks publicly accessible large-scale datasets.\n",
        "Most existing pre-trained models for time series are trained on small sample sizes from archives like [UCR and UEA](https://www.timeseriesclassification.com/),\n",
        "which contain only a few thousands or even hundreds of samples.\n",
        "Although these benchmark datasets have been instrumental in the progress of the time series community,\n",
        "their limited sample sizes and lack of generality pose challenges for pre-training deep learning models.\n",
        "\n",
        "Therefore, the development of large-scale, generic time series datasets (like ImageNet in CV) is of the utmost importance.\n",
        "Creating such datasets will greatly facilitate further research on pre-trained models specifically designed for time series analysis.\n",
        "and will improve the applicability of pre-trained models in time series forecasting.\n",
        "\n",
        "## Acknowledgements\n",
        "We express our appreciation to [Lysandre Debut](https://github.com/LysandreJik) and [Pedro Cuenca](https://github.com/pcuenca)\n",
        "their insightful comments and help during this project â¤ï¸.\n",
        "\n"
      ],
      "metadata": {
        "id": "WMNZspStRdh6"
      },
      "id": "WMNZspStRdh6"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}