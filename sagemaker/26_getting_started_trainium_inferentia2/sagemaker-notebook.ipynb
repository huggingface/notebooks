{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating Hugging Face Transformers with AWS Accelerators and Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will help you get started on how to train and deploy Hugging Face Transformers on Amazon SageMaker using AWS Accelerators, including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia2](https://aws.amazon.com/machine-learning/inferentia/). As the field of deep learning continues to evolve, the need for efficient and cost-effective solutions to train and deploy increasingly complex transformers model has become more critical than ever. AWS purpose-built accelerators are designed to deliver high performance at the lowest cost for deep learning inference and training.\n",
    "\n",
    "This notebook walks you through an end-to-end example on how to train a RoBERTa model with Hugging Face on AWS Trainium, and deploy it on AWS SageMaker using AWS Inferentia2 accelerators for inference. Benefit from faster time-to-train, up to 50% cost-to-train savings, and up to 4x higher throughput and 10x lower latency for inference compared to its first-generation.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. Setup AWS environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-tune RoBERTa using Hugging Face Transformers and Optimum Neuron on AWS Trainium\n",
    "4. Deploy model to inferntia2 and run inference \n",
    "\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers>=4.28.0\" \"datasets[s3]==2.9.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `emotion` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [emotion](https://github.com/dair-ai/emotion_dataset) dataset consists of 16000 training examples, 2000 validation examples, and 2000 testing examples.\n",
    "\n",
    "```python\n",
    "{\n",
    "  'text': 'im feeling rather festive here in south florida', \n",
    "  'label': 1\n",
    "}\n",
    "```\n",
    "\n",
    "To load theÂ `emotion`Â dataset, we use theÂ `load_dataset()`Â method from the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/ubuntu/.cache/huggingface/datasets/philschmid___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98197fec3e84e319bd6b4f525a6c13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 16000\n",
      "Validation dataset size: 2000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"philschmid/emotion\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset['validation'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we need to convert our inputs (text) to token IDs. This is done by a ðŸ¤— Transformers Tokenizer. If you are not sure what this means, check outÂ **[chapter 6](https://huggingface.co/course/chapter6/1?fw=tf)**Â of the Hugging Face Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 07:40:26.787782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 07:40:27.399962: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-02 07:40:27.622595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-02 07:40:27.622608: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-02 07:40:29.439889: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 07:40:29.439944: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 07:40:29.439949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id=\"roberta-base\"\n",
    "\n",
    "# Load tokenizer of RoBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Trainium requires the inputs to be of a static shape. To optimize our training throughput want to understand how long our inputs are to efficiently pad them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/philschmid___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-64679c6b492027f9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 88\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=[\"text\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our max sequence length is 87. We are going to use 128 as our max sequence length for training and inference and pad all inputs to this length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0bd063247c4121923a284caf7173c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/philschmid___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-a4f094e327cece66.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/philschmid___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-b3c48eb89cf1fc2f.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length',max_length=MAX_LENGTH, truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "# rename label to labels to match the expected input\n",
    "train_dataset =  tokenized_dataset['train'].rename_column(\"label\", \"labels\")\n",
    "validation_dataset =  tokenized_dataset['validation'].rename_column(\"label\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets print another sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(1), 'input_ids': tensor([    0,   118,  2198,  1346,    14,    51,   115,  3999,    33,    41,\n",
      "         3031, 24672,    53,  1782,    24,    95, 10122,    15, 19750,     5,\n",
      "          619,     9,     5,   157,   626,   278,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "print(tokenized_dataset['train'][randint(0, len(dataset['train']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537cb1a78fb4470c845e323be21c7ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3371d12bbf94b1084f74129405b8fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/startup-loft/train'\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/startup-loft/test'\n",
    "validation_dataset.save_to_disk(test_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine-tune RoBERTa using Hugging Face Transformers and Optimum Neuron on AWS Trainium\n",
    "\n",
    "Normally we would use theÂ [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)Â andÂ [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)Â to fine-tune PyTorch-based transformer models. \n",
    "\n",
    "But together with AWS, we have developed a `TrainiumTrainer` to improve performance, robustness, and safety when training on Trainium instances. The `TrainiumTrainer` also comes with a [model cache](https://huggingface.co/docs/optimum-neuron/guides/cache_system), which allows us to use precompiled models and configuration from Hugging Face Hub to skip the compilation step, which would be needed at the beginning of training. This can reduce the training time by ~3x. \n",
    "\n",
    "The `TrainiumTrainer` is part of the `optimum-neuron` library and can be used as a 1-to-1 replacement for the `Trainer`. You only have to adjust the import in your training script. \n",
    "\n",
    "```diff\n",
    "- from transformers import Trainer\n",
    "+ from optimum.neuron import TrainiumTrainer as Trainer\n",
    "```\n",
    "\n",
    "We prepared a simple [train.py](./scripts/train.py) training script based on the [\"Getting started with Pytorch 2.0 and Hugging Face Transformersâ€](https://www.philschmid.de/getting-started-pytorch-2-0-transformers#3-fine-tune--evaluate-bert-model-with-the-hugging-face-trainer) blog post with the `TrainiumTrainier`.\n",
    "\n",
    "In order to train on Amazon SageMaker we need to create a `HuggingFace` Estimator. The Estimator defines, which fine-tuning script (`entry_point`), which `instance_type`, which `hyperparameters`, etc should be used.\n",
    "\n",
    "Amazon SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainium_image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training-neuronx:1.13.0-transformers4.28.1-neuronx-py38-sdk2.9.1-ubuntu20.04-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-fsdp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_id': model_id, # model id from huggingface.co/models\n",
    "    'lr': 5e-5, # enable gradient checkpointing\n",
    "    'bf16': True, # enable mixed precision training\n",
    "    'per_device_train_batch_size': 16, # optimizer\n",
    "    'epochs': 3, # number of epochs to train\n",
    "}\n",
    "\n",
    "# estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train_with_export.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.trn1.2xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=200,\n",
    "    role=role,\n",
    "    job_name=job_name,\n",
    "    image_uri=trainium_image_uri,\n",
    "    py_version='py38',\n",
    "    hyperparameters = hyperparameters,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows how a model is trained and deployed with Amazon SageMaker:\n",
    "![assets](./assets/platform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy model to inferntia2 and run inference \n",
    "\n",
    "Now that we have trained our model, we want to deploy it to `inferentia2` on Amazon SageMaker so that we can use it for inference. When deploying models to `inferentia2`, we need to compile the model with the `neuron-sdk` or `optimum-neuron`. \n",
    "\n",
    "If you want to learn more about how to compile models for `inferentia2`, check out the [Optimum Neuron documentation](https://huggingface.co/docs/optimum-neuron/guides/export_model). \n",
    "\n",
    "As first we need to install `optimum-neuron` and the required neuron runtime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for amazon linux 2\n",
    "# !sudo yum install aws-neuronx-runtime-lib-2.* -y\n",
    "# for ubuntu 20.04\n",
    "!sudo apt-get install aws-neuronx-runtime-lib=2.12.* -y \n",
    "!pip install optimum optimum-neuron==0.0.3 --upgrade\n",
    "!pip install neuronx-cc==2.5.* torch-neuronx==1.13.0.1.6.1 --extra-index-url https://pip.repos.neuron.amazonaws.com --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load our trained model from S3 and compile it for `inferentia2`. We are using the `export()` function from `optimum-neuron` to compile our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.neuron import export\n",
    "from optimum.exporters.neuron.model_configs import RobertaNeuronConfig\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from tempfile import TemporaryDirectory\n",
    "import shutil\n",
    "\n",
    "with TemporaryDirectory() as tmp_dir:\n",
    "    # S3Downloader.download(model_data, tmp_dir)\n",
    "    S3Downloader.download(huggingface_estimator.model_data, tmp_dir)\n",
    "    shutil.unpack_archive(f\"{tmp_dir}/model.tar.gz\", tmp_dir)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(tmp_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tmp_dir)\n",
    "\n",
    "    neuron_config = RobertaNeuronConfig(config=model.config,\n",
    "                                           task=\"text-classification\",\n",
    "                                           batch_size=1, \n",
    "                                           sequence_length=128\n",
    "                                           )\n",
    "    output_path = Path(f\"tmp\")\n",
    "    # Export to Neuron model\n",
    "    export(\n",
    "        model=model,\n",
    "        config=neuron_config,\n",
    "        output=output_path.joinpath(\"neuron_model.pt\"),\n",
    "        auto_cast=\"all\",\n",
    "        auto_cast_type=\"bf16\",\n",
    "    )\n",
    "    model.config.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TheÂ [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit)Â supports zero-code deployments on top of theÂ [pipelineÂ feature](https://huggingface.co/transformers/main_classes/pipelines.html)Â from ðŸ¤— Transformers. This allows users to deploy Hugging Face transformers without an inference script [[Example](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)].\n",
    "\n",
    "Currently, this feature is not supported with AWS Inferentia2, which means we need to provide anÂ `inference.py`Â script for running inference.\n",
    "\n",
    "To use the inference script, we need to create anÂ `inference.py`Â script. In our example, we are going to overwrite theÂ `model_fn`Â to load our neuron model and theÂ `predict_fn`Â to create a text-classification pipeline.\n",
    "\n",
    "If you want to know more about theÂ `inference.py`Â script check out thisÂ **[example](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb)**. It explains amongst other things whatÂ `model_fn`Â andÂ `predict_fn`Â are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "# saved weights name\n",
    "TRACED_WEIGHTS_NAME = \"neuron_model.pt\"\n",
    "TRACED_SEQUENCE_LEGNTH = 128\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"1\"\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load tokenizer and neuron model from model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = torch.jit.load(os.path.join(model_dir, TRACED_WEIGHTS_NAME))\n",
    "    model_config = AutoConfig.from_pretrained(model_dir)\n",
    "\n",
    "    return model, tokenizer, model_config\n",
    "\n",
    "\n",
    "def predict_fn(data, model_tokenizer_model_config):\n",
    "    # destruct model, tokenizer and model config\n",
    "    model, tokenizer, model_config = model_tokenizer_model_config\n",
    "\n",
    "    # create embeddings for inputs\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    embeddings = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=TRACED_SEQUENCE_LEGNTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    # convert to tuple for neuron model\n",
    "    neuron_inputs = tuple(embeddings.values())\n",
    "\n",
    "    # run prediciton\n",
    "    with torch.no_grad():\n",
    "        predictions = model(*neuron_inputs)[0]\n",
    "        scores = torch.nn.Softmax(dim=1)(predictions)\n",
    "\n",
    "    # return dictonary, which will be json serializable\n",
    "    return [{\"label\": model_config.id2label[item.argmax().item()], \"score\": item.max().item()} for item in scores]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can deploy our neuron model to Amazon SageMaker we need to create a `model.tar.gz` archive with all our model artifacts and inference script. We can do this with the `tar` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy inference.py into the code/ directory of the model directory.\n",
    "!cp -r code/ tmp/code/\n",
    "# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd tmp\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upload our `model.tar.gz` to our session S3 bucket with sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import os \n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-2\"\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/startup-loft/compiled-models\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have uploaded ourÂ `model.tar.gz`Â to Amazon S3 can we create a customÂ `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n",
    "\n",
    "When we create the endpoint, SageMaker automatically provisions the specified inference instances and deploys our model to them. We can then send inference requests to the endpoint and receive predictions from our model. We can use the `deploy()` method from  our HuggingFace estimator, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferentia2_image_uri=\"763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.0-transformers4.28.1-neuronx-py38-sdk2.9.1-ubuntu20.04-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,       # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.12\",  # transformers version used\n",
    "   image_uri=inferentia2_image_uri,\n",
    "   py_version='py37',            # python version used\n",
    "   model_server_workers=2,\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model via neuron-cc\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.xlarge\" # AWS Inferentia2 Instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferentia2 is the second generation purpose built Machine Learning inference accelerator from AWS. The Inferentia2 device architecture is depicted below:\n",
    "\n",
    "![assets](./assets/inferentia2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_input = {\"inputs\":\"I love using the new Inferentia2 instance on Amazon SageMaker.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "\n",
    "predictor = HuggingFacePredictor(endpoint_name=\"huggingface-pytorch-inference-neuronx-m-2023-05-10-14-09-36-983\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to deploy our neuron compiled RoBERTa to AWS Inferentia2 on Amazon SageMaker. Now, let's test its performance. As a dummy load test, we will loop and send 5,000 requests to our endpoint and inspect the performance in cloudwatch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.aws.amazon.com/cloudwatch/home?region=us-east-2#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'huggingface-pytorch-inference-neuronx-m-2023-05-10-14-09-36-983~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'us-east-2~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20huggingface-pytorch-inference-neuronx-m-2023-05-10-14-09-36-983\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://console.aws.amazon.com/cloudwatch/home?region={sess.boto_region_name}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'{predictor.endpoint_name}~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'{sess.boto_region_name}~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [06:59<00:00, 11.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_requests = 5_000  # 1m requests\n",
    "for i in tqdm(range(total_requests)):\n",
    "    predictor.predict(sentiment_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average latency for our RoBERTA model is 1-2ms for a sequence length of 128.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up running endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
