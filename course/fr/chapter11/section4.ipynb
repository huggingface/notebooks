{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6LLOPZouLg"
   },
   "source": [
    "# Comment finetuner des LLMs avec des adaptateurs LoRA en utilisant Hugging Face TRL\n",
    "\n",
    "Ce *notebook* montre comment finetuner efficacement de grands mod√®les de langage en utilisant des adaptateurs LoRA (*Low-Rank Adaptation*). LoRA est une technique de finetuning efficace en termes de param√®tres qui :\n",
    "- g√®le les poids du mod√®le pr√©-entra√Æn√©\n",
    "- ajoute aux couches d'attention de petites matrices de d√©composition de rangs entra√Ænables\n",
    "- R√©duit g√©n√©ralement les param√®tres entra√Ænables d'environ 90%\n",
    "- Maintient les performances du mod√®le tout en √©tant √©conome en m√©moire\n",
    "\n",
    "Nous aborderons les points suivants\n",
    "1. Mise en place de l'environnement de d√©veloppement et configuration de LoRA\n",
    "2. Cr√©er et pr√©parer le jeu de donn√©es pour l'entra√Ænement de l'adaptateur\n",
    "3. Finetuner en utilisant `trl` et `SFTTrainer` avec les adaptateurs LoRA\n",
    "4. Tester le mod√®le et fusionner les adaptateurs (optionnel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqd9BXgouLi"
   },
   "source": [
    "## 1. Configurer l'environnement de d√©veloppement\n",
    "\n",
    "Notre premi√®re √©tape consiste √† installer les bibliothques d'Hugging Face et Pytorch, y compris trl, les transformers et les datasets. Si vous n'avez pas encore entendu parler de trl, ne vous inqui√©tez pas. Il s'agit d'une nouvelle biblioth√®que au-dessus des transformers et des datasets permetant de finetuner, rlhf, aligner les LLMs ouverts plus facilement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKvGVxImouLi"
   },
   "outputs": [],
   "source": [
    "# Installer les pr√©requis dans Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# S'authentifier sur Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# Pour plus de facilit√©, vous pouvez cr√©er une variable d'environnement contenant votre jeton de hub sous la forme HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHUzfwpKouLk"
   },
   "source": [
    "## 2. Charger le jeu de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z4p6Bvo7ouLk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger un √©chantillon de jeu de donn√©es\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO : d√©finir votre jeu de donn√©es et votre configuration en utilisant les param√®tres path et name\n",
    "dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOhJdtsouLk"
   },
   "source": [
    "## 3. Fine-tune LLM using `trl` and the `SFTTrainer` with LoRA\n",
    "\n",
    "The [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` provides integration with LoRA adapters through the [PEFT](https://huggingface.co/docs/peft/en/index) library. Key advantages of this setup include:\n",
    "\n",
    "1. **Memory Efficiency**: \n",
    "   - Only adapter parameters are stored in GPU memory\n",
    "   - Base model weights remain frozen and can be loaded in lower precision\n",
    "   - Enables fine-tuning of large models on consumer GPUs\n",
    "\n",
    "2. **Training Features**:\n",
    "   - Native PEFT/LoRA integration with minimal setup\n",
    "   - Support for QLoRA (Quantized LoRA) for even better memory efficiency\n",
    "\n",
    "3. **Adapter Management**:\n",
    "   - Adapter weight saving during checkpoints\n",
    "   - Features to merge adapters back into base model\n",
    "\n",
    "We'll use LoRA in our example, which combines LoRA with 4-bit quantization to further reduce memory usage without sacrificing performance. The setup requires just a few configuration steps:\n",
    "1. Define the LoRA configuration (rank, alpha, dropout)\n",
    "2. Create the SFTTrainer with PEFT config\n",
    "3. Train and save the adapter weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les biblioth√®ques n√©cessaires\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Charger le mod√®le et le tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# D√©finir le format de chat\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# D√©finir le nom du finetuning √† sauvegarder et/ou √† t√©l√©charger\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbuVArTHouLk"
   },
   "source": [
    "`SFTTrainer` supporte une int√©gration native avec `peft`, ce qui rend super facile le finetuning des LLMs en utilisant, par exemple, LoRA. Nous avons seulement besoin de cr√©er notre `LoraConfig` et de le fournir au Trainer.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercice : D√©finir les param√®tres de LoRA pour le finetuning</h2>\n",
    "    <p>Prenez un jeu de donn√©es provenant du Hub d'Hugging Face et finetun√© un mod√®le sur dessus. </p> \n",
    "    <p><b>Niveaux de difficult√©</b></p>\n",
    "    <p>üê¢ Utiliser les param√®tres g√©n√©raux pour un finetuning arbitraire</p>\n",
    "    <p>üêï Ajuster les param√®tres et v√©rifier les poids et les biais</p>\n",
    "    <p>ü¶Å Ajuster les param√®tres et montrer les changements dans les r√©sultats de l'inf√©rence</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO : Configurer les param√®tres de LoRA\n",
    "# r : dimension du rang des matrices LoRA (plus petite = plus de compression)\n",
    "rank_dimension = 6\n",
    "# lora_alpha : facteur d'√©chelle pour les couches LoRA (plus √©lev√© = adaptation plus forte)\n",
    "lora_alpha = 8\n",
    "# lora_dropout : probabilit√© de dropout pour les couches LoRA (aide √† pr√©venir le surentra√Ænement)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Dimension du rang, g√©n√©ralement entre 4 et 32\n",
    "    lora_alpha=lora_alpha,  # Facteur d'√©chelle LoRA, g√©n√©ralement 2x le rang\n",
    "    lora_dropout=lora_dropout,  # Probabilit√© de dropout probability pour les couches de LoRA\n",
    "    bias=\"none\",  # Type de biais pour le LoRA. Les biais correspondants seront mis √† jour pendant l'entra√Ænement\n",
    "    target_modules=\"all-linear\",  # Modules auxquels appliquer le LoRA\n",
    "    task_type=\"CAUSAL_LM\",  # Type de t√¢che pour l'architecture du mod√®le\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5NUDPcaouLl"
   },
   "source": [
    "Avant de commencer notre entra√Ænement, nous devons d√©finir les hyperparam√®tres (`TrainingArguments`) que nous voulons utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [],
   "source": [
    "# Configuration de l'entra√Ænement\n",
    "# Hyperparam√®tres bas√©s sur les recommandations du papier du QLoRA\n",
    "args = SFTConfig(\n",
    "    # Param√®tres de sortie\n",
    "    output_dir=finetune_name,  # R√©pertoire pour enregistrer les checkpoints du mod√®le\n",
    "    # Dur√©e de l'entra√Ænement\n",
    "    num_train_epochs=1,  # Nombre d'√©poques d'entra√Ænement\n",
    "    # Param√®tres de la taille des batchs\n",
    "    per_device_train_batch_size=2,  # Taille des batchs par GPU\n",
    "    gradient_accumulation_steps=2,  # Accumuler les gradients pour obtenir un plus grand batch efficace\n",
    "    # Optimisation de la m√©moire\n",
    "    gradient_checkpointing=True,  # √âchanger le calcul contre des √©conomies de m√©moire\n",
    "    # Param√®tres de l'optimiseur\n",
    "    optim=\"adamw_torch_fused\",  # Utiliser AdamW fusionn√© pour plus d'efficacit√©\n",
    "    learning_rate=2e-4,  # Taux d'apprentissage (papier du QLoRA)\n",
    "    max_grad_norm=0.3,  # Seuil d'√©cr√™tage du gradient\n",
    "    # Taux d'apprentissage\n",
    "    warmup_ratio=0.03,  # Portion de pas pour l'√©chauffement\n",
    "    lr_scheduler_type=\"constant\",  # Maintenir un rythme d'apprentissage constant apr√®s l'√©chauffement\n",
    "    # Enregistrement et sauvegarde\n",
    "    logging_steps=10,  # Enregistrement des m√©triques tous les N pas\n",
    "    save_strategy=\"epoch\",  # Sauvegarde du checkpoint √† chaque √©poque\n",
    "    # Param√®tres de pr√©cision\n",
    "    bf16=True,  # Utiliser la pr√©cision bfloat16\n",
    "    # Param√®tres d'int√©gration\n",
    "    push_to_hub=False,  # Ne pas pousser vers le Hub\n",
    "    report_to=\"none\",  # D√©sactiver l'enregistrement externe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGhR7uFBouLl"
   },
   "source": [
    "Nous avons maintenant tous les √©l√©ments n√©cessaires pour cr√©er notre `SFTTrainer` et commencer √† entra√Æner notre mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M00Har2douLl"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 1512  # longueur maximale des s√©quences pour le mod√®le et le paquetage du jeu de donn√©es\n",
    "\n",
    "# Cr√©er SFTTrainer avec la configuration LoRA\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,  # Configuration LoRA\n",
    "    max_seq_length=max_seq_length,  # Longueur maximale de la s√©quence\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,  # Activer l'emballage d'entr√©e pour plus d'efficacit√©\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # Tokens sp√©ciaux g√©r√©s par le gabarit\n",
    "        \"append_concat_token\": False,  # Aucun s√©parateur suppl√©mentaire n'est n√©cessaire\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_kRN24ouLl"
   },
   "source": [
    "Commencez √† entra√Æner notre mod√®le en appelant la m√©thode `train()` sur notre instance `Trainer`. Cela va d√©marrer la boucle d'entra√Ænement et entra√Æner notre mod√®le pendant 3 √©poques. Puisque nous utilisons une m√©thode PEFT, nous ne sauvegarderons que les poids du mod√®le adapt√© et non le mod√®le complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq4nIYqKouLl"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300e5dfbb4b54750b77324345c7591f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=1.6402628521124523, metrics={'train_runtime': 195.2398, 'train_samples_per_second': 1.485, 'train_steps_per_second': 0.369, 'total_flos': 282267289092096.0, 'train_loss': 1.6402628521124523, 'epoch': 0.993103448275862})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# commencer l'entra√Ænement, le mod√®le sera automatiquement sauvegard√© sur le Hub et dans le r√©pertoire de sortie.\n",
    "trainer.train()\n",
    "\n",
    "# Sauvegarder le mod√®le\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4HHSYYzouLl"
   },
   "source": [
    "L'entra√Ænement avec Flash Attention pour 3 √©poques avec un jeu de donn√©es de 15k √©chantillons a pris 4:14:36 sur un `g5.2xlarge`. L'instance co√ªte `1.21$/h` ce qui nous am√®ne √† un co√ªt total de seulement ~`5.3$`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C309KsXjouLl"
   },
   "source": [
    "### Fusionner l'adaptateur LoRA dans le mod√®le original\n",
    "\n",
    "Lors de l'utilisation de LoRA, nous n'entra√Ænons que les poids de l'adaptateur tout en gardant le mod√®le de base gel√©. Pendant l'entra√Ænement, nous sauvegardons uniquement ces poids d'adaptateur l√©gers (~2-10MB) plut√¥t qu'une copie compl√®te du mod√®le. Cependant, pour le d√©ploiement, vous pouvez vouloir fusionner les adaptateurs dans le mod√®le de base pour :\n",
    "\n",
    "1. **D√©ploiement simplifi√©** : Fichier de mod√®le unique au lieu du mod√®le de base + adaptateurs\n",
    "2. **Vitesse d'inf√©rence** : Pas de surcharge de calcul des adaptateurs\n",
    "3. **Compatibilit√© avec les frameworks** : Meilleure compatibilit√© avec les frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "# Chargement du mod√®le PEFT sur le CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Fusionner le mod√®le LoRA et le mod√®le de base et sauvegarder\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yO6E9quouLl"
   },
   "source": [
    "## 3. Tester le mod√®le et ex√©cuter l'inf√©rence\n",
    "\n",
    "Une fois l'entra√Ænement termin√©, nous voulons tester notre mod√®le. Nous allons charger diff√©rents √©chantillons du jeu de donn√©es original et √©valuer le mod√®le sur ces √©chantillons, en utilisant une boucle simple et l'*accuracy* comme m√©trique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercice bonus : Chargement de l'adaptateur LoRA</h2>\n",
    "    <p>Utilisez ce que vous avez appris dans le notebook pour charger votre adaptateur LoRA entra√Æn√© pour l'inf√©rence</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "I5B494OdouLl"
   },
   "outputs": [],
   "source": [
    "# lib√©rer la m√©moire\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1UhohVdouLl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Chargement du mod√®le avec l'adaptateur PEFT\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=merged_model, tokenizer=tokenizer, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99uFDAuuouLl"
   },
   "source": [
    "Testons quelques √©chantillons d'instructions et voyons comment le mod√®le se comporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-shSmUbvouLl",
    "outputId": "16d97c61-3b31-4040-c780-3c4de75c3824"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"What is the difference between a fruit and a vegetable? Give examples of each.\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
