{"cells":[{"cell_type":"markdown","metadata":{"id":"3fWUyFq31i8c"},"source":["# Entra√Æner un mod√®le de langage causal de z√©ro (TensorFlow)\n","\n","Ici nous entra√Ænons un mod√®le √† g√©n√©rer du code Python. Le Python utilisant des fonctions bas√©es sur des mots anglais, nous gardons un gpt-2 anglais dans l'optique d'obtenir de meilleures performances que ce que l'on pourrait s'attendre en utilisant un gpt-2 en fran√ßais."]},{"cell_type":"markdown","metadata":{"id":"o00Kesm41i8i"},"source":["Installez les biblioth√®ques ü§ó *Datasets* et ü§ó *Transformers* pour ex√©cuter ce *notebook*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"woZMLy_Y1i8m"},"outputs":[],"source":["!pip install datasets transformers[sentencepiece]\n","!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"pM3PZiS01i8q"},"source":["Vous aurez besoin de configurer git, adaptez votre email et votre nom dans la cellule suivante."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xy4iA02k1i8t"},"outputs":[],"source":["!git config --global user.email \"you@example.com\"\n","!git config --global user.name \"Your Name\""]},{"cell_type":"markdown","metadata":{"id":"NtabcJoO1i8w"},"source":["Vous devrez √©galement √™tre connect√© au Hub d'Hugging Face. Ex√©cutez ce qui suit et entrez vos informations d'identification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onVQiVBk1i8y"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHHi8kOU1i80"},"outputs":[],"source":["def any_keyword_in_string(string, keywords):\n","    for keyword in keywords:\n","        if keyword in string:\n","            return True\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmXQkhZJ1i83"},"outputs":[],"source":["filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n","example_1 = \"import numpy as np\"\n","example_2 = \"import pandas as pd\"\n","\n","print(\n","    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_r8DSR4Z1i87"},"outputs":[],"source":["from collections import defaultdict\n","from tqdm import tqdm\n","from datasets import Dataset\n","\n","\n","def filter_streaming_dataset(dataset, filters):\n","    filtered_dict = defaultdict(list)\n","    total = 0\n","    for sample in tqdm(iter(dataset)):\n","        total += 1\n","        if any_keyword_in_string(sample[\"content\"], filters):\n","            for k, v in sample.items():\n","                filtered_dict[k].append(v)\n","    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n","    return Dataset.from_dict(filtered_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sslYgwJ1i8-"},"outputs":[],"source":["# Cette cellule prendra beaucoup de temps √† s'ex√©cuter, donc vous devriez la sauter et aller √† la suivante !\n","from datasets import load_dataset\n","\n","split = \"train\"  # \"valid\"\n","filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n","\n","data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n","filtered_data = filter_streaming_dataset(data, filters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqbotDaQ1i9C"},"outputs":[],"source":["from datasets import load_dataset, DatasetDict\n","\n","ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n","ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n","\n","raw_datasets = DatasetDict(\n","    {\n","        \"train\": ds_train,  # .shuffle().select(range(50000)),\n","        \"valid\": ds_valid,  # .shuffle().select(range(500))\n","    }\n",")\n","\n","raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2rHdsuY1i9F"},"outputs":[],"source":["for key in raw_datasets[\"train\"][0]:\n","    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qihsnRG1i9H"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","context_length = 128\n","tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n","\n","outputs = tokenizer(\n","    raw_datasets[\"train\"][:2][\"content\"],\n","    truncation=True,\n","    max_length=context_length,\n","    return_overflowing_tokens=True,\n","    return_length=True,\n",")\n","\n","print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n","print(f\"Input chunk lengths: {(outputs['length'])}\")\n","print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCozdbMp1i9J"},"outputs":[],"source":["def tokenize(element):\n","    outputs = tokenizer(\n","        element[\"content\"],\n","        truncation=True,\n","        max_length=context_length,\n","        return_overflowing_tokens=True,\n","        return_length=True,\n","    )\n","    input_batch = []\n","    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n","        if length == context_length:\n","            input_batch.append(input_ids)\n","    return {\"input_ids\": input_batch}\n","\n","\n","tokenized_datasets = raw_datasets.map(\n","    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",")\n","tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89SBmIRk1i9M"},"outputs":[],"source":["from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n","\n","config = AutoConfig.from_pretrained(\n","    \"gpt2\",\n","    vocab_size=len(tokenizer),\n","    n_ctx=context_length,\n","    bos_token_id=tokenizer.bos_token_id,\n","    eos_token_id=tokenizer.eos_token_id,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kkNZGnv1i9O"},"outputs":[],"source":["model = TFGPT2LMHeadModel(config)\n","model(model.dummy_inputs)  # Construit le mod√®le\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFNvO17A1i9R"},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waKdL0nD1i9S"},"outputs":[],"source":["out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n","for key in out:\n","    print(f\"{key} shape: {out[key].shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-YgICD21i9T"},"outputs":[],"source":["tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    collate_fn=data_collator,\n","    shuffle=True,\n","    batch_size=32,\n",")\n","tf_eval_dataset = tokenized_dataset[\"valid\"].to_tf_dataset(\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    collate_fn=data_collator,\n","    shuffle=False,\n","    batch_size=32,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYdbDiQB1i9V"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kftp9PHW1i9W"},"outputs":[],"source":["from transformers import create_optimizer\n","import tensorflow as tf\n","\n","num_train_steps = len(tf_train_dataset)\n","optimizer, schedule = create_optimizer(\n","    init_lr=5e-5,\n","    num_warmup_steps=1_000,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n",")\n","model.compile(optimizer=optimizer)\n","\n","# Entra√Æner en mixed-precision float16\n","tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"coKUr5Pf1i9X"},"outputs":[],"source":["from transformers.keras_callbacks import PushToHubCallback\n","\n","callback = PushToHubCallback(output_dir=\"codeparrot-ds\", tokenizer=tokenizer)\n","\n","model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9J69EKkI1i9Y"},"outputs":[],"source":["from transformers import pipeline\n","\n","course_model = TFGPT2LMHeadModel.from_pretrained(\"huggingface-course/codeparrot-ds\")\n","course_tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/codeparrot-ds\")\n","pipe = pipeline(\n","    \"text-generation\", model=course_model, tokenizer=course_tokenizer, device=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTR2iAOa1i9a"},"outputs":[],"source":["txt = \"\"\"\\\n","# create some data\n","x = np.random.randn(100)\n","y = np.random.randn(100)\n","\n","# create scatter plot with x, y\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_hSO98A1i9b"},"outputs":[],"source":["txt = \"\"\"\\\n","# create some data\n","x = np.random.randn(100)\n","y = np.random.randn(100)\n","\n","# create dataframe from x and y\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmUipKiX1i9e"},"outputs":[],"source":["txt = \"\"\"\\\n","# dataframe with profession, income and name\n","df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n","\n","# calculate the mean income per profession\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcgR0Fry1i9g"},"outputs":[],"source":["txt = \"\"\"\n","# import random forest regressor from scikit-learn\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# fit random forest model with 300 estimators on X, y:\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}