{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplejidad de los modelos de longitud fija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La perplejidad, perplexity en ingl칠s (PPL), es una de las m칠tricas m치s comunes para evaluar modelos de lenguaje. Antes de sumergirnos, debemos tener en cuenta que esta m칠trica se aplica espec칤ficamente a modelos de lenguaje cl치sicos (a veces llamados modelos autorregresivos o causales) y no est치 bien definida para modelos de lenguaje enmascarados como BERT (ver [resumen del modelo](https://huggingface.co/docs/transformers/main/es/model_summary)).\n",
    "\n",
    "La perplejidad se define como la media negativa exponenciada del log-likelihood de una secuencia. Si tenemos una secuencia tokenizada $X = (x_0, x_1, \\dots, x_t)$, entonces la perplejidad de $X$ es,\n",
    "\n",
    "$$\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{<i}) } \\right\\}$$\n",
    "\n",
    "donde $\\log p_\\theta (x_i|x_{<i})$ es el log-likelihood del token i-칠simo condicionado a los tokens precedentes $x_{<i}$ seg칰n nuestro modelo. De manera intuitiva, se puede pensar en esto como una evaluaci칩n de la capacidad del modelo para predecir de manera uniforme entre el conjunto de tokens especificados en un corpus. Es importante destacar que el procedimiento de tokenizaci칩n tiene un impacto directo en la perplejidad de un modelo, lo cual siempre debe tenerse en cuenta al comparar diferentes modelos.\n",
    "\n",
    "Esto tambi칠n es equivalente a la exponenciaci칩n de la entrop칤a cruzada entre los datos y las predicciones del modelo. Para obtener m치s intuici칩n sobre la perplejidad y su relaci칩n con los Bits Por Car치cter (BPC) y la compresi칩n de datos, echa un vistazo a esta [fant치stica publicaci칩n en el blog de \"The Gradient\"](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C치lculo de PPL con modelos de longitud fija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no estuvi칠ramos limitados por el tama침o del contexto de un modelo, evaluar칤amos la perplejidad (PPL) del modelo auto regresivamente factorizando una secuencia y condicion치ndonos en toda la subsecuencia precedente en cada paso, como se muestra a continuaci칩n.\n",
    "\n",
    "<img width=\"600\" alt=\"Full decomposition of a sequence with unlimited context length\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_full.gif\"/>\n",
    "\n",
    "Sin embargo, al trabajar con modelos aproximados, generalmente tenemos una restricci칩n en la cantidad de tokens que el modelo puede procesar. La versi칩n m치s grande de [GPT-2](https://huggingface.co/docs/transformers/main/es/model_doc/gpt2), por ejemplo, tiene una longitud fija de 1024 tokens, por lo que no podemos calcular $p_\\theta(x_t|x_{<t})$ directamente cuando $t$ es mayor que 1024.\n",
    "\n",
    "En cambio, la secuencia se divide t칤picamente en subsecuencias iguales al tama침o m치ximo de entrada del modelo. Si el tama침o m치ximo de entrada, de un modelo es $k$, entonces aproximamos la probabilidad de un token $x_t$ condicion치ndonos solo en los $k-1$ tokens que lo preceden en lugar de todo el contexto. Al evaluar la perplejidad del modelo en una secuencia, un enfoque tentador pero sub 칩ptimo es dividir la secuencia en fragmentos independientes y sumar los log-likelihood descompuestos de cada segmento de manera independiente.\n",
    "\n",
    "<img width=\"600\" alt=\"Suboptimal PPL not taking advantage of full available context\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_chunked.gif\"/>\n",
    "\n",
    "Esto es r치pido de calcular, ya que la perplejidad de cada segmento se puede calcular en un solo pase hacia adelante, pero sirve como una aproximaci칩n pobre de la perplejidad completamente factorizada y generalmente dar치 como resultado una PPL m치s alta (peor) porque el modelo tendr치 menos contexto en la mayor칤a de los pasos de predicci칩n.\n",
    "\n",
    "En cambio, la PPL de modelos de longitud fija deber칤a evaluarse con una estrategia de ventana deslizante. Esto implica deslizar repetidamente la ventana de contexto para que el modelo tenga m치s contexto al hacer cada predicci칩n.\n",
    "\n",
    "<img width=\"600\" alt=\"Sliding window PPL taking advantage of all available context\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif\"/>\n",
    "\n",
    "Esta es una aproximaci칩n m치s cercana a la verdadera descomposici칩n de la probabilidad de la secuencia y generalmente dar치 como resultado una puntuaci칩n m치s favorable. La desventaja es que requiere un pase hacia adelante separado para cada token en el corpus. Un buen compromiso pr치ctico es emplear una ventana deslizante estratificada, moviendo el contexto con pasos m치s grandes en lugar de deslizarse de 1 token a la vez. Esto permite que la computaci칩n avance mucho m치s r치pido, mientras le da al modelo un contexto amplio para hacer\n",
    "predicciones en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: C치lculo de la perplejidad con GPT-2 en 游뱅 Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demostremos este proceso con GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "device = \"cuda\"\n",
    "model_id = \"openai-community/gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos el conjunto de datos WikiText-2 y evaluemos la perplejidad utilizando algunas estrategias de ventana deslizante diferentes. Dado que este conjunto de datos es peque침o y solo estamos realizando un pase hacia adelante sobre el conjunto, podemos cargar y codificar todo el conjunto de datos en la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con 游뱅 Transformers, simplemente podemos pasar los `input_ids` como las `labels` a nuestro modelo, y la media negativa del log-likelihood para cada token se devuelve como la p칠rdida. Sin embargo, con nuestro enfoque de ventana deslizante, hay superposici칩n en los tokens que pasamos al modelo en cada iteraci칩n. No queremos que el log-likelihood de los tokens que estamos tratando solo como contexto se incluya en nuestra p칠rdida, por lo que podemos establecer estos objetivos en `-100` para que se ignoren. El siguiente es un ejemplo de c칩mo podr칤amos hacer esto con un paso de `512`. Esto significa que el modelo tendr치 al menos `512` tokens como contexto al calcular el log-likelihood condicional de cualquier token (siempre que haya `512` tokens precedentes disponibles para condicionar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # puede ser diferente del paso en el 칰ltimo bucle\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # la p칠rdida se calcula utilizando CrossEntropyLoss, que promedia las etiquetas v치lidas\n",
    "        # N.B. el modelo solo calcula la p칠rdida sobre trg_len - 1 etiquetas, porque desplaza las etiqueta internamente\n",
    "        # a la izquierda por 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuta esto con la longitud de paso igual a la longitud m치xima de entrada es equivalente a la estrategia sub 칩ptima,\n",
    "sin ventana deslizante, que discutimos anteriormente. Cuanto menor sea el paso, m치s contexto tendr치 el modelo para\n",
    "realizar cada predicci칩n y, por lo general, mejor ser치 la perplejidad informada.\n",
    "\n",
    "Cuando ejecutamos lo anterior con `stride = 1024`, es decir, sin superposici칩n, la PPL resultante es `19.44`, que es\n",
    "aproximadamente la misma que la `19.93` informada en el art칤culo de GPT-2. Al utilizar `stride = 512` y, por lo tanto,\n",
    "emplear nuestra estrategia de ventana deslizante, esto disminuye a `16.45`. Esto no solo es una puntuaci칩n m치s favorable, sino que se calcula de una manera m치s cercana a la verdadera descomposici칩n autorregresiva de la probabilidad de una secuencia."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
