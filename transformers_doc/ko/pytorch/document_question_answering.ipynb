{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ(Document Question Answering) [[document_question_answering]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¬¸ì„œ ì‹œê°ì  ì§ˆì˜ ì‘ë‹µ(Document Visual Question Answering)ì´ë¼ê³ ë„ í•˜ëŠ”\n",
    "ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ(Document Question Answering)ì€ ë¬¸ì„œ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€ì„ ì£¼ëŠ” íƒœìŠ¤í¬ì…ë‹ˆë‹¤.\n",
    "ì´ íƒœìŠ¤í¬ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ì˜ ì…ë ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì˜ ì¡°í•©ì´ê³ , ì¶œë ¥ì€ ìì—°ì–´ë¡œ ëœ ë‹µë³€ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ í…ìŠ¤íŠ¸, ë‹¨ì–´ì˜ ìœ„ì¹˜(ë°”ìš´ë”© ë°•ìŠ¤), ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê°€ì´ë“œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ì„¤ëª…í•©ë‹ˆë‹¤:\n",
    "\n",
    "- [DocVQA dataset](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut)ì„ ì‚¬ìš©í•´ [LayoutLMv2](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/layoutlmv2) ë¯¸ì„¸ ì¡°ì •í•˜ê¸°\n",
    "- ì¶”ë¡ ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ì´ ì‘ì—…ê³¼ í˜¸í™˜ë˜ëŠ” ëª¨ë“  ì•„í‚¤í…ì²˜ì™€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë³´ë ¤ë©´ [ì‘ì—… í˜ì´ì§€](https://huggingface.co/tasks/image-to-text)ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "LayoutLMv2ëŠ” í† í°ì˜ ë§ˆì§€ë§‰ ì€ë‹‰ì¸µ ìœ„ì— ì§ˆì˜ ì‘ë‹µ í—¤ë“œë¥¼ ì¶”ê°€í•´ ë‹µë³€ì˜ ì‹œì‘ í† í°ê³¼ ë í† í°ì˜ ìœ„ì¹˜ë¥¼ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨ ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ íƒœìŠ¤í¬ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì¦‰, ë¬¸ë§¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¶”ì¶œí˜• ì§ˆì˜ ì‘ë‹µ(Extractive question answering)ìœ¼ë¡œ ë¬¸ì œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "ë¬¸ë§¥ì€ OCR ì—”ì§„ì˜ ì¶œë ¥ì—ì„œ ê°€ì ¸ì˜¤ë©°, ì—¬ê¸°ì„œëŠ” Googleì˜ Tesseractë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. LayoutLMv2ëŠ” detectron2, torchvision ë° í…Œì„œë™íŠ¸ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "```bash\n",
    "pip install -q transformers datasets\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "pip install torchvision\n",
    "```\n",
    "\n",
    "```bash\n",
    "sudo apt install tesseract-ocr\n",
    "pip install -q pytesseract\n",
    "```\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ëª¨ë‘ ì„¤ì¹˜í•œ í›„ ëŸ°íƒ€ì„ì„ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì»¤ë®¤ë‹ˆí‹°ì— ë‹¹ì‹ ì˜ ëª¨ë¸ì„ ê³µìœ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•´ì„œ ëª¨ë¸ì„ ğŸ¤— Hubì— ì—…ë¡œë“œí•˜ì„¸ìš”.\n",
    "í”„ë¡¬í”„íŠ¸ê°€ ì‹¤í–‰ë˜ë©´, ë¡œê·¸ì¸ì„ ìœ„í•´ í† í°ì„ ì…ë ¥í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª‡ ê°€ì§€ ì „ì—­ ë³€ìˆ˜ë¥¼ ì •ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° [[load-the-data]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ê°€ì´ë“œì—ì„œëŠ” ğŸ¤— Hubì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì „ì²˜ë¦¬ëœ DocVQAì˜ ì‘ì€ ìƒ˜í”Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "DocVQAì˜ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´, [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17)ì— ê°€ì… í›„ ë‹¤ìš´ë¡œë“œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œ í–ˆë‹¤ë©´, ì´ ê°€ì´ë“œë¥¼ ê³„ì† ì§„í–‰í•˜ê¸° ìœ„í•´ [ğŸ¤— datasetì— íŒŒì¼ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•](https://huggingface.co/docs/datasets/loading#local-and-remote-files)ì„ í™•ì¸í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nielsr/docvqa_1200_examples\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë³´ì‹œë‹¤ì‹œí”¼, ë°ì´í„° ì„¸íŠ¸ëŠ” ì´ë¯¸ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ë¬´ì‘ìœ„ë¡œ ì˜ˆì œë¥¼ ì‚´í´ë³´ë©´ì„œ íŠ¹ì„±ì„ í™•ì¸í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê° í•„ë“œê°€ ë‚˜íƒ€ë‚´ëŠ” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "* `id`: ì˜ˆì œì˜ id\n",
    "* `image`: ë¬¸ì„œ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ëŠ” PIL.Image.Image ê°ì²´\n",
    "* `query`: ì§ˆë¬¸ ë¬¸ìì—´ - ì—¬ëŸ¬ ì–¸ì–´ì˜ ìì—°ì–´ë¡œ ëœ ì§ˆë¬¸\n",
    "* `answers`: ì‚¬ëŒì´ ì£¼ì„ì„ ë‹¨ ì •ë‹µ ë¦¬ìŠ¤íŠ¸\n",
    "* `words` and `bounding_boxes`: OCRì˜ ê²°ê³¼ê°’ë“¤ì´ë©° ì´ ê°€ì´ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì„ ì˜ˆì •\n",
    "* `answer`: ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ëŠ” ë‹µë³€ì´ë©° ì´ ê°€ì´ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì„ ì˜ˆì •\n",
    "\n",
    "ì˜ì–´ë¡œ ëœ ì§ˆë¬¸ë§Œ ë‚¨ê¸°ê³  ë‹¤ë¥¸ ëª¨ë¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í¬í•¨í•˜ëŠ” `answer` íŠ¹ì„±ì„ ì‚­ì œí•˜ê² ìŠµë‹ˆë‹¤.\n",
    "ê·¸ë¦¬ê³  ì£¼ì„ ì‘ì„±ìê°€ ì œê³µí•œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì²« ë²ˆì§¸ ë‹µë³€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ë˜ëŠ” ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œì„ ì¶”ì¶œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = dataset.map(lambda example: {\"question\": example[\"query\"][\"en\"]}, remove_columns=[\"query\"])\n",
    "updated_dataset = updated_dataset.map(\n",
    "    lambda example: {\"answer\": example[\"answers\"][0]}, remove_columns=[\"answer\", \"answers\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ê°€ì´ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” LayoutLMv2 ì²´í¬í¬ì¸íŠ¸ëŠ” `max_position_embeddings = 512`ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤(ì´ ì •ë³´ëŠ” [ì²´í¬í¬ì¸íŠ¸ì˜ `config.json` íŒŒì¼](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤).\n",
    "ë°”ë¡œ ì˜ˆì œë¥¼ ì˜ë¼ë‚¼ ìˆ˜ë„ ìˆì§€ë§Œ, ê¸´ ë¬¸ì„œì˜ ëì— ë‹µë³€ì´ ìˆì–´ ì˜ë¦¬ëŠ” ìƒí™©ì„ í”¼í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì„ë² ë”©ì´ 512ë³´ë‹¤ ê¸¸ì–´ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ëª‡ ê°€ì§€ ì˜ˆì œë¥¼ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "ë°ì´í„° ì„¸íŠ¸ì— ìˆëŠ” ëŒ€ë¶€ë¶„ì˜ ë¬¸ì„œê°€ ê¸´ ê²½ìš° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ - ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ì‹¶ìœ¼ë©´ ì´ [ë…¸íŠ¸ë¶](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ì„ í™•ì¸í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = updated_dataset.filter(lambda x: len(x[\"words\"]) + len(x[\"question\"].split()) < 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ì‹œì ì—ì„œ ì´ ë°ì´í„° ì„¸íŠ¸ì˜ OCR íŠ¹ì„±ë„ ì œê±°í•´ ë³´ê² ìŠµë‹ˆë‹¤. OCR íŠ¹ì„±ì€ ë‹¤ë¥¸ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•œ ê²ƒìœ¼ë¡œ, ì´ ê°€ì´ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì˜ ì…ë ¥ ìš”êµ¬ ì‚¬í•­ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì¼ë¶€ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "ëŒ€ì‹ , ì›ë³¸ ë°ì´í„°ì— `LayoutLMv2Processor`ë¥¼ ì‚¬ìš©í•˜ì—¬ OCR ë° í† í°í™”ë¥¼ ëª¨ë‘ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ì…ë ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì´ë¯¸ì§€ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë ¤ë©´, [`LayoutLMv2` model documentation](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/layoutlmv2)ì—ì„œ ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ì…ë ¥ í¬ë§·ì„ í™•ì¸í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = updated_dataset.remove_columns(\"words\")\n",
    "updated_dataset = updated_dataset.remove_columns(\"bounding_boxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ, ë°ì´í„° íƒìƒ‰ì„ ì™„ë£Œí•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ ì˜ˆì‹œë¥¼ ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset[\"train\"][11][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/docvqa_example.jpg\" alt=\"DocVQA Image Example\"/>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì „ì²˜ë¦¬ [[preprocess-the-data]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ íƒœìŠ¤í¬ëŠ” ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬ì´ë©°, ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ì…ë ¥ì´ ëª¨ë¸ì˜ ìš”êµ¬ì— ë§ê²Œ ì „ì²˜ë¦¬ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì™€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ìˆëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ê²°í•©í•œ `LayoutLMv2Processor`ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒë¶€í„° ì‹œì‘í•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¬¸ì„œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ [[preprocessing-document-images]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¨¼ì €, í”„ë¡œì„¸ì„œì˜ `image_processor`ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì— ëŒ€í•œ ë¬¸ì„œ ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "ê¸°ë³¸ê°’ìœ¼ë¡œ, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 224x224ë¡œ ì¡°ì •í•˜ê³  ìƒ‰ìƒ ì±„ë„ì˜ ìˆœì„œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•œ í›„ ë‹¨ì–´ì™€ ì •ê·œí™”ëœ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì–»ê¸° ìœ„í•´ í…Œì„œë™íŠ¸ë¥¼ ì‚¬ìš©í•´ OCRë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ìš°ë¦¬ê°€ í•„ìš”í•œ ê²ƒê³¼ ê¸°ë³¸ê°’ì€ ì™„ì „íˆ ë™ì¼í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ë°°ì¹˜ì— ê¸°ë³¸ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ì ìš©í•˜ê³  OCRì˜ ê²°ê³¼ë¥¼ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = processor.image_processor\n",
    "\n",
    "\n",
    "def get_ocr_words_and_boxes(examples):\n",
    "    images = [image.convert(\"RGB\") for image in examples[\"image\"]]\n",
    "    encoded_inputs = image_processor(images)\n",
    "\n",
    "    examples[\"image\"] = encoded_inputs.pixel_values\n",
    "    examples[\"words\"] = encoded_inputs.words\n",
    "    examples[\"boxes\"] = encoded_inputs.boxes\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ì „ì²˜ë¦¬ë¥¼ ë°ì´í„° ì„¸íŠ¸ ì „ì²´ì— ë¹ ë¥´ê²Œ ì ìš©í•˜ë ¤ë©´ `map`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ [[preprocessing-text-data]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ì— OCRì„ ì ìš©í–ˆìœ¼ë©´ ë°ì´í„° ì„¸íŠ¸ì˜ í…ìŠ¤íŠ¸ ë¶€ë¶„ì„ ëª¨ë¸ì— ë§ê²Œ ì¸ì½”ë”©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ì´ ì¸ì½”ë”©ì—ëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ê°€ì ¸ì˜¨ ë‹¨ì–´ì™€ ë°•ìŠ¤ë¥¼ í† í° ìˆ˜ì¤€ì˜ `input_ids`, `attention_mask`, `token_type_ids` ë° `bbox`ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì´ í¬í•¨ë©ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ë ¤ë©´ í”„ë¡œì„¸ì„œì˜ `tokenizer`ê°€ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì „ì²˜ë¦¬ ì™¸ì—ë„ ëª¨ë¸ì„ ìœ„í•´ ë ˆì´ë¸”ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ğŸ¤— Transformersì˜ `xxxForQuestionAnswering` ëª¨ë¸ì˜ ê²½ìš°, ë ˆì´ë¸”ì€ `start_positions`ì™€ `end_positions`ë¡œ êµ¬ì„±ë˜ë©° ì–´ë–¤ í† í°ì´ ë‹µë³€ì˜ ì‹œì‘ê³¼ ëì— ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "ë ˆì´ë¸” ì¶”ê°€ë¥¼ ìœ„í•´ì„œ, ë¨¼ì € ë” í° ë¦¬ìŠ¤íŠ¸(ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸)ì—ì„œ í•˜ìœ„ ë¦¬ìŠ¤íŠ¸(ë‹¨ì–´ë¡œ ë¶„í• ëœ ë‹µë³€)ì„ ì°¾ì„ ìˆ˜ ìˆëŠ” í—¬í¼ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ í•¨ìˆ˜ëŠ” `words_list`ì™€ `answer_list`, ì´ë ‡ê²Œ ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "ê·¸ëŸ° ë‹¤ìŒ `words_list`ë¥¼ ë°˜ë³µí•˜ì—¬ `words_list`ì˜ í˜„ì¬ ë‹¨ì–´(words_list[i])ê°€ `answer_list`ì˜ ì²« ë²ˆì§¸ ë‹¨ì–´(answer_list[0])ì™€ ê°™ì€ì§€,\n",
    "í˜„ì¬ ë‹¨ì–´ì—ì„œ ì‹œì‘í•´ `answer_list`ì™€ ê°™ì€ ê¸¸ì´ë§Œí¼ì˜ `words_list`ì˜ í•˜ìœ„ ë¦¬ìŠ¤íŠ¸ê°€ `answer_list`ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "ì´ ì¡°ê±´ì´ ì°¸ì´ë¼ë©´ ì¼ì¹˜í•˜ëŠ” í•­ëª©ì„ ë°œê²¬í–ˆìŒì„ ì˜ë¯¸í•˜ë©°, í•¨ìˆ˜ëŠ” ì¼ì¹˜ í•­ëª©, ì‹œì‘ ì¸ë±ìŠ¤(idx) ë° ì¢…ë£Œ ì¸ë±ìŠ¤(idx + len(answer_list) - 1)ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤. ì¼ì¹˜í•˜ëŠ” í•­ëª©ì´ ë‘ ê°œ ì´ìƒ ë°œê²¬ë˜ë©´ í•¨ìˆ˜ëŠ” ì²« ë²ˆì§¸ í•­ëª©ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤. ì¼ì¹˜í•˜ëŠ” í•­ëª©ì´ ì—†ë‹¤ë©´ í•¨ìˆ˜ëŠ” (`None`, 0, 0)ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subfinder(words_list, answer_list):\n",
    "    matches = []\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for idx, i in enumerate(range(len(words_list))):\n",
    "        if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:\n",
    "            matches.append(answer_list)\n",
    "            start_indices.append(idx)\n",
    "            end_indices.append(idx + len(answer_list) - 1)\n",
    "    if matches:\n",
    "        return matches[0], start_indices[0], end_indices[0]\n",
    "    else:\n",
    "        return None, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ì •ë‹µì˜ ìœ„ì¹˜ë¥¼ ì°¾ëŠ”ì§€ ì„¤ëª…í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì˜ˆì œì—ì„œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question:  Who is in  cc in this letter?\n",
       "Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'development', 'internal', 'correspondence', 'to:', 'r.', 'h.', 'honeycutt', 'ce:', 't.f.', 'riehl', 'from:', '.', 'c.j.', 'cook', 'date:', 'may', '8,', '1995', 'subject:', 'review', 'of', 'existing', 'brainstorming', 'ideas/483', 'the', 'major', 'function', 'of', 'the', 'product', 'innovation', 'graup', 'is', 'to', 'develop', 'marketable', 'nove!', 'products', 'that', 'would', 'be', 'profitable', 'to', 'manufacture', 'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from', 'anything', 'seen', 'or', 'known', 'before.', 'innovation', 'is', 'defined', 'as:', 'something', 'new', 'or', 'different', 'introduced;', 'act', 'of', 'innovating;', 'introduction', 'of', 'new', 'things', 'or', 'methods.', 'the', 'products', 'may', 'incorporate', 'the', 'latest', 'technologies,', 'materials', 'and', 'know-how', 'available', 'to', 'give', 'then', 'a', 'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', 'and', 'categorize', 'a', 'list', 'of', 'existing', 'brainstorming', 'ideas.', 'ideas', 'were', 'grouped', 'into', 'two', 'major', 'categories', 'labeled', 'appearance', 'and', 'taste/aroma.', 'these', 'categories', 'are', 'used', 'for', 'novel', 'products', 'that', 'may', 'differ', 'from', 'a', 'visual', 'and/or', 'taste/aroma', 'point', 'of', 'view', 'compared', 'to', 'canventional', 'cigarettes.', 'other', 'categories', 'include', 'a', 'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appearance', 'this', 'category', 'is', 'used', 'for', 'novel', 'cigarette', 'constructions', 'that', 'yield', 'visually', 'different', 'products', 'with', 'minimal', 'changes', 'in', 'smoke', 'chemistry', 'two', 'cigarettes', 'in', 'cne.', 'emulti-plug', 'te', 'build', 'yaur', 'awn', 'cigarette.', 'eswitchable', 'menthol', 'or', 'non', 'menthol', 'cigarette.', '*cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'unburned', 'section', 'for', 'future', 'smoking.', 'Â«short', 'cigarette,', 'tobacco', 'section', '30', 'mm.', 'Â«extremely', 'fast', 'buming', 'cigarette.', 'Â«novel', 'cigarette', 'constructions', 'that', 'permit', 'a', 'significant', 'reduction', 'iretobacco', 'weight', 'while', 'maintaining', 'smoking', 'mechanics', 'and', 'visual', 'characteristics.', 'higher', 'basis', 'weight', 'paper:', 'potential', 'reduction', 'in', 'tobacco', 'weight.', 'Â«more', 'rigid', 'tobacco', 'column;', 'stiffing', 'agent', 'for', 'tobacco;', 'e.g.', 'starch', '*colored', 'tow', 'and', 'cigarette', 'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'or', 'in', 'an', 'ebony', 'and', 'ivory', 'brand', 'containing', 'a', 'mixture', 'of', 'all', 'black', '(black', 'paper', 'and', 'tow)', 'and', 'ail', 'white', 'cigarettes.', '499150498']\n",
       "Answer:  T.F. Riehl\n",
       "start_index 17\n",
       "end_index 18"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset_with_ocr[\"train\"][1]\n",
    "words = [word.lower() for word in example[\"words\"]]\n",
    "match, word_idx_start, word_idx_end = subfinder(words, example[\"answer\"].lower().split())\n",
    "print(\"Question: \", example[\"question\"])\n",
    "print(\"Words:\", words)\n",
    "print(\"Answer: \", example[\"answer\"])\n",
    "print(\"start_index\", word_idx_start)\n",
    "print(\"end_index\", word_idx_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•œí¸, ìœ„ ì˜ˆì œê°€ ì¸ì½”ë”©ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œë©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CLS] who is in cc in this letter? [SEP] wie baw brown & williamson tobacco corporation research & development ..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(example[\"question\"], example[\"words\"], example[\"boxes\"])\n",
    "tokenizer.decode(encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì¸ì½”ë”©ëœ ì…ë ¥ì—ì„œ ì •ë‹µì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
    "* `token_type_ids`ëŠ” ì–´ë–¤ í† í°ì´ ì§ˆë¬¸ì— ì†í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì–´ë–¤ í† í°ì´ ë¬¸ì„œì˜ ë‹¨ì–´ì— í¬í•¨ë˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.\n",
    "* `tokenizer.cls_token_id` ì…ë ¥ì˜ ì‹œì‘ ë¶€ë¶„ì— ìˆëŠ” íŠ¹ìˆ˜ í† í°ì„ ì°¾ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n",
    "* `word_ids`ëŠ” ì›ë³¸ `words`ì—ì„œ ì°¾ì€ ë‹µë³€ì„ ì „ì²´ ì¸ì½”ë”©ëœ ì…ë ¥ì˜ ë™ì¼í•œ ë‹µê³¼ ì¼ì¹˜ì‹œí‚¤ê³  ì¸ì½”ë”©ëœ ì…ë ¥ì—ì„œ ë‹µë³€ì˜ ì‹œì‘/ë ìœ„ì¹˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "ìœ„ ë‚´ìš©ë“¤ì„ ì—¼ë‘ì— ë‘ê³  ë°ì´í„° ì„¸íŠ¸ ì˜ˆì œì˜ ë°°ì¹˜ë¥¼ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(examples, max_length=512):\n",
    "    questions = examples[\"question\"]\n",
    "    words = examples[\"words\"]\n",
    "    boxes = examples[\"boxes\"]\n",
    "    answers = examples[\"answer\"]\n",
    "\n",
    "    # ì˜ˆì œ ë°°ì¹˜ë¥¼ ì¸ì½”ë”©í•˜ê³  start_positionsì™€ end_positionsë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
    "    encoding = tokenizer(questions, words, boxes, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # ë°°ì¹˜ì˜ ì˜ˆì œë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤\n",
    "    for i in range(len(questions)):\n",
    "        cls_index = encoding[\"input_ids\"][i].index(tokenizer.cls_token_id)\n",
    "\n",
    "        # ì˜ˆì œì˜ wordsì—ì„œ ë‹µë³€ì˜ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤\n",
    "        words_example = [word.lower() for word in words[i]]\n",
    "        answer = answers[i]\n",
    "        match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())\n",
    "\n",
    "        if match:\n",
    "            # ì¼ì¹˜í•˜ëŠ” í•­ëª©ì„ ë°œê²¬í•˜ë©´, `token_type_ids`ë¥¼ ì‚¬ìš©í•´ ì¸ì½”ë”©ì—ì„œ ë‹¨ì–´ê°€ ì‹œì‘í•˜ëŠ” ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤\n",
    "            token_type_ids = encoding[\"token_type_ids\"][i]\n",
    "            token_start_index = 0\n",
    "            while token_type_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(encoding[\"input_ids\"][i]) - 1\n",
    "            while token_type_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            word_ids = encoding.word_ids(i)[token_start_index : token_end_index + 1]\n",
    "            start_position = cls_index\n",
    "            end_position = cls_index\n",
    "\n",
    "            # wordsì˜ ë‹µë³€ ìœ„ì¹˜ì™€ ì¼ì¹˜í•  ë•Œê¹Œì§€ word_idsë¥¼ ë°˜ë³µí•˜ê³  `token_start_index`ë¥¼ ëŠ˜ë¦½ë‹ˆë‹¤\n",
    "            # ì¼ì¹˜í•˜ë©´ `token_start_index`ë¥¼ ì¸ì½”ë”©ì—ì„œ ë‹µë³€ì˜ `start_position`ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤\n",
    "            for id in word_ids:\n",
    "                if id == word_idx_start:\n",
    "                    start_position = token_start_index\n",
    "                else:\n",
    "                    token_start_index += 1\n",
    "\n",
    "            # ë¹„ìŠ·í•˜ê²Œ, ëì—ì„œ ì‹œì‘í•´ `word_ids`ë¥¼ ë°˜ë³µí•˜ë©° ë‹µë³€ì˜ `end_position`ì„ ì°¾ìŠµë‹ˆë‹¤\n",
    "            for id in word_ids[::-1]:\n",
    "                if id == word_idx_end:\n",
    "                    end_position = token_end_index\n",
    "                else:\n",
    "                    token_end_index -= 1\n",
    "\n",
    "            start_positions.append(start_position)\n",
    "            end_positions.append(end_position)\n",
    "\n",
    "        else:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "\n",
    "    encoding[\"image\"] = examples[\"image\"]\n",
    "    encoding[\"start_positions\"] = start_positions\n",
    "    encoding[\"end_positions\"] = end_positions\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì´ ì „ì²˜ë¦¬ í•¨ìˆ˜ê°€ ìˆìœ¼ë‹ˆ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_dataset = dataset_with_ocr[\"train\"].map(\n",
    "    encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr[\"train\"].column_names\n",
    ")\n",
    "encoded_test_dataset = dataset_with_ocr[\"test\"].map(\n",
    "    encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr[\"test\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¸ì½”ë”©ëœ ë°ì´í„° ì„¸íŠ¸ì˜ íŠ¹ì„±ì´ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'start_positions': Value(dtype='int64', id=None),\n",
       " 'end_positions': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í‰ê°€ [[evaluation]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µì„ í‰ê°€í•˜ë ¤ë©´ ìƒë‹¹í•œ ì–‘ì˜ í›„ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‹œê°„ì´ ë„ˆë¬´ ë§ì´ ê±¸ë¦¬ì§€ ì•Šë„ë¡ ì´ ê°€ì´ë“œì—ì„œëŠ” í‰ê°€ ë‹¨ê³„ë¥¼ ìƒëµí•©ë‹ˆë‹¤.\n",
    "[Trainer](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer)ê°€ í›ˆë ¨ ê³¼ì •ì—ì„œ í‰ê°€ ì†ì‹¤(evaluation loss)ì„ ê³„ì† ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëŒ€ëµì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì¶”ì¶œì (Extractive) ì§ˆì˜ ì‘ë‹µì€ ë³´í†µ F1/exact match ë°©ë²•ì„ ì‚¬ìš©í•´ í‰ê°€ë©ë‹ˆë‹¤.\n",
    "ì§ì ‘ êµ¬í˜„í•´ë³´ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´, Hugging Face courseì˜ [Question Answering chapter](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)ì„ ì°¸ê³ í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í›ˆë ¨ [[train]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶•í•˜í•©ë‹ˆë‹¤! ì´ ê°€ì´ë“œì˜ ê°€ì¥ ì–´ë ¤ìš´ ë¶€ë¶„ì„ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìœ¼ë‹ˆ ì´ì œ ë‚˜ë§Œì˜ ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "í›ˆë ¨ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤:\n",
    "* ì „ì²˜ë¦¬ì—ì„œì˜ ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ [AutoModelForDocumentQuestionAnswering](https://huggingface.co/docs/transformers/main/ko/model_doc/auto#transformers.AutoModelForDocumentQuestionAnswering)ìœ¼ë¡œ ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "* [TrainingArguments](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.TrainingArguments)ë¡œ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •í•©ë‹ˆë‹¤.\n",
    "* ì˜ˆì œë¥¼ ë°°ì¹˜ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” [DefaultDataCollator](https://huggingface.co/docs/transformers/main/ko/main_classes/data_collator#transformers.DefaultDataCollator)ê°€ ì ë‹¹í•©ë‹ˆë‹¤.\n",
    "* ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, ë°ì´í„° ì½œë ˆì´í„°(Data collator)ì™€ í•¨ê»˜ [Trainer](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer)ì— í›ˆë ¨ ì¸ìˆ˜ë“¤ì„ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "* [train()](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer.train)ì„ í˜¸ì¶œí•´ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForDocumentQuestionAnswering\n",
    "\n",
    "model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TrainingArguments](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.TrainingArguments)ì—ì„œ `output_dir`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ê³ , ì ì ˆí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "ëª¨ë¸ì„ ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•˜ë ¤ë©´ `push_to_hub`ë¥¼ `True`ë¡œ ì„¤ì •í•˜ì„¸ìš” (ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ Hugging Faceì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤).\n",
    "ì´ ê²½ìš° `output_dir`ì€ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ í‘¸ì‹œí•  ë ˆí¬ì§€í† ë¦¬ì˜ ì´ë¦„ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# ë³¸ì¸ì˜ ë ˆí¬ì§€í† ë¦¬ IDë¡œ ë°”ê¾¸ì„¸ìš”\n",
    "repo_id = \"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repo_id,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=20,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°„ë‹¨í•œ ë°ì´í„° ì½œë ˆì´í„°ë¥¼ ì •ì˜í•˜ì—¬ ì˜ˆì œë¥¼ í•¨ê»˜ ë°°ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ, ëª¨ë“  ê²ƒì„ í•œ ê³³ì— ëª¨ì•„ [train()](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer.train)ì„ í˜¸ì¶œí•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    processing_class=processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìµœì¢… ëª¨ë¸ì„ ğŸ¤— Hubì— ì¶”ê°€í•˜ë ¤ë©´, ëª¨ë¸ ì¹´ë“œë¥¼ ìƒì„±í•˜ê³  `push_to_hub`ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.create_model_card()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ë¡  [[inference]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ LayoutLMv2 ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê³  ğŸ¤— Hubì— ì—…ë¡œë“œí–ˆìœ¼ë‹ˆ ì¶”ë¡ ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì¶”ë¡ ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ ë³´ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ `Pipeline`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is â€˜presidingâ€™ TRRF GENERAL SESSION (PART 1)?'\n",
       "['TRRF Vice President', 'lee a. waller']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"test\"][2]\n",
    "question = example[\"query\"][\"en\"]\n",
    "image = example[\"image\"]\n",
    "print(question)\n",
    "print(example[\"answers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ ë‹¤ìŒ, ëª¨ë¸ë¡œ ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µì„ í•˜ê¸° ìœ„í•´ íŒŒì´í”„ë¼ì¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì´ë¯¸ì§€ + ì§ˆë¬¸ ì¡°í•©ì„ ì „ë‹¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9949808120727539,\n",
       "  'answer': 'Lee A. Waller',\n",
       "  'start': 55,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"document-question-answering\", model=\"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\")\n",
    "qa_pipeline(image, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì›í•œë‹¤ë©´ íŒŒì´í”„ë¼ì¸ì˜ ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n",
    "1. ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì„ ê°€ì ¸ì™€ ëª¨ë¸ì˜ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì— ë§ê²Œ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "2. ëª¨ë¸ì„ í†µí•´ ê²°ê³¼ ë˜ëŠ” ì „ì²˜ë¦¬ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "3. ëª¨ë¸ì€ ì–´ë–¤ í† í°ì´ ë‹µë³€ì˜ ì‹œì‘ì— ìˆëŠ”ì§€, ì–´ë–¤ í† í°ì´ ë‹µë³€ì´ ëì— ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” `start_logits`ì™€ `end_logits`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ë‘˜ ë‹¤ (batch_size, sequence_length) í˜•íƒœë¥¼ ê°–ìŠµë‹ˆë‹¤.\n",
    "4. `start_logits`ì™€ `end_logits`ì˜ ë§ˆì§€ë§‰ ì°¨ì›ì„ ìµœëŒ€ë¡œ ë§Œë“œëŠ” ê°’ì„ ì°¾ì•„ ì˜ˆìƒ `start_idx`ì™€ `end_idx`ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n",
    "5. í† í¬ë‚˜ì´ì €ë¡œ ë‹µë³€ì„ ë””ì½”ë”©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lee a. waller'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "from transformers import AutoModelForDocumentQuestionAnswering\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\")\n",
    "model = AutoModelForDocumentQuestionAnswering.from_pretrained(\"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoding = processor(image.convert(\"RGB\"), question, return_tensors=\"pt\")\n",
    "    outputs = model(**encoding)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    predicted_start_idx = start_logits.argmax(-1).item()\n",
    "    predicted_end_idx = end_logits.argmax(-1).item()\n",
    "\n",
    "processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
