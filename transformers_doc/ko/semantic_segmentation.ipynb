{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì˜ë¯¸ì  ë¶„í• (Semantic segmentation)[[semantic-segmentation]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dKE8SIt9C-w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dKE8SIt9C-w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜ë¯¸ì  ë¶„í• (semantic segmentation)ì€ ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì— ë ˆì´ë¸” ë˜ëŠ” í´ë˜ìŠ¤ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤. ë¶„í• (segmentation)ì—ëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ìˆìœ¼ë©°, ì˜ë¯¸ì  ë¶„í• ì˜ ê²½ìš° ë™ì¼í•œ ë¬¼ì²´ì˜ ê³ ìœ  ì¸ìŠ¤í„´ìŠ¤ë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‘ ë¬¼ì²´ ëª¨ë‘ ë™ì¼í•œ ë ˆì´ë¸”ì´ ì§€ì •ë©ë‹ˆë‹¤(ì˜ˆì‹œë¡œ, \"car-1\" ê³¼ \"car-2\" ëŒ€ì‹  \"car\"ë¡œ ì§€ì •í•©ë‹ˆë‹¤).\n",
    "ì‹¤ìƒí™œì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆëŠ” ì˜ë¯¸ì  ë¶„í• ì˜ ì ìš© ì‚¬ë¡€ë¡œëŠ” ë³´í–‰ìì™€ ì¤‘ìš”í•œ êµí†µ ì •ë³´ë¥¼ ì‹ë³„í•˜ëŠ” ììœ¨ ì£¼í–‰ ìë™ì°¨ í•™ìŠµ, ì˜ë£Œ ì´ë¯¸ì§€ì˜ ì„¸í¬ì™€ ì´ìƒ ì§•í›„ ì‹ë³„, ê·¸ë¦¬ê³  ìœ„ì„± ì´ë¯¸ì§€ì˜ í™˜ê²½ ë³€í™” ëª¨ë‹ˆí„°ë§ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ë²ˆ ê°€ì´ë“œì—ì„œ ë°°ìš¸ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. [SceneParse150](https://huggingface.co/datasets/scene_parse_150) ë°ì´í„° ì„¸íŠ¸ë¥¼ ì´ìš©í•´ [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) ë¯¸ì„¸ ì¡°ì •í•˜ê¸°.\n",
    "2. ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ê¸°.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ì´ ì‘ì—…ê³¼ í˜¸í™˜ë˜ëŠ” ëª¨ë“  ì•„í‚¤í…ì²˜ì™€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë³´ë ¤ë©´ [ì‘ì—… í˜ì´ì§€](https://huggingface.co/tasks/image-segmentation)ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:\n",
    "\n",
    "```bash\n",
    "pip install -q datasets transformers evaluate\n",
    "```\n",
    "ì»¤ë®¤ë‹ˆí‹°ì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ê³µìœ í•  ìˆ˜ ìˆë„ë¡ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚˜ë©´ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SceneParse150 ë°ì´í„° ì„¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°[[load-sceneparse150-dataset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ SceneParse150 ë°ì´í„° ì„¸íŠ¸ì˜ ë” ì‘ì€ ë¶€ë¶„ ì§‘í•©ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë°ì´í„° ì„¸íŠ¸ ì „ì²´ì— ëŒ€í•œ í›ˆë ¨ì— ë§ì€ ì‹œê°„ì„ í• ì• í•˜ê¸° ì „ì— ì‹¤í—˜ì„ í†µí•´ ëª¨ë“  ê²ƒì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì„¸íŠ¸ì˜ `train`ì„ `train_test_split` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë¦¬ê³  ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,\n",
       " 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,\n",
       " 'scene_category': 368}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `image`: ì¥ë©´ì˜ PIL ì´ë¯¸ì§€ì…ë‹ˆë‹¤.\n",
    "- `annotation`: ë¶„í•  ì§€ë„(segmentation map)ì˜ PIL ì´ë¯¸ì§€ì…ë‹ˆë‹¤. ëª¨ë¸ì˜ íƒ€ê²Ÿì´ê¸°ë„ í•©ë‹ˆë‹¤.\n",
    "- `scene_category`: \"ì£¼ë°©\" ë˜ëŠ” \"ì‚¬ë¬´ì‹¤\"ê³¼ ê°™ì´ ì´ë¯¸ì§€ ì¥ë©´ì„ ì„¤ëª…í•˜ëŠ” ì¹´í…Œê³ ë¦¬ IDì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” ë‘˜ ë‹¤ PIL ì´ë¯¸ì§€ì¸ `image`ì™€ `annotation`ë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‚˜ì¤‘ì— ëª¨ë¸ì„ ì„¤ì •í•  ë•Œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë ˆì´ë¸” IDë¥¼ ë ˆì´ë¸” í´ë˜ìŠ¤ì— ë§¤í•‘í•˜ëŠ” ì‚¬ì „ë„ ë§Œë“¤ê³  ì‹¶ì„ ê²ƒì…ë‹ˆë‹¤. Hubì—ì„œ ë§¤í•‘ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  `id2label` ë° `label2id` ì‚¬ì „ì„ ë§Œë“œì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì „ì²˜ë¦¬í•˜ê¸°[[preprocess]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒ ë‹¨ê³„ëŠ” ëª¨ë¸ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ì™€ ì£¼ì„ì„ ì¤€ë¹„í•˜ê¸° ìœ„í•´ SegFormer ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ì„¸íŠ¸ì™€ ê°™ì€ ì¼ë¶€ ë°ì´í„° ì„¸íŠ¸ëŠ” ë°°ê²½ í´ë˜ìŠ¤ë¡œ ì œë¡œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë°°ê²½ í´ë˜ìŠ¤ëŠ” 150ê°œì˜ í´ë˜ìŠ¤ì— ì‹¤ì œë¡œëŠ” í¬í•¨ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— `do_reduce_labels=True` ë¥¼ ì„¤ì •í•´ ëª¨ë“  ë ˆì´ë¸”ì—ì„œ ë°°ê²½ í´ë˜ìŠ¤ë¥¼ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤. ì œë¡œ ì¸ë±ìŠ¤ëŠ” `255`ë¡œ ëŒ€ì²´ë˜ë¯€ë¡œ SegFormerì˜ ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ë¬´ì‹œë©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"nvidia/mit-b0\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ì— ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì—¬ ê³¼ì í•©ì— ëŒ€í•´ ëª¨ë¸ì„ ë³´ë‹¤ ê°•ê±´í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” [torchvision](https://pytorch.org/vision/stable/index.html)ì˜ [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ì†ì„±ì„ ì„ì˜ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ìì‹ ì´ ì›í•˜ëŠ” ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ColorJitter\n",
    "\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ì™€ ì£¼ì„ì„ ì¤€ë¹„í•˜ê¸° ìœ„í•´ ë‘ ê°œì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì´ í•¨ìˆ˜ë“¤ì€ ì´ë¯¸ì§€ë¥¼ `pixel_values`ë¡œ, ì£¼ì„ì„ `labels`ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. í›ˆë ¨ ì„¸íŠ¸ì˜ ê²½ìš° ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì— ì´ë¯¸ì§€ë¥¼ ì œê³µí•˜ê¸° ì „ì— `jitter`ë¥¼ ì ìš©í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ê²½ìš° ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” `images`ë¥¼ ìë¥´ê³  ì •ê·œí™”í•˜ë©°, í…ŒìŠ¤íŠ¸ ì¤‘ì—ëŠ” ë°ì´í„° ì¦ê°•ì´ ì ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ `labels`ë§Œ ìë¦…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ì— `jitter`ë¥¼ ì ìš©í•˜ë ¤ë©´, ğŸ¤— Datasets `set_transform` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì¦‰ì‹œ ë³€í™˜ì´ ì ìš©ë˜ê¸° ë•Œë¬¸ì— ë” ë¹ ë¥´ê³  ë””ìŠ¤í¬ ê³µê°„ì„ ëœ ì°¨ì§€í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ì— ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì—¬ ê³¼ì í•©ì— ëŒ€í•´ ëª¨ë¸ì„ ë³´ë‹¤ ê°•ê±´í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ì†ì„±ì„ ì„ì˜ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ìì‹ ì´ ì›í•˜ëŠ” ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë³„ê°œì˜ ë‘ ë³€í™˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:\n",
    "- ì´ë¯¸ì§€ ì¦ê°•ì„ í¬í•¨í•˜ëŠ” í•™ìŠµ ë°ì´í„° ë³€í™˜\n",
    "- ğŸ¤— Transformersì˜ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì€ ì±„ë„ ìš°ì„  ë ˆì´ì•„ì›ƒì„ ê¸°ëŒ€í•˜ê¸° ë•Œë¬¸ì—, ì´ë¯¸ì§€ë§Œ ë°”ê¾¸ëŠ” ê²€ì¦ ë°ì´í„° ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def aug_transforms(image):\n",
    "    image = tf.keras.utils.img_to_array(image)\n",
    "    image = tf.image.random_brightness(image, 0.25)\n",
    "    image = tf.image.random_contrast(image, 0.5, 2.0)\n",
    "    image = tf.image.random_saturation(image, 0.75, 1.25)\n",
    "    image = tf.image.random_hue(image, 0.1)\n",
    "    image = tf.transpose(image, (2, 0, 1))\n",
    "    return image\n",
    "\n",
    "\n",
    "def transforms(image):\n",
    "    image = tf.keras.utils.img_to_array(image)\n",
    "    image = tf.transpose(image, (2, 0, 1))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ëª¨ë¸ì„ ìœ„í•´ ë‘ ê°œì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì´ë¯¸ì§€ ë° ì£¼ì„ ë°°ì¹˜ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ë“¤ì€ ì´ë¯¸ì§€ ë³€í™˜ì„ ì ìš©í•˜ê³  ì´ì „ì— ë¡œë“œí•œ `image_processor`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ `pixel_values`ë¡œ, ì£¼ì„ì„ `label`ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. `ImageProcessor` ëŠ” ì´ë¯¸ì§€ì˜ í¬ê¸° ì¡°ì •ê³¼ ì •ê·œí™”ë„ ì²˜ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transforms(example_batch):\n",
    "    images = [aug_transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²´ ë°ì´í„° ì§‘í•©ì— ì „ì²˜ë¦¬ ë³€í™˜ì„ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasets `set_transform` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "ì¦‰ì‹œ ë³€í™˜ì´ ì ìš©ë˜ê¸° ë•Œë¬¸ì— ë” ë¹ ë¥´ê³  ë””ìŠ¤í¬ ê³µê°„ì„ ëœ ì°¨ì§€í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í‰ê°€í•˜ê¸°[[evaluate]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í›ˆë ¨ ì¤‘ì— ë©”íŠ¸ë¦­ì„ í¬í•¨í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ë°©ë²•ì„ ë¹ ë¥´ê²Œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ íƒœìŠ¤í¬ì—ì„œëŠ” [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) ë©”íŠ¸ë¦­ì„ ë¡œë“œí•˜ì„¸ìš” (ë©”íŠ¸ë¦­ì„ ë¡œë“œí•˜ê³  ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì‚´í´ë³´ì„¸ìš”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ë©”íŠ¸ë¦­ì„ `compute`í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì˜ˆì¸¡ì„ ë¨¼ì € ë¡œì§“ìœ¼ë¡œ ë³€í™˜í•œ ë‹¤ìŒ, ë ˆì´ë¸”ì˜ í¬ê¸°ì— ë§ê²Œ ëª¨ì–‘ì„ ë‹¤ì‹œ ì§€ì •í•´ì•¼ `compute`ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits = tf.transpose(logits, perm=[0, 2, 3, 1])\n",
    "    logits_resized = tf.image.resize(\n",
    "        logits,\n",
    "        size=tf.shape(labels)[1:],\n",
    "        method=\"bilinear\",\n",
    "    )\n",
    "\n",
    "    pred_labels = tf.argmax(logits_resized, axis=-1)\n",
    "    metrics = metric.compute(\n",
    "        predictions=pred_labels,\n",
    "        references=labels,\n",
    "        num_labels=num_labels,\n",
    "        ignore_index=-1,\n",
    "        reduce_labels=image_processor.do_reduce_labels,\n",
    "    )\n",
    "\n",
    "    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "    metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
    "    metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
    "    return {\"val_\" + k: v for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ `compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¸ë ˆì´ë‹ì„ ì„¤ì •í•  ë•Œ ì´ í•¨ìˆ˜ë¡œ ëŒì•„ê°€ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµí•˜ê¸°[[train]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ë§Œì•½ [Trainer](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer)ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì—¬ê¸°](https://huggingface.co/docs/transformers/main/ko/tasks/../training#finetune-with-trainer)ì—ì„œ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ ì‚´í´ë³´ì„¸ìš”!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ì´ì œ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! [AutoModelForSemanticSegmentation](https://huggingface.co/docs/transformers/main/ko/model_doc/auto#transformers.AutoModelForSemanticSegmentation)ë¡œ SegFormerë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ëª¨ë¸ì— ë ˆì´ë¸” IDì™€ ë ˆì´ë¸” í´ë˜ìŠ¤ ê°„ì˜ ë§¤í•‘ì„ ì „ë‹¬í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì„¸ ë‹¨ê³„ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ [TrainingArguments](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.TrainingArguments)ì— ì •ì˜í•©ë‹ˆë‹¤. `image` ì—´ì´ ì‚­ì œë˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì—´ì„ ì œê±°í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. `image` ì—´ì´ ì—†ìœ¼ë©´ `pixel_values`ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ëŸ° ê²½ìš°ë¥¼ ë°©ì§€í•˜ë ¤ë©´ `remove_unused_columns=False`ë¡œ ì„¤ì •í•˜ì„¸ìš”! ìœ ì¼í•˜ê²Œ í•„ìš”í•œ ë‹¤ë¥¸ ë§¤ê°œë³€ìˆ˜ëŠ” ëª¨ë¸ì„ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ëŠ” `output_dir`ì…ë‹ˆë‹¤. `push_to_hub=True`ë¥¼ ì„¤ì •í•˜ì—¬ ì´ ëª¨ë¸ì„ Hubì— í‘¸ì‹œí•©ë‹ˆë‹¤(ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ Hugging Faceì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤). ê° ì—í¬í¬ê°€ ëë‚  ë•Œë§ˆë‹¤ [Trainer](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer)ê°€ IoU ë©”íŠ¸ë¦­ì„ í‰ê°€í•˜ê³  í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "2. ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, í† í¬ë‚˜ì´ì €, ë°ì´í„° ì½œë ˆì´í„°, `compute_metrics` í•¨ìˆ˜ì™€ í•¨ê»˜ í•™ìŠµ ì¸ìë¥¼ [Trainer](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer)ì— ì „ë‹¬í•˜ì„¸ìš”.\n",
    "3. ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•´ [train()](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer.train)ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segformer-b0-scene-parse-150\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì´ ì™„ë£Œë˜ë©´, ëˆ„êµ¬ë‚˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ [push_to_hub()](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer.push_to_hub) ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ Hubì— ëª¨ë¸ì„ ê³µìœ í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Kerasë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, ë¨¼ì € [ê¸°ë³¸ íŠœí† ë¦¬ì–¼](https://huggingface.co/docs/transformers/main/ko/tasks/../training#train-a-tensorflow-model-with-keras)ì„ í™•ì¸í•´ë³´ì„¸ìš”!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:\n",
    "1. í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ê³  ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n",
    "2. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ì„¸ìš”.\n",
    "3. ğŸ¤— Datasetì„ `tf.data.Dataset`ë¡œ ë³€í™˜í•˜ì„¸ìš”.\n",
    "4. ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ì„¸ìš”.\n",
    "5. ì½œë°±ì„ ì¶”ê°€í•˜ì—¬ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ê³  ğŸ¤— Hubì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\n",
    "6. `fit()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ì„ ì‹¤í–‰í•˜ì„¸ìš”.\n",
    "\n",
    "í•˜ì´í¼íŒŒë¼ë¯¸í„°, ì˜µí‹°ë§ˆì´ì €, í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 50\n",
    "num_train_steps = len(train_ds) * num_epochs\n",
    "learning_rate = 6e-5\n",
    "weight_decay_rate = 0.01\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ë ˆì´ë¸” ë§¤í•‘ê³¼ í•¨ê»˜ [TFAutoModelForSemanticSegmentation](https://huggingface.co/docs/transformers/main/ko/model_doc/auto#transformers.TFAutoModelForSemanticSegmentation)ì„ ì‚¬ìš©í•˜ì—¬ SegFormerë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì˜µí‹°ë§ˆì´ì €ë¡œ ì»´íŒŒì¼í•©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ëª¨ë‘ ë””í´íŠ¸ë¡œ íƒœìŠ¤í¬ ê´€ë ¨ ì†ì‹¤ í•¨ìˆ˜ê°€ ìˆìœ¼ë¯€ë¡œ ì›ì¹˜ ì•Šìœ¼ë©´ ì§€ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSemanticSegmentation\n",
    "\n",
    "model = TFAutoModelForSemanticSegmentation.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.compile(optimizer=optimizer)  # ì†ì‹¤ í•¨ìˆ˜ ì¸ìê°€ ì—†ìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_tf_dataset` ì™€ [DefaultDataCollator](https://huggingface.co/docs/transformers/main/ko/main_classes/data_collator#transformers.DefaultDataCollator)ë¥¼ ì‚¬ìš©í•´ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = train_ds.to_tf_dataset(\n",
    "    columns=[\"pixel_values\", \"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = test_ds.to_tf_dataset(\n",
    "    columns=[\"pixel_values\", \"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜ˆì¸¡ìœ¼ë¡œ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  ëª¨ë¸ì„ ğŸ¤— Hubë¡œ í‘¸ì‹œí•˜ë ¤ë©´ [Keras callbacks](https://huggingface.co/docs/transformers/main/ko/tasks/../main_classes/keras_callbacks)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. `compute_metrics` í•¨ìˆ˜ë¥¼ [KerasMetricCallback](https://huggingface.co/docs/transformers/main/ko/main_classes/keras_callbacks#transformers.KerasMetricCallback)ì— ì „ë‹¬í•˜ê³ , ëª¨ë¸ ì—…ë¡œë“œë¥¼ ìœ„í•´ [PushToHubCallback](https://huggingface.co/docs/transformers/main/ko/main_classes/keras_callbacks#transformers.PushToHubCallback)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=[\"labels\"]\n",
    ")\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(output_dir=\"scene_segmentation\", tokenizer=image_processor)\n",
    "\n",
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í¬í¬ ìˆ˜ì™€ í•¨ê»˜ `fit()`ì„ í˜¸ì¶œí•˜ê³ , ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_eval_dataset,\n",
    "    callbacks=callbacks,\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶•í•˜í•©ë‹ˆë‹¤! ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê³  ğŸ¤— Hubì— ê³µìœ í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ë¡ í•˜ê¸°[[inference]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í–ˆìœ¼ë‹ˆ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "ì¶”ë¡ í•  ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ds[0][\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png\" alt=\"Image of bedroom\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "ì¶”ë¡ ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •í•œ ëª¨ë¸ì„ ì‹œí—˜í•´ ë³´ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ `pipeline()`ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„í• ì„ ìœ„í•œ `pipeline`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': None,\n",
       "  'label': 'wall',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},\n",
       " {'score': None,\n",
       "  'label': 'sky',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},\n",
       " {'score': None,\n",
       "  'label': 'floor',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},\n",
       " {'score': None,\n",
       "  'label': 'ceiling',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},\n",
       " {'score': None,\n",
       "  'label': 'bed ',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},\n",
       " {'score': None,\n",
       "  'label': 'windowpane',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},\n",
       " {'score': None,\n",
       "  'label': 'cabinet',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},\n",
       " {'score': None,\n",
       "  'label': 'chair',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},\n",
       " {'score': None,\n",
       "  'label': 'armchair',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "segmenter = pipeline(\"image-segmentation\", model=\"my_awesome_seg_model\")\n",
    "segmenter(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì›í•˜ëŠ” ê²½ìš° `pipeline`ì˜ ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê³  `pixel_values`ì„ GPUì— ë°°ì¹˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # ê°€ëŠ¥í•˜ë‹¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ CPUë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "encoding = image_processor(image, return_tensors=\"pt\")\n",
    "pixel_values = encoding.pixel_values.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  `logits`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(pixel_values=pixel_values)\n",
    "logits = outputs.logits.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ë¡œì§“ì˜ í¬ê¸°ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë‹¤ì‹œ ì¡°ì •í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_logits = nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "pred_seg = upsampled_logits.argmax(dim=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ì…ë ¥ì„ TensorFlow í…ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MariaK/scene_segmentation\")\n",
    "inputs = image_processor(image, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  `logits`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSemanticSegmentation\n",
    "\n",
    "model = TFAutoModelForSemanticSegmentation.from_pretrained(\"MariaK/scene_segmentation\")\n",
    "logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ë¡œê·¸ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì¬ì¡°ì •í•˜ê³  í´ë˜ìŠ¤ ì°¨ì›ì— argmaxë¥¼ ì ìš©í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.transpose(logits, [0, 2, 3, 1])\n",
    "\n",
    "upsampled_logits = tf.image.resize(\n",
    "    logits,\n",
    "    # `image.size`ê°€ ë„ˆë¹„ì™€ ë†’ì´ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— `image`ì˜ ëª¨ì–‘ì„ ë°˜ì „ì‹œí‚µë‹ˆë‹¤\n",
    "    image.size[::-1],\n",
    ")\n",
    "\n",
    "pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ë ¤ë©´ [dataset color palette](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)ë¥¼ ê° í´ë˜ìŠ¤ë¥¼ RGB ê°’ì— ë§¤í•‘í•˜ëŠ” `ade_palette()`ë¡œ ë¡œë“œí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ë¯¸ì§€ì™€ ì˜ˆì¸¡ëœ ë¶„í•  ì§€ë„(segmentation map)ì„ ê²°í•©í•˜ì—¬ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[pred_seg == label, :] = color\n",
    "color_seg = color_seg[..., ::-1]  # BGRë¡œ ë³€í™˜\n",
    "\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5  # ë¶„í•  ì§€ë„ìœ¼ë¡œ ì´ë¯¸ì§€ êµ¬ì„±\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png\" alt=\"Image of bedroom overlaid with segmentation map\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
