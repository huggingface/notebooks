{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤— PEFTë¡œ ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸° [[load-adapters-with-peft]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft) ë°©ë²•ì€ ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ë¯¸ì„¸ ì¡°ì • ì¤‘ ê³ ì •ì‹œí‚¤ê³ , ê·¸ ìœ„ì— í›ˆë ¨í•  ìˆ˜ ìˆëŠ” ë§¤ìš° ì ì€ ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜(ì–´ëŒ‘í„°)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ì–´ëŒ‘í„°ëŠ” ì‘ì—…ë³„ ì •ë³´ë¥¼ í•™ìŠµí•˜ë„ë¡ í›ˆë ¨ë©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì™„ì „íˆ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì— í•„ì í•˜ëŠ” ê²°ê³¼ë¥¼ ìƒì„±í•˜ë©´ì„œ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹„êµì  ì ì€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ PEFTë¡œ í›ˆë ¨ëœ ì–´ëŒ‘í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì „ì²´ ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ì‘ê¸° ë•Œë¬¸ì— ê³µìœ , ì €ì¥ ë° ê°€ì ¸ì˜¤ê¸°ê°€ í¸ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "<div class=\"flex flex-col justify-center\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png\"/>\n",
    "  <figcaption class=\"text-center\">Hubì— ì €ì¥ëœ OPTForCausalLM ëª¨ë¸ì˜ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ëŠ” ìµœëŒ€ 700MBì— ë‹¬í•˜ëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ì˜ ì „ì²´ í¬ê¸°ì— ë¹„í•´ ì•½ 6MBì— ë¶ˆê³¼í•©ë‹ˆë‹¤.</figcaption>\n",
    "</div>\n",
    "\n",
    "ğŸ¤— PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ë¬¸ì„œ](https://huggingface.co/docs/peft/index)ë¥¼ í™•ì¸í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¤ì • [[setup]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— PEFTë¥¼ ì„¤ì¹˜í•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”:\n",
    "\n",
    "```bash\n",
    "pip install peft\n",
    "```\n",
    "\n",
    "ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ë³´ê³  ì‹¶ë‹¤ë©´, ë‹¤ìŒ ì†ŒìŠ¤ì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤:\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/huggingface/peft.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì§€ì›ë˜ëŠ” PEFT ëª¨ë¸ [[supported-peft-models]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— TransformersëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë¶€ PEFT ë°©ë²•ì„ ì§€ì›í•˜ë©°, ë¡œì»¬ì´ë‚˜ Hubì— ì €ì¥ëœ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì˜¤ê³  ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ ì‰½ê²Œ ì‹¤í–‰í•˜ê±°ë‚˜ í›ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ì„ ì§€ì›í•©ë‹ˆë‹¤:\n",
    "\n",
    "- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)\n",
    "- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)\n",
    "- [AdaLoRA](https://huggingface.co/papers/2303.10512)\n",
    "\n",
    "ğŸ¤— PEFTì™€ ê´€ë ¨ëœ ë‹¤ë¥¸ ë°©ë²•(ì˜ˆ: í”„ë¡¬í”„íŠ¸ í›ˆë ¨ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ íŠœë‹) ë˜ëŠ” ì¼ë°˜ì ì¸ ğŸ¤— PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ë¬¸ì„œ](https://huggingface.co/docs/peft/index)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸° [[load-a-peft-adapter]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformersì—ì„œ PEFT ì–´ëŒ‘í„° ëª¨ë¸ì„ ê°€ì ¸ì˜¤ê³  ì‚¬ìš©í•˜ë ¤ë©´ Hub ì €ì¥ì†Œë‚˜ ë¡œì»¬ ë””ë ‰í„°ë¦¬ì— `adapter_config.json` íŒŒì¼ê³¼ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤. ê·¸ëŸ° ë‹¤ìŒ `AutoModelFor` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ PEFT ì–´ëŒ‘í„° ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì¸ê³¼ ê´€ê³„ ì–¸ì–´ ëª¨ë¸ìš© PEFT ì–´ëŒ‘í„° ëª¨ë¸ì„ ê°€ì ¸ì˜¤ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì‹­ì‹œì˜¤:\n",
    "\n",
    "1. PEFT ëª¨ë¸ IDë¥¼ ì§€ì •í•˜ì‹­ì‹œì˜¤.\n",
    "2. [AutoModelForCausalLM](https://huggingface.co/docs/transformers/main/ko/model_doc/auto#transformers.AutoModelForCausalLM) í´ë˜ìŠ¤ì— ì „ë‹¬í•˜ì‹­ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "`AutoModelFor` í´ë˜ìŠ¤ë‚˜ ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤(ì˜ˆ: `OPTForCausalLM` ë˜ëŠ” `LlamaForCausalLM`) ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì—¬ PEFT ì–´ëŒ‘í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "`load_adapter` ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ PEFT ì–´ëŒ‘í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ë¹„íŠ¸ ë˜ëŠ” 4ë¹„íŠ¸ë¡œ ê°€ì ¸ì˜¤ê¸° [[load-in-8bit-or-4bit]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bitsandbytes` í†µí•©ì€ 8ë¹„íŠ¸ì™€ 4ë¹„íŠ¸ ì •ë°€ë„ ë°ì´í„° ìœ í˜•ì„ ì§€ì›í•˜ë¯€ë¡œ í° ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ë•Œ ìœ ìš©í•˜ë©´ì„œ ë©”ëª¨ë¦¬ë„ ì ˆì•½í•©ë‹ˆë‹¤. ëª¨ë¸ì„ í•˜ë“œì›¨ì–´ì— íš¨ê³¼ì ìœ¼ë¡œ ë¶„ë°°í•˜ë ¤ë©´ [from_pretrained()](https://huggingface.co/docs/transformers/main/ko/main_classes/model#transformers.PreTrainedModel.from_pretrained)ì— `load_in_8bit` ë˜ëŠ” `load_in_4bit` ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ê³  `device_map=\"auto\"`ë¥¼ ì„¤ì •í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìƒˆ ì–´ëŒ‘í„° ì¶”ê°€ [[add-a-new-adapter]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìƒˆ ì–´ëŒ‘í„°ê°€ í˜„ì¬ ì–´ëŒ‘í„°ì™€ ë™ì¼í•œ ìœ í˜•ì¸ ê²½ìš°ì— í•œí•´ ê¸°ì¡´ ì–´ëŒ‘í„°ê°€ ìˆëŠ” ëª¨ë¸ì— ìƒˆ ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•˜ë ¤ë©´ `~peft.PeftModel.add_adapter`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ëª¨ë¸ì— ê¸°ì¡´ LoRA ì–´ëŒ‘í„°ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ” ê²½ìš°:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    init_lora_weights=False\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìƒˆ ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•˜ë ¤ë©´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach new adapter with same config\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ `~peft.PeftModel.set_adapter`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•  ì–´ëŒ‘í„°ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use adapter_1\n",
    "model.set_adapter(\"adapter_1\")\n",
    "output = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n",
    "\n",
    "# use adapter_2\n",
    "model.set_adapter(\"adapter_2\")\n",
    "output_enabled = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì–´ëŒ‘í„° í™œì„±í™” ë° ë¹„í™œì„±í™” [[enable-and-disable-adapters]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì— ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•œ í›„ ì–´ëŒ‘í„° ëª¨ë“ˆì„ í™œì„±í™” ë˜ëŠ” ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ëŒ‘í„° ëª¨ë“ˆì„ í™œì„±í™”í•˜ë ¤ë©´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "adapter_model_id = \"ybelkada/opt-350m-lora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "text = \"Hello\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "peft_config = PeftConfig.from_pretrained(adapter_model_id)\n",
    "\n",
    "# to initiate with random weights\n",
    "peft_config.init_lora_weights = False\n",
    "\n",
    "model.add_adapter(peft_config)\n",
    "model.enable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì–´ëŒ‘í„° ëª¨ë“ˆì„ ë¹„í™œì„±í™”í•˜ë ¤ë©´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.disable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT ì–´ëŒ‘í„° í›ˆë ¨ [[train-a-peft-adapter]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT ì–´ëŒ‘í„°ëŠ” [Trainer](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer) í´ë˜ìŠ¤ì—ì„œ ì§€ì›ë˜ë¯€ë¡œ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì— ë§ê²Œ ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª‡ ì¤„ì˜ ì½”ë“œë¥¼ ì¶”ê°€í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ LoRA ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨í•˜ë ¤ë©´:\n",
    "\n",
    "<Tip>\n",
    "\n",
    "[Trainer](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer)ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì´ ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´ [ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸°](https://huggingface.co/docs/transformers/main/ko/training) íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "1. ì‘ì—… ìœ í˜• ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì •í•˜ì—¬ ì–´ëŒ‘í„° êµ¬ì„±ì„ ì •ì˜í•©ë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ `~peft.LoraConfig`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ëª¨ë¸ì— ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ì´ì œ ëª¨ë¸ì„ [Trainer](https://huggingface.co/docs/transformers/main/ko/main_classes/trainer#transformers.Trainer)ì— ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, ...)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í›ˆë ¨í•œ ì–´ëŒ‘í„°ë¥¼ ì €ì¥í•˜ê³  ë‹¤ì‹œ ê°€ì ¸ì˜¤ë ¤ë©´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_dir)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
