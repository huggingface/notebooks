{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì˜ìƒ ë¶„ë¥˜ [[video-classification]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜ìƒ ë¶„ë¥˜ëŠ” ì˜ìƒ ì „ì²´ì— ë ˆì´ë¸” ë˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì§€ì •í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ê° ì˜ìƒì—ëŠ” í•˜ë‚˜ì˜ í´ë˜ìŠ¤ê°€ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ì˜ìƒ ë¶„ë¥˜ ëª¨ë¸ì€ ì˜ìƒì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì–´ëŠ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ ì˜ìƒì´ ì–´ë–¤ ë‚´ìš©ì¸ì§€ ë¶„ë¥˜í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ìƒ ë¶„ë¥˜ì˜ ì‹¤ì œ ì‘ìš© ì˜ˆëŠ” í”¼íŠ¸ë‹ˆìŠ¤ ì•±ì—ì„œ ìœ ìš©í•œ ë™ì‘ / ìš´ë™ ì¸ì‹ ì„œë¹„ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë˜í•œ ì‹œê° ì¥ì• ì¸ì´ ì´ë™í•  ë•Œ ë³´ì¡°í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "\n",
    "ì´ ê°€ì´ë“œì—ì„œëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:\n",
    "\n",
    "1. [UCF101](https://www.crcv.ucf.edu/data/UCF101.php) ë°ì´í„° ì„¸íŠ¸ì˜ í•˜ìœ„ ì§‘í•©ì„ í†µí•´ [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸°.\n",
    "2. ë¯¸ì„¸ ì¡°ì •í•œ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ê¸°.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì„¤ëª…í•˜ëŠ” ì‘ì—…ì€ ë‹¤ìŒ ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤:\n",
    "\n",
    "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
    "\n",
    "[TimeSformer](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/timesformer), [VideoMAE](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/videomae)\n",
    "\n",
    "<!--End of the generated tip-->\n",
    "\n",
    "</Tip>\n",
    "\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:\n",
    "```bash\n",
    "pip install -q pytorchvideo transformers evaluate\n",
    "```\n",
    "\n",
    "ì˜ìƒì„ ì²˜ë¦¬í•˜ê³  ì¤€ë¹„í•˜ê¸° ìœ„í•´ [PyTorchVideo](https://pytorchvideo.org/)(ì´í•˜ `pytorchvideo`)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì»¤ë®¤ë‹ˆí‹°ì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ê³µìœ í•  ìˆ˜ ìˆë„ë¡ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚˜ë©´ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCF101 ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° [[load-ufc101-dataset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[UCF-101](https://www.crcv.ucf.edu/data/UCF101.php) ë°ì´í„° ì„¸íŠ¸ì˜ í•˜ìœ„ ì§‘í•©(subset)ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ í•™ìŠµí•˜ëŠ”ë° ë” ë§ì€ ì‹œê°„ì„ í• ì• í•˜ê¸° ì „ì— ë°ì´í„°ì˜ í•˜ìœ„ ì§‘í•©ì„ ë¶ˆëŸ¬ì™€ ëª¨ë“  ê²ƒì´ ì˜ ì‘ë™í•˜ëŠ”ì§€ ì‹¤í—˜í•˜ê³  í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
    "filename = \"UCF101_subset.tar.gz\"\n",
    "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì„¸íŠ¸ì˜ í•˜ìœ„ ì§‘í•©ì´ ë‹¤ìš´ë¡œë“œ ë˜ë©´, ì••ì¶•ëœ íŒŒì¼ì˜ ì••ì¶•ì„ í•´ì œí•´ì•¼ í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(file_path) as t:\n",
    "     t.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²´ ë°ì´í„° ì„¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "```bash\n",
    "UCF101_subset/\n",
    "    train/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "    val/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "    test/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "```\n",
    "\n",
    "\n",
    "ì •ë ¬ëœ ì˜ìƒì˜ ê²½ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "```bash\n",
    "...\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\n",
    "...\n",
    "```\n",
    "\n",
    "ë™ì¼í•œ ê·¸ë£¹/ì¥ë©´ì— ì†í•˜ëŠ” ì˜ìƒ í´ë¦½ì€ íŒŒì¼ ê²½ë¡œì—ì„œ `g`ë¡œ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, `v_ApplyEyeMakeup_g07_c04.avi`ì™€ `v_ApplyEyeMakeup_g07_c06.avi` ì´ ìˆìŠµë‹ˆë‹¤. ì´ ë‘˜ì€ ê°™ì€ ê·¸ë£¹ì…ë‹ˆë‹¤.\n",
    "\n",
    "ê²€ì¦ ë° í‰ê°€ ë°ì´í„° ë¶„í• ì„ í•  ë•Œ, [ë°ì´í„° ëˆ„ì¶œ(data leakage)](https://www.kaggle.com/code/alexisbcook/data-leakage)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë™ì¼í•œ ê·¸ë£¹ / ì¥ë©´ì˜ ì˜ìƒ í´ë¦½ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•˜ìœ„ ì§‘í•©ì€ ì´ëŸ¬í•œ ì •ë³´ë¥¼ ê³ ë ¤í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ ë‹¤ìŒìœ¼ë¡œ, ë°ì´í„° ì„¸íŠ¸ì— ì¡´ì¬í•˜ëŠ” ë¼ë²¨ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ ë„ì›€ì´ ë  ë”•ì…”ë„ˆë¦¬(dictionary data type)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "* `label2id`: í´ë˜ìŠ¤ ì´ë¦„ì„ ì •ìˆ˜ì— ë§¤í•‘í•©ë‹ˆë‹¤.\n",
    "* `id2label`: ì •ìˆ˜ë¥¼ í´ë˜ìŠ¤ ì´ë¦„ì— ë§¤í•‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress']."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” ì´ 10ê°œì˜ ê³ ìœ í•œ í´ë˜ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤. ê° í´ë˜ìŠ¤ë§ˆë‹¤ 30ê°œì˜ ì˜ìƒì´ í›ˆë ¨ ì„¸íŠ¸ì— ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•´ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° [[load-a-model-to-fine-tune]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ì „ í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ì™€ ì²´í¬í¬ì¸íŠ¸ì— ì—°ê´€ëœ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ìƒ ë¶„ë¥˜ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì¸ì½”ë”ì—ëŠ” ë¯¸ë¦¬ í•™ìŠµëœ ë§¤ê°œë³€ìˆ˜ê°€ ì œê³µë˜ë©°, ë¶„ë¥˜ í—¤ë“œ(ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´)ëŠ” ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ì˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‘ì„±í•  ë•ŒëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œê°€ ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "model_ckpt = \"MCG-NJU/videomae-base\"\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” ë™ì•ˆ, ë‹¤ìŒê³¼ ê°™ì€ ê²½ê³ ë¥¼ ë§ˆì£¼ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "```bash\n",
    "Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']\n",
    "- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "```\n",
    "\n",
    "\n",
    "ìœ„ ê²½ê³ ëŠ” ìš°ë¦¬ê°€ ì¼ë¶€ ê°€ì¤‘ì¹˜(ì˜ˆ: `classifier` ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥)ë¥¼ ë²„ë¦¬ê³  ìƒˆë¡œìš´ `classifier` ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì¤ë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” ë¯¸ë¦¬ í•™ìŠµëœ ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ìƒˆë¡œìš´ í—¤ë“œë¥¼ ì¶”ê°€í•˜ê³  ìˆìœ¼ë¯€ë¡œ, ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ê¸° ì „ì— ë¯¸ì„¸ ì¡°ì •í•˜ë¼ê³  ê²½ê³ ë¥¼ ë³´ë‚´ëŠ” ê²ƒì€ ë‹¹ì—°í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ì œ ìš°ë¦¬ëŠ” ì´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ ** ì´ [ì²´í¬í¬ì¸íŠ¸](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics)ëŠ” ë„ë©”ì¸ì´ ë§ì´ ì¤‘ì²©ëœ ìœ ì‚¬í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì–»ì€ ì²´í¬í¬ì¸íŠ¸ì´ë¯€ë¡œ ì´ ì‘ì—…ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `MCG-NJU/videomae-base-finetuned-kinetics` ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì–»ì€ [ì²´í¬í¬ì¸íŠ¸](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset)ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° ì„¸íŠ¸ ì¤€ë¹„í•˜ê¸°[[prepare-the-datasets-for-training]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜ìƒ ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ [PyTorchVideo ë¼ì´ë¸ŒëŸ¬ë¦¬](https://pytorchvideo.org/)ë¥¼ í™œìš©í•  ê²ƒì…ë‹ˆë‹¤. í•„ìš”í•œ ì¢…ì†ì„±ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ ë³€í™˜ì—ëŠ” 'ê· ì¼í•œ ì‹œê°„ ìƒ˜í”Œë§(uniform temporal subsampling)', 'í”½ì…€ ì •ê·œí™”(pixel normalization)', 'ëœë¤ ì˜ë¼ë‚´ê¸°(random cropping)' ë° 'ëœë¤ ìˆ˜í‰ ë’¤ì§‘ê¸°(random horizontal flipping)'ì˜ ì¡°í•©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê²€ì¦ ë° í‰ê°€ ë°ì´í„° ì„¸íŠ¸ ë³€í™˜ì—ëŠ” 'ëœë¤ ì˜ë¼ë‚´ê¸°'ì™€ 'ëœë¤ ë’¤ì§‘ê¸°'ë¥¼ ì œì™¸í•œ ë™ì¼í•œ ë³€í™˜ ì²´ì¸ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [PyTorchVideo ê³µì‹ ë¬¸ì„œ](https://pytorchvideo.org)ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n",
    "\n",
    "ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ê´€ë ¨ëœ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "* ì˜ìƒ í”„ë ˆì„ í”½ì…€ì„ ì •ê·œí™”í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ í‰ê· ê³¼ í‘œì¤€ í¸ì°¨\n",
    "* ì˜ìƒ í”„ë ˆì„ì´ ì¡°ì •ë  ê³µê°„ í•´ìƒë„\n",
    "\n",
    "\n",
    "ë¨¼ì €, ëª‡ ê°€ì§€ ìƒìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ë°ì´í„° ì„¸íŠ¸ì— íŠ¹í™”ëœ ì „ì²˜ë¦¬(transform)ê³¼ ë°ì´í„° ì„¸íŠ¸ ìì²´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë¨¼ì € í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    RandomCrop(resize_to),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°™ì€ ë°©ì‹ì˜ ì‘ì—… íë¦„ì„ ê²€ì¦ê³¼ í‰ê°€ ì„¸íŠ¸ì—ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì°¸ê³ **: ìœ„ì˜ ë°ì´í„° ì„¸íŠ¸ì˜ íŒŒì´í”„ë¼ì¸ì€ [ê³µì‹ íŒŒì´í† ì¹˜ ì˜ˆì œ](https://pytorchvideo.org/docs/tutorial_classification#dataset)ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” UCF-101 ë°ì´í„°ì…‹ì— ë§ê²Œ [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ ì´ í•¨ìˆ˜ëŠ” [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. `LabeledVideoDataset` í´ë˜ìŠ¤ëŠ” PyTorchVideo ë°ì´í„°ì…‹ì—ì„œ ëª¨ë“  ì˜ìƒ ê´€ë ¨ ì‘ì—…ì˜ ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ë”°ë¼ì„œ PyTorchVideoì—ì„œ ë¯¸ë¦¬ ì œê³µí•˜ì§€ ì•ŠëŠ” ì‚¬ìš©ì ì§€ì • ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´, ì´ í´ë˜ìŠ¤ë¥¼ ì ì ˆí•˜ê²Œ í™•ì¥í•˜ë©´ ë©ë‹ˆë‹¤. ë” ìì„¸í•œ ì‚¬í•­ì´ ì•Œê³  ì‹¶ë‹¤ë©´ `data` API [ë¬¸ì„œ](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) ë¥¼ ì°¸ê³ í•˜ì„¸ìš”. ë˜í•œ ìœ„ì˜ ì˜ˆì‹œì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¥¼ ê°–ëŠ” ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´, `pytorchvideo.data.Ucf101()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë° ë¬¸ì œê°€ ì—†ì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„° ì„¸íŠ¸ì— ì˜ìƒì˜ ê°œìˆ˜ë¥¼ ì•Œê¸° ìœ„í•´ `num_videos` ì¸ìˆ˜ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# (300, 30, 75)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë” ë‚˜ì€ ë””ë²„ê¹…ì„ ìœ„í•´ ì „ì²˜ë¦¬ ì˜ìƒ ì‹œê°í™”í•˜ê¸°[[visualize-the-preprocessed-video-for-better-debugging]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "    \n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)\n",
    "\n",
    "sample_video = next(iter(train_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif\" alt=\"Person playing basketball\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ í›ˆë ¨í•˜ê¸°[[train-the-model]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformersì˜ [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer)ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œë³´ì„¸ìš”. `Trainer`ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ë ¤ë©´ í›ˆë ¨ ì„¤ì •ê³¼ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.  ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ì…ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ëŠ” í›ˆë ¨ì„ êµ¬ì„±í•˜ëŠ” ëª¨ë“  ì†ì„±ì„ í¬í•¨í•˜ë©°, í›ˆë ¨ ì¤‘ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  ì¶œë ¥ í´ë” ì´ë¦„ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ë˜í•œ ğŸ¤— Hubì˜ ëª¨ë¸ ì €ì¥ì†Œì˜ ëª¨ë“  ì •ë³´ë¥¼ ë™ê¸°í™”í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ëŒ€ë¶€ë¶„ì˜ í›ˆë ¨ ì¸ìˆ˜ëŠ” ë”°ë¡œ ì„¤ëª…í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì—ì„œ ì¤‘ìš”í•œ ì¸ìˆ˜ëŠ” `remove_unused_columns=False` ì…ë‹ˆë‹¤. ì´ ì¸ìëŠ” ëª¨ë¸ì˜ í˜¸ì¶œ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ëª¨ë“  ì†ì„± ì—´(columns)ì„ ì‚­ì œí•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ ì¼ë°˜ì ìœ¼ë¡œ Trueì…ë‹ˆë‹¤. ì´ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ ì—´ì„ ì‚­ì œí•˜ëŠ” ê²ƒì´ ì´ìƒì ì´ë©°, ì…ë ¥ì„ ëª¨ë¸ì˜ í˜¸ì¶œ í•¨ìˆ˜ë¡œ í’€ê¸°(unpack)ê°€ ì‰¬ì›Œì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ê²½ìš°ì—ëŠ” `pixel_values`(ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ í•„ìˆ˜ì ì¸ í‚¤)ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥('video'ê°€ íŠ¹íˆ ê·¸ë ‡ìŠµë‹ˆë‹¤)ì´ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ remove_unused_columnsì„ Falseë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\n",
    "num_epochs = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytorchvideo.data.Ucf101()` í•¨ìˆ˜ë¡œ ë°˜í™˜ë˜ëŠ” ë°ì´í„° ì„¸íŠ¸ëŠ” `__len__` ë©”ì†Œë“œê°€ ì´ì‹ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ,  `TrainingArguments`ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•  ë•Œ `max_steps`ë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒìœ¼ë¡œ, í‰ê°€ì§€í‘œë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ì˜ˆì¸¡ê°’ì—ì„œ í‰ê°€ì§€í‘œë¥¼ ê³„ì‚°í•  í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. í•„ìš”í•œ ì „ì²˜ë¦¬ ì‘ì—…ì€ ì˜ˆì¸¡ëœ ë¡œì§“(logits)ì— argmax ê°’ì„ ì·¨í•˜ëŠ” ê²ƒë¿ì…ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**í‰ê°€ì— ëŒ€í•œ ì°¸ê³ ì‚¬í•­**:\n",
    "\n",
    "[VideoMAE ë…¼ë¬¸](https://arxiv.org/abs/2203.12602)ì—ì„œ ì €ìëŠ” ë‹¤ìŒê³¼ ê°™ì€ í‰ê°€ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì˜ìƒì—ì„œ ì—¬ëŸ¬ í´ë¦½ì„ ì„ íƒí•˜ê³  ê·¸ í´ë¦½ì— ë‹¤ì–‘í•œ í¬ë¡­ì„ ì ìš©í•˜ì—¬ ì§‘ê³„ ì ìˆ˜ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ê°„ë‹¨í•¨ê³¼ ê°„ê²°í•¨ì„ ìœ„í•´ í•´ë‹¹ ì „ëµì„ ê³ ë ¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ, ì˜ˆì œë¥¼ ë¬¶ì–´ì„œ ë°°ì¹˜ë¥¼ í˜•ì„±í•˜ëŠ” `collate_fn`ì„ ì •ì˜í•´ì•¼í•©ë‹ˆë‹¤. ê° ë°°ì¹˜ëŠ” `pixel_values`ì™€ `labels`ë¼ëŠ” 2ê°œì˜ í‚¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ì´ ëª¨ë“  ê²ƒì„ ë°ì´í„° ì„¸íŠ¸ì™€ í•¨ê»˜ `Trainer`ì— ì „ë‹¬í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ë¥¼ ì´ë¯¸ ì²˜ë¦¬í–ˆëŠ”ë°ë„ ë¶ˆêµ¬í•˜ê³  `image_processor`ë¥¼ í† í¬ë‚˜ì´ì € ì¸ìˆ˜ë¡œ ë„£ì€ ì´ìœ ëŠ” JSONìœ¼ë¡œ ì €ì¥ë˜ëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ êµ¬ì„± íŒŒì¼ì´ Hubì˜ ì €ì¥ì†Œì— ì—…ë¡œë“œë˜ë„ë¡ í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "\n",
    "`train` ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì´ ì™„ë£Œë˜ë©´, ëª¨ë¸ì„ `push_to_hub()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í—ˆë¸Œì— ê³µìœ í•˜ì—¬ ëˆ„êµ¬ë‚˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ë¡ í•˜ê¸°[[inference]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¢‹ìŠµë‹ˆë‹¤. ì´ì œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì¶”ë¡ í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì¶”ë¡ ì— ì‚¬ìš©í•  ì˜ìƒì„ ë¶ˆëŸ¬ì˜¤ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_video = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif\" alt=\"Teams playing basketball\"/>\n",
    "</div>\n",
    "\n",
    "ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline)ì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ë¡œ ì˜ìƒ ë¶„ë¥˜ë¥¼ í•˜ê¸° ìœ„í•´ `pipeline`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì˜ìƒì„ ì „ë‹¬í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9272987842559814, 'label': 'BasketballDunk'},\n",
       " {'score': 0.017777055501937866, 'label': 'BabyCrawling'},\n",
       " {'score': 0.01663011871278286, 'label': 'BalanceBeam'},\n",
       " {'score': 0.009560945443809032, 'label': 'BandMarching'},\n",
       " {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "video_cls = pipeline(model=\"my_awesome_video_cls_model\")\n",
    "video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§Œì•½ ì›í•œë‹¤ë©´ ìˆ˜ë™ìœ¼ë¡œ `pipeline`ì˜ ê²°ê³¼ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, video):\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
    "    inputs = {\n",
    "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
    "        \"labels\": torch.tensor(\n",
    "            [sample_test_video[\"label\"]]\n",
    "        ),  # this can be skipped if you don't have labels available.\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì— ì…ë ¥ê°’ì„ ë„£ê³  `logits`ì„ ë°˜í™˜ë°›ìœ¼ì„¸ìš”:\n",
    "\n",
    "```\n",
    ">>> logits = run_inference(trained_model, sample_test_video[\"video\"])\n",
    "```\n",
    "\n",
    "`logits`ì„ ë””ì½”ë”©í•˜ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Predicted class: BasketballDunk"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
