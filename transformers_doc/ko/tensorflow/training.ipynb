{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¯¸ì„¸ íŠœë‹í•˜ê¸°[[finetune-a-pretrained-model]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ìƒë‹¹í•œ ì´ì ì´ ìˆìŠµë‹ˆë‹¤. ê³„ì‚° ë¹„ìš©ê³¼ íƒ„ì†Œë°œìêµ­ì„ ì¤„ì´ê³ , ì²˜ìŒë¶€í„° ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ í•„ìš” ì—†ì´ ìµœì‹  ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ¤— TransformersëŠ” ë‹¤ì–‘í•œ ì‘ì—…ì„ ìœ„í•´ ì‚¬ì „ í•™ìŠµëœ ìˆ˜ì²œ ê°œì˜ ëª¨ë¸ì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ìì‹ ì˜ ì‘ì—…ê³¼ ê´€ë ¨ëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ í•™ìŠµí•©ë‹ˆë‹¤. ì´ê²ƒì€ ë¯¸ì„¸ íŠœë‹ì´ë¼ê³  í•˜ëŠ” ë§¤ìš° ê°•ë ¥í•œ í›ˆë ¨ ê¸°ë²•ì…ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ë‹¹ì‹ ì´ ì„ íƒí•œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¯¸ì„¸ íŠœë‹í•©ë‹ˆë‹¤:\n",
    "\n",
    "* ğŸ¤— Transformersë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¯¸ì„¸ íŠœë‹í•˜ê¸° `Trainer`.\n",
    "* Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ TensorFlowì—ì„œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¯¸ì„¸ íŠœë‹í•˜ê¸°.\n",
    "* ê¸°ë³¸ PyTorchì—ì„œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¯¸ì„¸ íŠœë‹í•˜ê¸°.\n",
    "\n",
    "<a id='data-processing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ì…‹ ì¤€ë¹„[[prepare-a-dataset]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¯¸ì„¸ íŠœë‹í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  í›ˆë ¨í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•˜ì„¸ìš”. ì´ì „ íŠœí† ë¦¬ì–¼ì—ì„œ í›ˆë ¨ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ë“œë ¸ëŠ”ë°, ì§€ê¸ˆì´ ë°°ìš¸ ê±¸ ë˜ì§šì„ ê¸°íšŒì…ë‹ˆë‹¤!\n",
    "\n",
    "ë¨¼ì € [Yelp ë¦¬ë·°](https://huggingface.co/datasets/yelp_review_full) ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ íŒ¨ë”© ë° ì˜ë¼ë‚´ê¸° ì „ëµì„ í¬í•¨í•˜ë ¤ë©´ í† í¬ë‚˜ì´ì €ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë ¤ë©´ ğŸ¤— Dataset [`map`](https://huggingface.co/docs/datasets/process.html#map) ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•„ìš”í•œ ê²½ìš° ë¯¸ì„¸ íŠœë‹ì„ ìœ„í•´ ë°ì´í„°ì…‹ì˜ ì‘ì€ ë¶€ë¶„ ì§‘í•©ì„ ë§Œë“¤ì–´ ë¯¸ì„¸ íŠœë‹ ì‘ì—… ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trainer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ì„œë¶€í„°ëŠ” ì‚¬ìš©í•˜ë ¤ëŠ” í”„ë ˆì„ì›Œí¬ì— í•´ë‹¹í•˜ëŠ” ì„¹ì…˜ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. ì˜¤ë¥¸ìª½ ì‚¬ì´ë“œë°”ì˜ ë§í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¡œ ì´ë™í•  ìˆ˜ ìˆìœ¼ë©°, íŠ¹ì • í”„ë ˆì„ì›Œí¬ì˜ ëª¨ë“  ì½˜í…ì¸ ë¥¼ ìˆ¨ê¸°ë ¤ë©´ í•´ë‹¹ í”„ë ˆì„ì›Œí¬ ë¸”ë¡ì˜ ì˜¤ë¥¸ìª½ ìƒë‹¨ì— ìˆëŠ” ë²„íŠ¼ì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤!\n",
    "\n",
    "<a id='keras'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rnTGBy2ax1c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rnTGBy2ax1c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kerasë¡œ í…ì„œí”Œë¡œìš° ëª¨ë¸ í›ˆë ¨í•˜ê¸°[[train-a-tensorflow-model-with-keras]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ì„œí”Œë¡œìš°ì—ì„œ ğŸ¤— Transformers ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kerasìš© ë°ì´í„° ë¡œë“œ[[loading-data-for-keras]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras APIë¡œ ğŸ¤— Transformers ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë ¤ë©´ ë°ì´í„°ì…‹ì„ Kerasê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ë°ì´í„° ì„¸íŠ¸ê°€ ì‘ì€ ê²½ìš°, ì „ì²´ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ Kerasë¡œ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "ë” ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ì „ì— ë¨¼ì € ì´ ì‘ì—…ì„ ì‹œë„í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë¨¼ì € ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. [GLUE ë²¤ì¹˜ë§ˆí¬](https://huggingface.co/datasets/glue)ì˜ CoLA ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "ê°„ë‹¨í•œ ë°”ì´ë„ˆë¦¬ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—…ì´ë¯€ë¡œ ì§€ê¸ˆì€ í›ˆë ¨ ë°ì´í„° ë¶„í• ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "dataset = dataset[\"train\"]  # Just take the training split for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³  ë°ì´í„°ë¥¼ NumPy ë°°ì—´ë¡œ í† í°í™”í•©ë‹ˆë‹¤. ë ˆì´ë¸”ì€ ì´ë¯¸ 0ê³¼ 1ë¡œ ëœ ë¦¬ìŠ¤íŠ¸ì´ê¸° ë•Œë¬¸ì— í† í°í™”í•˜ì§€ ì•Šê³  ë°”ë¡œ NumPy ë°°ì—´ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œ, [`compile`](https://keras.io/api/models/model_training_apis/#compile-method), [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "\n",
    "model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ëª¨ë¸ì„ `compile()`í•  ë•Œ ì†ì‹¤ ì¸ìˆ˜ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤! \n",
    "ì´ ì¸ìˆ˜ë¥¼ ë¹„ì›Œë‘ë©´ í—ˆê¹… í˜ì´ìŠ¤ ëª¨ë¸ì€ ì‘ì—…ê³¼ ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ì í•©í•œ ì†ì‹¤ì„ ìë™ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤. \n",
    "ì›í•œë‹¤ë©´ ì–¸ì œë“ ì§€ ì§ì ‘ ì†ì‹¤ì„ ì§€ì •í•˜ì—¬ ì´ë¥¼ ì¬ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ì´ ì ‘ê·¼ ë°©ì‹ì€ ì†Œê·œëª¨ ë°ì´í„° ì§‘í•©ì—ì„œëŠ” ì˜ ì‘ë™í•˜ì§€ë§Œ, ëŒ€ê·œëª¨ ë°ì´í„° ì§‘í•©ì—ì„œëŠ” ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™œ ê·¸ëŸ´ê¹Œìš”?\n",
    "í† í°í™”ëœ ë°°ì—´ê³¼ ë ˆì´ë¸”ì„ ë©”ëª¨ë¦¬ì— ì™„ì „íˆ ë¡œë“œí•˜ê³  NumPyëŠ” \"ë“¤ì­‰ë‚ ì­‰í•œ\" ë°°ì—´ì„ ì²˜ë¦¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—,\n",
    "ëª¨ë“  í† í°í™”ëœ ìƒ˜í”Œì„ ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ê°€ì¥ ê¸´ ìƒ˜í”Œì˜ ê¸¸ì´ë§Œí¼ íŒ¨ë”©í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë°°ì—´ì´ í›¨ì”¬ ë” ì»¤ì§€ê³  ì´ íŒ¨ë”© í† í°ìœ¼ë¡œ ì¸í•´ í•™ìŠµ ì†ë„ë„ ëŠë ¤ì§‘ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ë¥¼ tf.data.Datasetìœ¼ë¡œ ë¡œë“œí•˜ê¸°[[loading-data-as-a-tfdatadataset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§€ëŠ” ê²ƒì„ í”¼í•˜ë ¤ë©´ ë°ì´í„°ë¥¼ `tf.data.Dataset`ìœ¼ë¡œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›í•œë‹¤ë©´ ì§ì ‘\n",
    "`tf.data` íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì‘ì„±í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì´ ì‘ì—…ì„ ê°„í¸í•˜ê²Œ ìˆ˜í–‰í•˜ëŠ” ìˆ˜ ìˆëŠ” ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- `prepare_tf_dataset()`: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì´ ë°©ë²•ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ë©”ì„œë“œì´ê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ê²€ì‚¬í•˜ì—¬ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì—´ì„ ìë™ìœ¼ë¡œ íŒŒì•…í•˜ê³ \n",
    "ë‚˜ë¨¸ì§€ëŠ” ë²„ë ¤ì„œ ë” ë‹¨ìˆœí•˜ê³  ì„±ëŠ¥ì´ ì¢‹ì€ ë°ì´í„° ì§‘í•©ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- `to_tf_dataset`: ì´ ë°©ë²•ì€ ì¢€ ë” ë‚®ì€ ìˆ˜ì¤€ì´ë©°, í¬í•¨í•  'ì—´'ê³¼ 'ë ˆì´ë¸”'ì„ ì •í™•íˆ ì§€ì •í•˜ì—¬\n",
    "ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì •í™•íˆ ì œì–´í•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©í•˜ë©°, í¬í•¨í•  'columns'ê³¼ 'label_cols'ì„ ì •í™•íˆ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "`prepare_tf_dataset()`ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € ë‹¤ìŒ ì½”ë“œ ìƒ˜í”Œê³¼ ê°™ì´ í† í¬ë‚˜ì´ì € ì¶œë ¥ì„ ë°ì´í„° ì„¸íŠ¸ì— ì—´ë¡œ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"text\"])\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í—ˆê¹… í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥ë˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëŠ˜ë¦¬ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì„¸ìš”! \n",
    "ì—´ì´ ì¶”ê°€ë˜ë©´ ë°ì´í„°ì…‹ì—ì„œ ë°°ì¹˜ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ê³  ê° ë°°ì¹˜ì— íŒ¨ë”©ì„ ì¶”ê°€í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì „ì²´ ë°ì´í„°ì…‹ì— íŒ¨ë”©ì„ ì¶”ê°€í•˜ëŠ” ê²ƒë³´ë‹¤ íŒ¨ë”© í† í°ì˜ ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = model.prepare_tf_dataset(dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì˜ ì½”ë“œ ìƒ˜í”Œì—ì„œëŠ” ë°°ì¹˜ê°€ ë¡œë“œë  ë•Œ ì˜¬ë°”ë¥´ê²Œ íŒ¨ë”©í•  ìˆ˜ ìˆë„ë¡ `prepare_tf_dataset`ì— í† í¬ë‚˜ì´ì €ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ë°ì´í„°ì…‹ì˜ ëª¨ë“  ìƒ˜í”Œ ê¸¸ì´ê°€ ê°™ê³  íŒ¨ë”©ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ì´ ì¸ìˆ˜ë¥¼ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ìƒ˜í”Œì„ ì±„ìš°ëŠ” ê²ƒë³´ë‹¤ ë” ë³µì¡í•œ ì‘ì—…(ì˜ˆ: ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ì˜ í† í° ì†ìƒ ëª¨ë¸ë§)ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í† í°ì„ ì†ìƒì‹œì¼œì•¼ í•˜ëŠ” ê²½ìš°, \n",
    "`collate_fn` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒ˜í”Œ ëª©ë¡ì„ ë°°ì¹˜ë¡œ ë³€í™˜í•˜ê³  ì›í•˜ëŠ” ì „ì²˜ë¦¬ë¥¼ ì ìš©í•  í•¨ìˆ˜ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "[ì˜ˆì‹œ](https://github.com/huggingface/transformers/tree/main/examples) ë˜ëŠ” \n",
    "[ë…¸íŠ¸ë¶](https://huggingface.co/docs/transformers/notebooks)ì„ ì°¸ì¡°í•˜ì—¬ ì´ ì ‘ê·¼ ë°©ì‹ì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ëª¨ìŠµì„ í™•ì¸í•˜ì„¸ìš”.\n",
    "\n",
    "`tf.data.Dataset`ì„ ìƒì„±í•œ í›„ì—ëŠ” ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³  í›ˆë ¨(fit)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(3e-5))\n",
    "\n",
    "model.fit(tf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pytorch_native'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ë³¸ íŒŒì´í† ì¹˜ë¡œ í›ˆë ¨í•˜ê¸°[[train-in-native-pytorch]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ê°€ ìë£Œ[[additional-resources]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë” ë§ì€ ë¯¸ì„¸ íŠœë‹ ì˜ˆì œëŠ” ë‹¤ìŒì„ ì°¸ì¡°í•˜ì„¸ìš”:\n",
    "\n",
    "- [ğŸ¤— Trnasformers ì˜ˆì œ](https://github.com/huggingface/transformers/tree/main/examples)ì—ëŠ” PyTorch ë° í…ì„œí”Œë¡œìš°ì—ì„œ ì¼ë°˜ì ì¸ NLP ì‘ì—…ì„ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” ìŠ¤í¬ë¦½íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- [ğŸ¤— Transformers ë…¸íŠ¸ë¶](https://huggingface.co/docs/transformers/main/ko/notebooks)ì—ëŠ” PyTorch ë° í…ì„œí”Œë¡œìš°ì—ì„œ íŠ¹ì • ì‘ì—…ì„ ìœ„í•´ ëª¨ë¸ì„ ë¯¸ì„¸ íŠœë‹í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ë‹¤ì–‘í•œ ë…¸íŠ¸ë¶ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
