{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì»´í“¨í„° ë¹„ì „ì„ ìœ„í•œ ì§€ì‹ ì¦ë¥˜[[Knowledge-Distillation-for-Computer-Vision]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì§€ì‹ ì¦ë¥˜(Knowledge distillation)ëŠ” ë” í¬ê³  ë³µì¡í•œ ëª¨ë¸(êµì‚¬)ì—ì„œ ë” ì‘ê³  ê°„ë‹¨í•œ ëª¨ë¸(í•™ìƒ)ë¡œ ì§€ì‹ì„ ì „ë‹¬í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. í•œ ëª¨ë¸ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì§€ì‹ì„ ì¦ë¥˜í•˜ê¸° ìœ„í•´, íŠ¹ì • ì‘ì—…(ì´ ê²½ìš° ì´ë¯¸ì§€ ë¶„ë¥˜)ì— ëŒ€í•´ í•™ìŠµëœ ì‚¬ì „ í›ˆë ¨ëœ êµì‚¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , ëœë¤ìœ¼ë¡œ ì´ˆê¸°í™”ëœ í•™ìƒ ëª¨ë¸ì„ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤. ê·¸ë‹¤ìŒ, í•™ìƒ ëª¨ë¸ì´ êµì‚¬ ëª¨ë¸ì˜ ì¶œë ¥ì„ ëª¨ë°©í•˜ì—¬ ë‘ ëª¨ë¸ì˜ ì¶œë ¥ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í›ˆë ¨í•©ë‹ˆë‹¤. ì´ ê¸°ë²•ì€ Hinton ë“± ì—°êµ¬ì§„ì˜ [Distilling the Knowledge in a Neural Network](https://huggingface.co/papers/1503.02531)ì—ì„œ ì²˜ìŒ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” íŠ¹ì • ì‘ì—…ì— ë§ì¶˜ ì§€ì‹ ì¦ë¥˜ë¥¼ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤. ì´ë²ˆì—ëŠ” [beans dataset](https://huggingface.co/datasets/beans)ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê°€ì´ë“œëŠ” [ë¯¸ì„¸ ì¡°ì •ëœ ViT ëª¨ë¸](https://huggingface.co/merve/vit-mobilenet-beans-224) (êµì‚¬ ëª¨ë¸)ì„ [MobileNet](https://huggingface.co/google/mobilenet_v2_1.4_224) (í•™ìƒ ëª¨ë¸)ìœ¼ë¡œ ì¦ë¥˜í•˜ëŠ” ë°©ë²•ì„ ğŸ¤— Transformersì˜ [Trainer API](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer) ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "ì¦ë¥˜ì™€ ê³¼ì • í‰ê°€ë¥¼ ìœ„í•´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ ë´…ì‹œë‹¤.\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets accelerate tensorboard evaluate --upgrade\n",
    "```\n",
    "\n",
    "ì´ ì˜ˆì œì—ì„œëŠ” `merve/beans-vit-224` ëª¨ë¸ì„ êµì‚¬ ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ beans ë°ì´í„°ì…‹ì—ì„œ íŒŒì¸ íŠœë‹ëœ `google/vit-base-patch16-224-in21k` ê¸°ë°˜ì˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì„ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ MobileNetV2ë¡œ ì¦ë¥˜í•´ë³¼ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"beans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ê²½ìš° ë‘ ëª¨ë¸ì˜ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œê°€ ë™ì¼í•œ í•´ìƒë„ë¡œ ë™ì¼í•œ ì¶œë ¥ì„ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì—, ë‘ê°€ì§€ë¥¼ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì˜ ëª¨ë“  ë¶„í• ë§ˆë‹¤ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ `dataset`ì˜ `map()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•  ê²ƒ ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "teacher_processor = AutoImageProcessor.from_pretrained(\"merve/beans-vit-224\")\n",
    "\n",
    "def process(examples):\n",
    "    processed_inputs = teacher_processor(examples[\"image\"])\n",
    "    return processed_inputs\n",
    "\n",
    "processed_datasets = dataset.map(process, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìƒ ëª¨ë¸(ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ MobileNet)ì´ êµì‚¬ ëª¨ë¸(íŒŒì¸ íŠœë‹ëœ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸)ì„ ëª¨ë°©í•˜ë„ë¡ í•  ê²ƒ ì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë¨¼ì € êµì‚¬ì™€ í•™ìƒ ëª¨ë¸ì˜ ë¡œì§“ ì¶œë ¥ê°’ì„ êµ¬í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ê° ì¶œë ¥ê°’ì„ ë§¤ê°œë³€ìˆ˜ `temperature` ê°’ìœ¼ë¡œ ë‚˜ëˆ„ëŠ”ë°, ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ê° ì†Œí”„íŠ¸ íƒ€ê²Ÿì˜ ì¤‘ìš”ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ `lambda` ëŠ” ì¦ë¥˜ ì†ì‹¤ì˜ ì¤‘ìš”ë„ì— ê°€ì¤‘ì¹˜ë¥¼ ì¤ë‹ˆë‹¤. ì´ ì˜ˆì œì—ì„œëŠ” `temperature=5`ì™€ `lambda=0.5`ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. í•™ìƒê³¼ êµì‚¬ ê°„ì˜ ë°œì‚°ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ Kullback-Leibler Divergence ì†ì‹¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‘ ë°ì´í„° Pì™€ Qê°€ ì£¼ì–´ì¡Œì„ ë•Œ, KL DivergenceëŠ” Që¥¼ ì‚¬ìš©í•˜ì—¬ Pë¥¼ í‘œí˜„í•˜ëŠ” ë° ì–¼ë§Œí¼ì˜ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œì§€ë¥¼ ë§í•´ì¤ë‹ˆë‹¤. ë‘ ë°ì´í„°ê°€ ë™ì¼í•˜ë‹¤ë©´, KL DivergenceëŠ” 0ì´ë©°, Që¡œ Pë¥¼ ì„¤ëª…í•˜ëŠ” ë° ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì§€ì‹ ì¦ë¥˜ì˜ ë§¥ë½ì—ì„œ KL DivergenceëŠ” ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ImageDistilTrainer(Trainer):\n",
    "    def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.teacher.to(device)\n",
    "        self.teacher.eval()\n",
    "        self.temperature = temperature\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def compute_loss(self, student, inputs, return_outputs=False):\n",
    "        student_output = self.student(**inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "          teacher_output = self.teacher(**inputs)\n",
    "\n",
    "        #  êµì‚¬ì™€ í•™ìƒì˜ ì†Œí”„íŠ¸ íƒ€ê²Ÿ(soft targets) ê³„ì‚°\n",
    "\n",
    "        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n",
    "\n",
    "        # ì†ì‹¤(loss) ê³„ì‚°\n",
    "        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "\n",
    "        # ì‹¤ì œ ë ˆì´ë¸” ì†ì‹¤ ê³„ì‚°\n",
    "        student_target_loss = student_output.loss\n",
    "\n",
    "        # ìµœì¢… ì†ì‹¤ ê³„ì‚°\n",
    "        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss\n",
    "        return (loss, student_output) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ Hugging Face Hubì— ë¡œê·¸ì¸í•˜ì—¬ `Trainer`ë¥¼ í†µí•´ Hugging Face Hubì— ëª¨ë¸ì„ í‘¸ì‹œí•  ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ `TrainingArguments`, êµì‚¬ ëª¨ë¸ê³¼ í•™ìƒ ëª¨ë¸ì„ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my-awesome-model\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repo_name,\n",
    "    )\n",
    "\n",
    "num_labels = len(processed_datasets[\"train\"].features[\"labels\"].names)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "teacher_model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"merve/beans-vit-224\",\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# MobileNetV2 ë°‘ë°”ë‹¥ë¶€í„° í•™ìŠµ\n",
    "student_config = MobileNetV2Config()\n",
    "student_config.num_labels = num_labels\n",
    "student_model = MobileNetV2ForImageClassification(student_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” í›ˆë ¨ ê³¼ì •ì—ì„œ ëª¨ë¸ì˜ `accuracy`ì™€ `f1`ì„ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    acc = accuracy.compute(references=labels, predictions=np.argmax(predictions, axis=1))\n",
    "    return {\"accuracy\": acc[\"accuracy\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì •ì˜í•œ í›ˆë ¨ ì¸ìˆ˜ë¡œ `Trainer`ë¥¼ ì´ˆê¸°í™”í•´ë´…ì‹œë‹¤. ë˜í•œ ë°ì´í„° ì½œë ˆì´í„°(data collator)ë¥¼ ì´ˆê¸°í™”í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "trainer = ImageDistilTrainer(\n",
    "    student_model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    training_args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=teacher_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    temperature=5,\n",
    "    lambda_param=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(processed_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ì˜ ì •í™•ë„ëŠ” 72%ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì¦ë¥˜ì˜ íš¨ìœ¨ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ beans ë°ì´í„°ì…‹ì—ì„œ MobileNetì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ì˜€ê³ , í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œì˜ ì •í™•ë„ëŠ” 63% ì˜€ìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ëœ êµì‚¬ ëª¨ë¸, í•™ìƒ êµ¬ì¡°, ì¦ë¥˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‹œë„í•´ë³´ì‹œê³  ê²°ê³¼ë¥¼ ë³´ê³ í•˜ê¸°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. ì¦ë¥˜ëœ ëª¨ë¸ì˜ í›ˆë ¨ ë¡œê·¸ì™€ ì²´í¬í¬ì¸íŠ¸ëŠ” [ì´ ì €ì¥ì†Œ](https://huggingface.co/merve/vit-mobilenet-beans-224)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìœ¼ë©°, ì²˜ìŒë¶€í„° í›ˆë ¨ëœ MobileNetV2ëŠ” ì´ [ì €ì¥ì†Œ](https://huggingface.co/merve/resnet-mobilenet-beans-5)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
