{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì´ë¯¸ì§€ ìº¡ì…”ë‹[[image-captioning]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ ìº¡ì…”ë‹(Image captioning)ì€ ì£¼ì–´ì§„ ì´ë¯¸ì§€ì— ëŒ€í•œ ìº¡ì…˜ì„ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. \n",
    "ì´ë¯¸ì§€ ìº¡ì…”ë‹ì€ ì‹œê° ì¥ì• ì¸ì´ ë‹¤ì–‘í•œ ìƒí™©ì„ íƒìƒ‰í•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë„ë¡ ì‹œê° ì¥ì• ì¸ì„ ë³´ì¡°í•˜ëŠ” ë“± ì‹¤ìƒí™œì—ì„œ í”íˆ í™œìš©ë©ë‹ˆë‹¤. \n",
    "ë”°ë¼ì„œ ì´ë¯¸ì§€ ìº¡ì…”ë‹ì€ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•¨ìœ¼ë¡œì¨ ì‚¬ëŒë“¤ì˜ ì½˜í…ì¸  ì ‘ê·¼ì„±ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê°€ì´ë“œì—ì„œëŠ” ì†Œê°œí•  ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "* ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.\n",
    "* íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate -q\n",
    "pip install jiwer -q\n",
    "```\n",
    "\n",
    "Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ë©´ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì— ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í¬ì¼“ëª¬ BLIP ìº¡ì…˜ ë°ì´í„°ì„¸íŠ¸ ê°€ì ¸ì˜¤ê¸°[[load-the-pokmon-blip-captions-dataset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ì´ë¯¸ì§€-ìº¡ì…˜} ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì„¸íŠ¸ë¥¼ ê°€ì ¸ì˜¤ë ¤ë©´ ğŸ¤— Dataset ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. \n",
    "PyTorchì—ì„œ ìì‹ ë§Œì˜ ì´ë¯¸ì§€ ìº¡ì…˜ ë°ì´í„°ì„¸íŠ¸ë¥¼ ë§Œë“¤ë ¤ë©´ [ì´ ë…¸íŠ¸ë¶](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb)ì„ ì°¸ì¡°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"lambdalabs/pokemon-blip-captions\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image', 'text'],\n",
    "        num_rows: 833\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "ì´ ë°ì´í„°ì„¸íŠ¸ëŠ” `image`ì™€ `text`ë¼ëŠ” ë‘ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ë§ì€ ì´ë¯¸ì§€ ìº¡ì…˜ ë°ì´í„°ì„¸íŠ¸ì—ëŠ” ì´ë¯¸ì§€ë‹¹ ì—¬ëŸ¬ ê°œì˜ ìº¡ì…˜ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \n",
    "ì´ëŸ¬í•œ ê²½ìš°, ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµ ì¤‘ì— ì‚¬ìš© ê°€ëŠ¥í•œ ìº¡ì…˜ ì¤‘ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œì„ ì¶”ì¶œí•©ë‹ˆë‹¤. \n",
    "\n",
    "</Tip>\n",
    "\n",
    "[~datasets.Dataset.train_test_split] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì„¸íŠ¸ì˜ í•™ìŠµ ë¶„í• ì„ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµ ì„¸íŠ¸ì˜ ìƒ˜í”Œ ëª‡ ê°œë¥¼ ì‹œê°í™”í•´ ë´…ì‹œë‹¤.\n",
    "Let's visualize a couple of samples from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_images(images, captions):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, 12))\n",
    "        plt.title(caption)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "sample_images_to_visualize = [np.array(train_ds[i][\"image\"]) for i in range(5)]\n",
    "sample_captions = [train_ds[i][\"text\"] for i in range(5)]\n",
    "plot_images(sample_images_to_visualize, sample_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_training_images_image_cap.png\" alt=\"Sample training images\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ì„¸íŠ¸ ì „ì²˜ë¦¬[[preprocess-the-dataset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ì„¸íŠ¸ì—ëŠ” ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¼ëŠ” ë‘ ê°€ì§€ ì–‘ì‹ì´ ìˆê¸° ë•Œë¬¸ì—, ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì„ ëª¨ë‘ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²˜ë¦¬ ì‘ì—…ì„ ìœ„í•´, íŒŒì¸íŠœë‹í•˜ë ¤ëŠ” ëª¨ë¸ì— ì—°ê²°ëœ í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "checkpoint = \"microsoft/git-base\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í”„ë¡œì„¸ì„œëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í¬ê¸° ì¡°ì • ë° í”½ì…€ í¬ê¸° ì¡°ì •ì„ í¬í•¨í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ê³  ìº¡ì…˜ì„ í† í°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    captions = [x for x in example_batch[\"text\"]]\n",
    "    inputs = processor(images=images, text=captions, padding=\"max_length\")\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs\n",
    "\n",
    "\n",
    "train_ds.set_transform(transforms)\n",
    "test_ds.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ì„¸íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ ì´ì œ íŒŒì¸íŠœë‹ì„ ìœ„í•´ ëª¨ë¸ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ë³¸ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°[[load-a-base-model]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"microsoft/git-base\"](https://huggingface.co/microsoft/git-base)ë¥¼ [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) ê°ì²´ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í‰ê°€[[evaluate]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ ìº¡ì…˜ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ [Rouge ì ìˆ˜](https://huggingface.co/spaces/evaluate-metric/rouge) ë˜ëŠ” [ë‹¨ì–´ ì˜¤ë¥˜ìœ¨(Word Error Rate)](https://huggingface.co/spaces/evaluate-metric/wer)ë¡œ í‰ê°€í•©ë‹ˆë‹¤. \n",
    "ì´ ê°€ì´ë“œì—ì„œëŠ” ë‹¨ì–´ ì˜¤ë¥˜ìœ¨(WER)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. \n",
    "\n",
    "ì´ë¥¼ ìœ„í•´ ğŸ¤— Evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. \n",
    "WERì˜ ì ì¬ì  ì œí•œ ì‚¬í•­ ë° ê¸°íƒ€ ë¬¸ì œì ì€ [ì´ ê°€ì´ë“œ](https://huggingface.co/spaces/evaluate-metric/wer)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "wer = load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predicted = logits.argmax(-1)\n",
    "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)\n",
    "    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "    return {\"wer_score\": wer_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ![[train!]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ğŸ¤— `Trainer`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. \n",
    "\n",
    "ë¨¼ì €, `TrainingArguments`ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì¸ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-pokemon\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=50,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµ ì¸ìˆ˜ë¥¼ ë°ì´í„°ì„¸íŠ¸, ëª¨ë¸ê³¼ í•¨ê»˜ ğŸ¤— Trainerì— ì „ë‹¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ `Trainer` ê°ì²´ì—ì„œ `train()`ì„ í˜¸ì¶œí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì´ ì§„í–‰ë˜ë©´ì„œ í•™ìŠµ ì†ì‹¤ì´ ì›í™œí•˜ê²Œ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "í•™ìŠµì´ ì™„ë£Œë˜ë©´ ëª¨ë“  ì‚¬ëŒì´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ `push_to_hub()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í—ˆë¸Œì— ê³µìœ í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ë¡ [[inference]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_ds`ì—ì„œ ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/test_image_image_cap.png\" alt=\"Test image\"/>\n",
    "</div>\n",
    "    \n",
    "ëª¨ë¸ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate`ë¥¼ í˜¸ì¶œí•˜ê³  ì˜ˆì¸¡ì„ ë””ì½”ë”©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "a drawing of a pink and blue pokemon\n",
    "```\n",
    "\n",
    "íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ê½¤ ê´œì°®ì€ ìº¡ì…˜ì„ ìƒì„±í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
