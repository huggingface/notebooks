{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê°ê´€ì‹ ë¬¸ì œ[[multiple-choice]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°ê´€ì‹ ê³¼ì œëŠ” ë¬¸ë§¥ê³¼ í•¨ê»˜ ì—¬ëŸ¬ ê°œì˜ í›„ë³´ ë‹µë³€ì´ ì œê³µë˜ê³  ëª¨ë¸ì´ ì •ë‹µì„ ì„ íƒí•˜ë„ë¡ í•™ìŠµëœë‹¤ëŠ” ì ì„ ì œì™¸í•˜ë©´ ì§ˆì˜ì‘ë‹µê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì§„í–‰í•˜ëŠ” ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. [SWAG](https://huggingface.co/datasets/swag) ë°ì´í„° ì„¸íŠ¸ì˜ 'regular' êµ¬ì„±ìœ¼ë¡œ [BERT](https://huggingface.co/google-bert/bert-base-uncased)ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì—¬ëŸ¬ ì˜µì…˜ê³¼ ì¼ë¶€ ì»¨í…ìŠ¤íŠ¸ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê°€ì¥ ì í•©í•œ ë‹µì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "2. ì¶”ë¡ ì— ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate\n",
    "```\n",
    "\n",
    "ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•  ìˆ˜ ìˆë„ë¡ í—ˆê¹…í˜ì´ìŠ¤ ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë©”ì‹œì§€ê°€ í‘œì‹œë˜ë©´ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWAG ë°ì´í„° ì„¸íŠ¸ ê°€ì ¸ì˜¤ê¸°[[load-swag-dataset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¨¼ì € ğŸ¤— Datasets  ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ SWAG ë°ì´í„°ì…‹ì˜ 'ì¼ë°˜' êµ¬ì„±ì„ ê°€ì ¸ì˜µë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "swag = load_dataset(\"swag\", \"regular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ë°ì´í„°ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'fold-ind': '3416',\n",
       " 'gold-source': 'gold',\n",
       " 'label': 0,\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'video-id': 'anetv_jkn6uvmqwh4'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swag[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ì—ëŠ” ë§ì€ í•„ë“œê°€ ìˆëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤:\n",
    "\n",
    "- `sent1` ë° `sent2`: ì´ í•„ë“œëŠ” ë¬¸ì¥ì´ ì–´ë–»ê²Œ ì‹œì‘ë˜ëŠ”ì§€ ë³´ì—¬ì£¼ë©°, ì´ ë‘ í•„ë“œë¥¼ í•©ì¹˜ë©´ `ì‹œì‘ êµ¬ì ˆ(startphrase)` í•„ë“œê°€ ë©ë‹ˆë‹¤.\n",
    "- `ì¢…ë£Œ êµ¬ì ˆ(ending)`: ë¬¸ì¥ì´ ì–´ë–»ê²Œ ëë‚  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ê°€ëŠ¥í•œ ì¢…ë£Œ êµ¬ì ˆë¥¼ ì œì‹œí•˜ì§€ë§Œ ê·¸ ì¤‘ í•˜ë‚˜ë§Œ ì •ë‹µì…ë‹ˆë‹¤.\n",
    "- `ë ˆì´ë¸”(label)`: ì˜¬ë°”ë¥¸ ë¬¸ì¥ ì¢…ë£Œ êµ¬ì ˆì„ ì‹ë³„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì „ì²˜ë¦¬[[preprocess]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒ ë‹¨ê³„ëŠ” ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ë„¤ ê°€ì§€ ê°€ëŠ¥í•œ êµ¬ì ˆì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ BERT í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìƒì„±í•˜ë ¤ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. `sent1` í•„ë“œë¥¼ ë„¤ ê°œ ë³µì‚¬í•œ ë‹¤ìŒ ê°ê°ì„ `sent2`ì™€ ê²°í•©í•˜ì—¬ ë¬¸ì¥ì´ ì‹œì‘ë˜ëŠ” ë°©ì‹ì„ ì¬í˜„í•©ë‹ˆë‹¤.\n",
    "2. `sent2`ë¥¼ ë„¤ ê°€ì§€ ê°€ëŠ¥í•œ ë¬¸ì¥ êµ¬ì ˆ ê°ê°ê³¼ ê²°í•©í•©ë‹ˆë‹¤.\n",
    "3. ì´ ë‘ ëª©ë¡ì„ í† í°í™”í•  ìˆ˜ ìˆë„ë¡ í‰íƒ„í™”(flatten)í•˜ê³ , ê° ì˜ˆì œì— í•´ë‹¹í•˜ëŠ” `input_ids`, `attention_mask` ë° `labels` í•„ë“œë¥¼ ê°–ë„ë¡ ë‹¤ì°¨ì›í™”(unflatten) í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n",
    "    question_headers = examples[\"sent2\"]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²´ ë°ì´í„° ì§‘í•©ì— ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasets `map` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `batched=True`ë¥¼ ì„¤ì •í•˜ì—¬ ë°ì´í„° ì§‘í•©ì˜ ì—¬ëŸ¬ ìš”ì†Œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë©´ `map` í•¨ìˆ˜ì˜ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_swag = swag.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataCollatorForMultipleChoice`ëŠ” ëª¨ë“  ëª¨ë¸ ì…ë ¥ì„ í‰íƒ„í™”í•˜ê³  íŒ¨ë”©ì„ ì ìš©í•˜ë©° ê·¸ ê²°ê³¼ë¥¼ ê²°ê³¼ë¥¼ ë‹¤ì°¨ì›í™”í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForMultipleChoice\n",
    "collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í‰ê°€ í•˜ê¸°[[evaluate]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í›ˆë ¨ ì¤‘ì— ë©”íŠ¸ë¦­ì„ í¬í•¨í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ğŸ¤—[Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ë°©ë²•ì„ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì—ì„œëŠ” [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) ì§€í‘œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤(ğŸ¤— Evaluate [ë‘˜ëŸ¬ë³´ê¸°](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì°¸ì¡°í•˜ì—¬ ì§€í‘œë¥¼ ê°€ì ¸ì˜¤ê³  ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ì„¸ìš”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë¦¬ê³  ì˜ˆì¸¡ê³¼ ë ˆì´ë¸”ì„ `compute`ì— ì „ë‹¬í•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ `compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìœ¼ë©°, í›ˆë ¨ì„ ì„¤ì •í•  ë•Œ ì´ í•¨ìˆ˜ë¡œ ëŒì•„ê°€ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í›ˆë ¨ í•˜ê¸°[[train]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ê°ê´€ì‹ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ë³´ë‹¤ ì‹¬ì¸µì ì¸ ì˜ˆëŠ” ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
    "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\n",
    "ë˜ëŠ” [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ë¡  í•˜ê¸°[[inference]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í–ˆìœ¼ë‹ˆ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "í…ìŠ¤íŠ¸ì™€ ë‘ ê°œì˜ í›„ë³´ ë‹µì•ˆì„ ì‘ì„±í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"France has a bread law, Le DÃ©cret Pain, with strict rules on what is allowed in a traditional baguette.\"\n",
    "candidate1 = \"The law does not apply to croissants and brioche.\"\n",
    "candidate2 = \"The law applies to baguettes.\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
