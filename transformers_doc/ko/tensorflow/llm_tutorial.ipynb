{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ ìƒì„±í•˜ê¸° [[generation-with-llms]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM ë˜ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ ìƒì„±ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤. ê°„ë‹¨íˆ ë§í•˜ë©´, ì£¼ì–´ì§„ ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë‹¤ìŒ ë‹¨ì–´(ì •í™•í•˜ê²ŒëŠ” í† í°)ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ í›ˆë ¨ëœ ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ ë³€í™˜ê¸° ëª¨ë¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. í† í°ì„ í•œ ë²ˆì— í•˜ë‚˜ì”© ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ì— ìƒˆë¡œìš´ ë¬¸ì¥ì„ ìƒì„±í•˜ë ¤ë©´ ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ê²ƒ ì™¸ì— ë” ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, ìê¸°íšŒê·€ ìƒì„±ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ìê¸°íšŒê·€ ìƒì„±ì€ ëª‡ ê°œì˜ ì´ˆê¸° ì…ë ¥ê°’ì„ ì œê³µí•œ í›„, ê·¸ ì¶œë ¥ì„ ë‹¤ì‹œ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë°˜ë³µì ìœ¼ë¡œ í˜¸ì¶œí•˜ëŠ” ì¶”ë¡  ê³¼ì •ì…ë‹ˆë‹¤. ğŸ¤— Transformersì—ì„œëŠ” [generate()](https://huggingface.co/docs/transformers/main/ko/main_classes/text_generation#transformers.GenerationMixin.generate) ë©”ì†Œë“œê°€ ì´ ì—­í• ì„ í•˜ë©°, ì´ëŠ” ìƒì„± ê¸°ëŠ¥ì„ ê°€ì§„ ëª¨ë“  ëª¨ë¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë£¨ê²Œ ë©ë‹ˆë‹¤:\n",
    "\n",
    "* LLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "* ì¼ë°˜ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¬¸ì œ í•´ê²°\n",
    "* LLMì„ ìµœëŒ€í•œ í™œìš©í•˜ê¸° ìœ„í•œ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:\n",
    "\n",
    "```bash\n",
    "pip install transformers bitsandbytes>=0.39.0 -q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í…ìŠ¤íŠ¸ ìƒì„± [[generate-text]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§(causal language modeling)](https://huggingface.co/docs/transformers/main/ko/tasks/language_modeling)ì„ ëª©ì ìœ¼ë¡œ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì€ ì¼ë ¨ì˜ í…ìŠ¤íŠ¸ í† í°ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ê·¸ ê²°ê³¼ë¡œ ë‹¤ìŒ í† í°ì´ ë‚˜ì˜¬ í™•ë¥  ë¶„í¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "<!-- [GIF 1 -- FWD PASS] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"LLMì˜ ì „ë°© íŒ¨ìŠ¤\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "LLMê³¼ ìê¸°íšŒê·€ ìƒì„±ì„ í•¨ê»˜ ì‚¬ìš©í•  ë•Œ í•µì‹¬ì ì¸ ë¶€ë¶„ì€ ì´ í™•ë¥  ë¶„í¬ë¡œë¶€í„° ë‹¤ìŒ í† í°ì„ ì–´ë–»ê²Œ ê³ ë¥¼ ê²ƒì¸ì§€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë°˜ë³µ ê³¼ì •ì— ì‚¬ìš©ë  í† í°ì„ ê²°ì •í•˜ëŠ” í•œ, ì–´ë– í•œ ë°©ë²•ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤. í™•ë¥  ë¶„í¬ì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ í† í°ì„ ì„ íƒí•˜ëŠ” ê²ƒì²˜ëŸ¼ ê°„ë‹¨í•  ìˆ˜ë„ ìˆê³ , ê²°ê³¼ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•˜ê¸° ì „ì— ìˆ˜ì‹­ ê°€ì§€ ë³€í™˜ì„ ì ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³µì¡í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "<!-- [GIF 2 -- TEXT GENERATION] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"ìê¸°íšŒê·€ ìƒì„±ì€ í™•ë¥  ë¶„í¬ì—ì„œ ë‹¤ìŒ í† í°ì„ ë°˜ë³µì ìœ¼ë¡œ ì„ íƒí•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "ìœ„ì—ì„œ ì„¤ëª…í•œ ê³¼ì •ì€ ì–´ë–¤ ì¢…ë£Œ ì¡°ê±´ì´ ì¶©ì¡±ë  ë•Œê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ëª¨ë¸ì´ ì‹œí€€ìŠ¤ì˜ ë(EOS í† í°)ì„ ì¶œë ¥í•  ë•Œê¹Œì§€ë¥¼ ì¢…ë£Œ ì¡°ê±´ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ì´ìƒì ì…ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ë¯¸ë¦¬ ì •ì˜ëœ ìµœëŒ€ ê¸¸ì´ì— ë„ë‹¬í–ˆì„ ë•Œ ìƒì„±ì´ ì¤‘ë‹¨ë©ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì´ ì˜ˆìƒëŒ€ë¡œ ë™ì‘í•˜ê¸° ìœ„í•´ì„  í† í° ì„ íƒ ë‹¨ê³„ì™€ ì •ì§€ ì¡°ê±´ì„ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ, ê° ëª¨ë¸ì—ëŠ” ê¸°ë³¸ ìƒì„± ì„¤ì •ì´ ì˜ ì •ì˜ëœ [GenerationConfig](https://huggingface.co/docs/transformers/main/ko/main_classes/text_generation#transformers.GenerationConfig) íŒŒì¼ì´ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤.\n",
    "\n",
    "ì½”ë“œë¥¼ í™•ì¸í•´ë´…ì‹œë‹¤!\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ê¸°ë³¸ LLM ì‚¬ìš©ì— ê´€ì‹¬ì´ ìˆë‹¤ë©´, ìš°ë¦¬ì˜ [`Pipeline`](https://huggingface.co/docs/transformers/main/ko/pipeline_tutorial) ì¸í„°í˜ì´ìŠ¤ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ LLMì€ ì–‘ìí™”ë‚˜ í† í° ì„ íƒ ë‹¨ê³„ì—ì„œì˜ ë¯¸ì„¸í•œ ì œì–´ì™€ ê°™ì€ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ì¢…ì¢… í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‘ì—…ì€ [generate()](https://huggingface.co/docs/transformers/main/ko/main_classes/text_generation#transformers.GenerationMixin.generate)ë¥¼ í†µí•´ ê°€ì¥ ì˜ ìˆ˜í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LLMì„ ì´ìš©í•œ ìê¸°íšŒê·€ ìƒì„±ì€ ìì›ì„ ë§ì´ ì†Œëª¨í•˜ë¯€ë¡œ, ì ì ˆí•œ ì²˜ë¦¬ëŸ‰ì„ ìœ„í•´ GPUì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ë¨¼ì €, ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_pretrained` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ë•Œ 2ê°œì˜ í”Œë˜ê·¸ë¥¼ ì£¼ëª©í•˜ì„¸ìš”:\n",
    "\n",
    "- `device_map`ì€ ëª¨ë¸ì´ GPUë¡œ ì´ë™ë˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "- `load_in_4bit`ëŠ” ë¦¬ì†ŒìŠ¤ ìš”êµ¬ ì‚¬í•­ì„ í¬ê²Œ ì¤„ì´ê¸° ìœ„í•´ [4ë¹„íŠ¸ ë™ì  ì–‘ìí™”](https://huggingface.co/docs/transformers/main/ko/main_classes/quantization)ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì™¸ì—ë„ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆì§€ë§Œ, LLMì„ ì²˜ìŒ ì‹œì‘í•  ë•Œ ì´ ì„¤ì •ì„ ì¶”ì²œí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì–´ì„œ í…ìŠ¤íŠ¸ ì…ë ¥ì„ [í† í¬ë‚˜ì´ì €](https://huggingface.co/docs/transformers/main/ko/tokenizer_summary)ìœ¼ë¡œ ì „ì²˜ë¦¬í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_inputs` ë³€ìˆ˜ì—ëŠ” í† í°í™”ëœ í…ìŠ¤íŠ¸ ì…ë ¥ê³¼ í•¨ê»˜ ì–´í…ì…˜ ë§ˆìŠ¤í¬ê°€ ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. [generate()](https://huggingface.co/docs/transformers/main/ko/main_classes/text_generation#transformers.GenerationMixin.generate)ëŠ” ì–´í…ì…˜ ë§ˆìŠ¤í¬ê°€ ì œê³µë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ì—ë„ ì´ë¥¼ ì¶”ë¡ í•˜ë ¤ê³  ë…¸ë ¥í•˜ì§€ë§Œ, ìµœìƒì˜ ì„±ëŠ¥ì„ ìœ„í•´ì„œëŠ” ê°€ëŠ¥í•˜ë©´ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. \n",
    "\n",
    "ë§ˆì§€ë§‰ìœ¼ë¡œ [generate()](https://huggingface.co/docs/transformers/main/ko/main_classes/text_generation#transformers.GenerationMixin.generate) ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•´ ìƒì„±ëœ í† í°ì„ ì–»ì€ í›„, ì´ë¥¼ ì¶œë ¥í•˜ê¸° ì „ì— í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A list of colors: red, blue, green, yellow, black, white, and brown'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ê²Œ ì „ë¶€ì…ë‹ˆë‹¤! ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ LLMì˜ ëŠ¥ë ¥ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¼ë°˜ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¬¸ì œ [[common-pitfalls]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ìƒì„± ì „ëµ](https://huggingface.co/docs/transformers/main/ko/generation_strategies)ì´ ë§ê³ , ê¸°ë³¸ê°’ì´ í•­ìƒ ì‚¬ìš© ì‚¬ë¡€ì— ì í•©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶œë ¥ì´ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ë•Œ í”íˆ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ëª©ë¡ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Mistral has no pad token by default\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìƒì„±ëœ ì¶œë ¥ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë‹¤ [[generated-output-is-too-shortlong]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GenerationConfig](https://huggingface.co/docs/transformers/main/ko/main_classes/text_generation#transformers.GenerationConfig) íŒŒì¼ì—ì„œ ë³„ë„ë¡œ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´, `generate`ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ìµœëŒ€ 20ê°œì˜ í† í°ì„ ë°˜í™˜í•©ë‹ˆë‹¤. `generate` í˜¸ì¶œì—ì„œ `max_new_tokens`ì„ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë°˜í™˜í•  ìˆ˜ ìˆëŠ” ìƒˆ í† í°ì˜ ìµœëŒ€ ìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. LLM(ì •í™•í•˜ê²ŒëŠ” [ë””ì½”ë” ì „ìš© ëª¨ë¸](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt))ì€ ì…ë ¥ í”„ë¡¬í”„íŠ¸ë„ ì¶œë ¥ì˜ ì¼ë¶€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# By default, the output will contain up to 20 tokens\n",
    "generated_ids = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting `max_new_tokens` allows you to control the maximum length\n",
    "generated_ids = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜ëª»ëœ ìƒì„± ëª¨ë“œ [[incorrect-generation-mode]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê¸°ë³¸ì ìœ¼ë¡œ [GenerationConfig](https://huggingface.co/docs/transformers/main/ko/main_classes/text_generation#transformers.GenerationConfig) íŒŒì¼ì—ì„œ ë³„ë„ë¡œ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´, `generate`ëŠ” ê° ë°˜ë³µì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í°ì„ ì„ íƒí•©ë‹ˆë‹¤(ê·¸ë¦¬ë”” ë””ì½”ë”©). í•˜ë ¤ëŠ” ì‘ì—…ì— ë”°ë¼ ì´ ë°©ë²•ì€ ë°”ëŒì§í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì±—ë´‡ì´ë‚˜ ì—ì„¸ì´ ì‘ì„±ê³¼ ê°™ì€ ì°½ì˜ì ì¸ ì‘ì—…ì€ ìƒ˜í”Œë§ì´ ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ë²ˆì—­ê³¼ ê°™ì€ ì…ë ¥ ê¸°ë°˜ ì‘ì—…ì€ ê·¸ë¦¬ë”” ë””ì½”ë”©ì´ ë” ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `do_sample=True`ë¡œ ìƒ˜í”Œë§ì„ í™œì„±í™”í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ì£¼ì œì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì´ [ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://huggingface.co/blog/how-to-generate)ì—ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat. I am a cat. I am a cat. I am a cat'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed or reproducibility -- you don't need this unless you want full reproducibility\n",
    "from transformers import set_seed\n",
    "set_seed(0)\n",
    "\n",
    "model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# LLM + greedy decoding = repetitive, boring output\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat.\\nI just need to be. I am always.\\nEvery time'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With sampling, the output becomes more creative!\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜ëª»ëœ íŒ¨ë”© [[wrong-padding-side]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMì€ [ë””ì½”ë” ì „ìš©](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt) êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆì–´, ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì§€ì†ì ìœ¼ë¡œ ë°˜ë³µ ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°ì˜ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ íŒ¨ë”© ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤. LLMì€ íŒ¨ë”© í† í°ì—ì„œ ì‘ë™ì„ ì´ì–´ê°€ë„ë¡ ì„¤ê³„ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, ì…ë ¥ ì™¼ìª½ì— íŒ¨ë”©ì´ ì¶”ê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì–´í…ì…˜ ë§ˆìŠ¤í¬ë„ ê¼­ `generate` í•¨ìˆ˜ì— ì „ë‹¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer initialized above has right-padding active by default: the 1st sequence,\n",
    "# which is shorter, has padding on the right side. Generation fails.\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids[0], skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 3, 4, 5, 6,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With left-padding, it works as expected!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO: when the prompting guide is ready, mention the importance of setting the right prompt in this section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ê°€ ìë£Œ [[further-resources]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìê¸°íšŒê·€ ìƒì„± í”„ë¡œì„¸ìŠ¤ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë‹¨ìˆœí•œ í¸ì´ì§€ë§Œ, LLMì„ ìµœëŒ€í•œ í™œìš©í•˜ë ¤ë©´ ì—¬ëŸ¬ ê°€ì§€ ìš”ì†Œë¥¼ ê³ ë ¤í•´ì•¼ í•˜ë¯€ë¡œ ì‰½ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. LLMì— ëŒ€í•œ ë” ê¹Šì€ ì´í•´ì™€ í™œìš©ì„ ìœ„í•œ ë‹¤ìŒ ë‹¨ê³„ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "<!-- TODO: complete with new guides -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê³ ê¸‰ ìƒì„± ì‚¬ìš© [[advanced-generate-usage]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ê°€ì´ë“œ](https://huggingface.co/docs/transformers/main/ko/generation_strategies)ëŠ” ë‹¤ì–‘í•œ ìƒì„± ë°©ë²•ì„ ì œì–´í•˜ëŠ” ë°©ë²•, ìƒì„± ì„¤ì • íŒŒì¼ì„ ì„¤ì •í•˜ëŠ” ë°©ë²•, ì¶œë ¥ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "2. [GenerationConfig](https://huggingface.co/docs/transformers/main/ko/main_classes/text_generation#transformers.GenerationConfig)ì™€ [generate()](https://huggingface.co/docs/transformers/main/ko/main_classes/text_generation#transformers.GenerationMixin.generate), [generate-related classes](https://huggingface.co/docs/transformers/main/ko/internal/generation_utils)ë¥¼ ì°¸ì¡°í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM ë¦¬ë”ë³´ë“œ [[llm-leaderboards]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ì˜ í’ˆì§ˆì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.\n",
    "2. [Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)ëŠ” LLM ì²˜ë¦¬ëŸ‰ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì§€ì—° ì‹œê°„ ë° ì²˜ë¦¬ëŸ‰ [[latency-and-throughput]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ ì¤„ì´ë ¤ë©´, ë™ì  ì–‘ìí™”ì— ëŒ€í•œ [ê°€ì´ë“œ](https://huggingface.co/docs/transformers/main/ko/main_classes/quantization)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ [[related-libraries]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`text-generation-inference`](https://github.com/huggingface/text-generation-inference)ëŠ” LLMì„ ìœ„í•œ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì— ì í•©í•œ ì„œë²„ì…ë‹ˆë‹¤.\n",
    "2. [`optimum`](https://github.com/huggingface/optimum)ì€ íŠ¹ì • í•˜ë“œì›¨ì–´ ì¥ì¹˜ì—ì„œ LLMì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ğŸ¤— Transformersë¥¼ í™•ì¥í•œ ê²ƒì…ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
