{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì´ë¯¸ì§€ ë¶„ë¥˜[[image-classification]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tjAIM7BOYhw?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tjAIM7BOYhw?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ ë¶„ë¥˜ëŠ” ì´ë¯¸ì§€ì— ë ˆì´ë¸” ë˜ëŠ” í´ë˜ìŠ¤ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë˜ëŠ” ì˜¤ë””ì˜¤ ë¶„ë¥˜ì™€ ë‹¬ë¦¬ ì…ë ¥ì€\n",
    "ì´ë¯¸ì§€ë¥¼ êµ¬ì„±í•˜ëŠ” í”½ì…€ ê°’ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ë¶„ë¥˜ì—ëŠ” ìì—°ì¬í•´ í›„ í”¼í•´ ê°ì§€, ë†ì‘ë¬¼ ê±´ê°• ëª¨ë‹ˆí„°ë§, ì˜ë£Œ ì´ë¯¸ì§€ì—ì„œ ì§ˆë³‘ì˜ ì§•í›„ ê²€ì‚¬ ì§€ì› ë“±\n",
    "ë‹¤ì–‘í•œ ì‘ìš© ì‚¬ë¡€ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê°€ì´ë“œì—ì„œëŠ” ë‹¤ìŒì„ ì„¤ëª…í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. [Food-101](https://huggingface.co/datasets/food101) ë°ì´í„° ì„¸íŠ¸ì—ì„œ [ViT](https://huggingface.co/docs/transformers/main/ko/tasks/model_doc/vit)ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ì‹í’ˆ í•­ëª©ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n",
    "2. ì¶”ë¡ ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "<Tip>\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì„¤ëª…í•˜ëŠ” ì‘ì—…ì€ ë‹¤ìŒ ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ì˜í•´ ì§€ì›ë©ë‹ˆë‹¤:\n",
    "\n",
    "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
    "\n",
    "[BEiT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/beit), [BiT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/bit), [ConvNeXT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/convnext), [ConvNeXTV2](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/convnextv2), [CvT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/cvt), [Data2VecVision](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/data2vec-vision), [DeiT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/deit), [DiNAT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/dinat), [EfficientFormer](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/efficientformer), [EfficientNet](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/efficientnet), [FocalNet](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/focalnet), [ImageGPT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/imagegpt), [LeViT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/levit), [MobileNetV1](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/mobilenet_v1), [MobileNetV2](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/mobilenet_v2), [MobileViT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/mobilevit), [NAT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/nat), [Perceiver](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/perceiver), [PoolFormer](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/poolformer), [RegNet](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/regnet), [ResNet](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/resnet), [SegFormer](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/segformer), [Swin Transformer](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/swin), [Swin Transformer V2](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/swinv2), [VAN](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/van), [ViT](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/vit), [ViT Hybrid](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/vit_hybrid), [ViTMSN](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/vit_msn)\n",
    "<!--End of the generated tip-->\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì—, í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate\n",
    "```\n",
    "\n",
    "Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì— ê³µìœ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë©”ì‹œì§€ê°€ í‘œì‹œë˜ë©´, í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food-101 ë°ì´í„° ì„¸íŠ¸ ê°€ì ¸ì˜¤ê¸°[[load-food101-dataset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ Food-101 ë°ì´í„° ì„¸íŠ¸ì˜ ë” ì‘ì€ ë¶€ë¶„ ì§‘í•©ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ\n",
    "í›ˆë ¨ì— ë§ì€ ì‹œê°„ì„ í• ì• í•˜ê¸° ì „ì— ì‹¤í—˜ì„ í†µí•´ ëª¨ë“  ê²ƒì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì„¸íŠ¸ì˜ `train`ì„ `train_test_split` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë¦¬ê³  ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F52AFC8AC50>,\n",
       " 'label': 79}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì„¸íŠ¸ì˜ ê° ì˜ˆì œì—ëŠ” ë‘ ê°œì˜ í•„ë“œê°€ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- `image`: ì‹í’ˆ í•­ëª©ì˜ PIL ì´ë¯¸ì§€\n",
    "- `label`: ì‹í’ˆ í•­ëª©ì˜ ë ˆì´ë¸” í´ë˜ìŠ¤\n",
    "\n",
    "ëª¨ë¸ì´ ë ˆì´ë¸” IDì—ì„œ ë ˆì´ë¸” ì´ë¦„ì„ ì‰½ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡\n",
    "ë ˆì´ë¸” ì´ë¦„ì„ ì •ìˆ˜ë¡œ ë§¤í•‘í•˜ê³ , ì •ìˆ˜ë¥¼ ë ˆì´ë¸” ì´ë¦„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ì‚¬ì „ì„ ë§Œë“œì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ë ˆì´ë¸” IDë¥¼ ë ˆì´ë¸” ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prime_rib'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(79)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì „ì²˜ë¦¬[[preprocess]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒ ë‹¨ê³„ëŠ” ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ViT ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ì— ëª‡ ê°€ì§€ ì´ë¯¸ì§€ ë³€í™˜ì„ ì ìš©í•˜ì—¬ ê³¼ì í•©ì— ëŒ€í•´ ëª¨ë¸ì„ ë” ê²¬ê³ í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ì—¬ê¸°ì„œ Torchvisionì˜ [`transforms`](https://pytorch.org/vision/stable/transforms.html) ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì›í•˜ëŠ” ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ë¯¸ì§€ì˜ ì„ì˜ ë¶€ë¶„ì„ í¬ë¡­í•˜ê³  í¬ê¸°ë¥¼ ì¡°ì •í•œ ë‹¤ìŒ, ì´ë¯¸ì§€ í‰ê· ê³¼ í‘œì¤€ í¸ì°¨ë¡œ ì •ê·œí™”í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³€í™˜ì„ ì ìš©í•˜ê³  ì´ë¯¸ì§€ì˜ `pixel_values`(ëª¨ë¸ì— ëŒ€í•œ ì…ë ¥)ë¥¼ ë°˜í™˜í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasets `with_transform`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ì˜ ìš”ì†Œë¥¼ ê°€ì ¸ì˜¬ ë•Œ ë³€í™˜ì´ ì¦‰ì‹œ ì ìš©ë©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ `DefaultDataCollator`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì œ ë°°ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ğŸ¤— Transformersì˜ ë‹¤ë¥¸ ë°ì´í„° ì½œë ˆì´í„°ì™€ ë‹¬ë¦¬, `DefaultDataCollator`ëŠ” íŒ¨ë”©ê³¼ ê°™ì€ ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì„ ë³´ë‹¤ ê²¬ê³ í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ë°ì´í„° ì„¸íŠ¸ì˜ í›ˆë ¨ ë¶€ë¶„ì— ë°ì´í„° ì¦ê°•ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "ì—¬ê¸°ì„œ Keras ì „ì²˜ë¦¬ ë ˆì´ì–´ë¡œ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ë³€í™˜(ë°ì´í„° ì¦ê°• í¬í•¨)ê³¼\n",
    "ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ë³€í™˜(ì¤‘ì•™ í¬ë¡œí•‘, í¬ê¸° ì¡°ì •, ì •ê·œí™”ë§Œ)ì„ ì •ì˜í•©ë‹ˆë‹¤. \n",
    "`tf.image` ë˜ëŠ” ë‹¤ë¥¸ ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    "train_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"train_data_augmentation\",\n",
    ")\n",
    "\n",
    "val_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.CenterCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ],\n",
    "    name=\"val_data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ í•œ ë²ˆì— í•˜ë‚˜ì˜ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë¼ ì´ë¯¸ì§€ ë°°ì¹˜ì— ì ì ˆí•œ ë³€í™˜ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def convert_to_tf_tensor(image: Image):\n",
    "    np_image = np.array(image)\n",
    "    tf_image = tf.convert_to_tensor(np_image)\n",
    "    # `expand_dims()` is used to add a batch dimension since\n",
    "    # the TF augmentation layers operates on batched inputs.\n",
    "    return tf.expand_dims(tf_image, 0)\n",
    "\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Datasets `set_transform`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦‰ì‹œ ë³€í™˜ì„ ì ìš©í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food[\"train\"].set_transform(preprocess_train)\n",
    "food[\"test\"].set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìµœì¢… ì „ì²˜ë¦¬ ë‹¨ê³„ë¡œ `DefaultDataCollator`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì œ ë°°ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ğŸ¤— Transformersì˜ ë‹¤ë¥¸ ë°ì´í„° ì½œë ˆì´í„°ì™€ ë‹¬ë¦¬\n",
    "`DefaultDataCollator`ëŠ” íŒ¨ë”©ê³¼ ê°™ì€ ì¶”ê°€ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í‰ê°€[[evaluate]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í›ˆë ¨ ì¤‘ì— í‰ê°€ ì§€í‘œë¥¼ í¬í•¨í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
    "ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í‰ê°€ ë°©ë²•ì„ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì—ì„œëŠ” \n",
    "[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) í‰ê°€ ì§€í‘œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (ğŸ¤— Evaluate [ë¹ ë¥¸ ë‘˜ëŸ¬ë³´ê¸°](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì°¸ì¡°í•˜ì—¬ í‰ê°€ ì§€í‘œë¥¼ ê°€ì ¸ì˜¤ê³  ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ì„¸ìš”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ì˜ˆì¸¡ê³¼ ë ˆì´ë¸”ì„ `compute`ì— ì „ë‹¬í•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ `compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìœ¼ë©°, í›ˆë ¨ì„ ì„¤ì •í•˜ë©´ ì´ í•¨ìˆ˜ë¡œ ë˜ëŒì•„ì˜¬ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í›ˆë ¨[[train]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "`Trainer`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, [ì—¬ê¸°](https://huggingface.co/docs/transformers/main/ko/tasks/../training#train-with-pytorch-trainer)ì—ì„œ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ì´ì œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! `AutoModelForImageClassification`ë¡œ ViTë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ì˜ˆìƒë˜ëŠ” ë ˆì´ë¸” ìˆ˜, ë ˆì´ë¸” ë§¤í•‘ ë° ë ˆì´ë¸” ìˆ˜ë¥¼ ì§€ì •í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì„¸ ë‹¨ê³„ë§Œ ê±°ì¹˜ë©´ ëì…ë‹ˆë‹¤:\n",
    "\n",
    "1. `TrainingArguments`ì—ì„œ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ì„¸ìš”. `image` ì—´ì´ ì‚­ì œë˜ê¸° ë•Œë¬¸ì— ë¯¸ì‚¬ìš© ì—´ì„ ì œê±°í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. `image` ì—´ì´ ì—†ìœ¼ë©´ `pixel_values`ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ë™ì‘ì„ ë°©ì§€í•˜ë ¤ë©´ `remove_unused_columns=False`ë¡œ ì„¤ì •í•˜ì„¸ìš”! ë‹¤ë¥¸ ìœ ì¼í•œ í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ëŠ” ëª¨ë¸ ì €ì¥ ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ëŠ” `output_dir`ì…ë‹ˆë‹¤. `push_to_hub=True`ë¡œ ì„¤ì •í•˜ë©´ ì´ ëª¨ë¸ì„ í—ˆë¸Œì— í‘¸ì‹œí•©ë‹ˆë‹¤(ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ Hugging Faceì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤). ê° ì—í­ì´ ëë‚  ë•Œë§ˆë‹¤, `Trainer`ê°€ ì •í™•ë„ë¥¼ í‰ê°€í•˜ê³  í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "2. `Trainer`ì— ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, í† í¬ë‚˜ì´ì €, ë°ì´í„° ì½œë ˆì´í„° ë° `compute_metrics` í•¨ìˆ˜ì™€ í•¨ê»˜ í›ˆë ¨ ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "3. `train()`ì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_food_model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í›ˆë ¨ì´ ì™„ë£Œë˜ë©´, ëª¨ë“  ì‚¬ëŒì´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ `push_to_hub()` ë©”ì†Œë“œë¡œ ëª¨ë¸ì„ í—ˆë¸Œì— ê³µìœ í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, ë¨¼ì € [ê¸°ë³¸ íŠœí† ë¦¬ì–¼](https://huggingface.co/docs/transformers/main/ko/tasks/./training#train-a-tensorflow-model-with-keras)ì„ í™•ì¸í•˜ì„¸ìš”!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:\n",
    "1. í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ê³  ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "2. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.\n",
    "3. ğŸ¤— Datasetì„ `tf.data.Dataset`ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "4. ëª¨ë¸ì„ ì»´íŒŒì¼í•©ë‹ˆë‹¤.\n",
    "5. ì½œë°±ì„ ì¶”ê°€í•˜ê³  í›ˆë ¨ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ `fit()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "6. ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ ğŸ¤— Hubì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì´í¼íŒŒë¼ë¯¸í„°, ì˜µí‹°ë§ˆì´ì € ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ì„ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_train_steps = len(food[\"train\"]) * num_epochs\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ë ˆì´ë¸” ë§¤í•‘ê³¼ í•¨ê»˜ `TFAuto ModelForImageClassification`ìœ¼ë¡œ ViTë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì„¸íŠ¸ë¥¼ `to_tf_dataset`ì™€ `data_collator`ë¥¼ ì‚¬ìš©í•˜ì—¬ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = food[\"train\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# converting our test dataset to tf.data.Dataset\n",
    "tf_eval_dataset = food[\"test\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compile()`ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ëª¨ë¸ì„ êµ¬ì„±í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜ˆì¸¡ì—ì„œ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  ëª¨ë¸ì„ ğŸ¤— Hubë¡œ í‘¸ì‹œí•˜ë ¤ë©´ [Keras callbacks](https://huggingface.co/docs/transformers/main/ko/tasks/../main_classes/keras_callbacks)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "`compute_metrics` í•¨ìˆ˜ë¥¼ [KerasMetricCallback](https://huggingface.co/docs/transformers/main/ko/tasks/../main_classes/keras_callbacks#transformers.KerasMetricCallback)ì— ì „ë‹¬í•˜ê³ , \n",
    "[PushToHubCallback](https://huggingface.co/docs/transformers/main/ko/tasks/../main_classes/keras_callbacks#transformers.PushToHubCallback)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"food_classifier\",\n",
    "    tokenizer=image_processor,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í­ ìˆ˜ì™€ í•¨ê»˜ `fit()`ì„ í˜¸ì¶œí•˜ê³ ,\n",
    "ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1/5\n",
       "250/250 [==============================] - 313s 1s/step - loss: 2.5623 - val_loss: 1.4161 - accuracy: 0.9290\n",
       "Epoch 2/5\n",
       "250/250 [==============================] - 265s 1s/step - loss: 0.9181 - val_loss: 0.6808 - accuracy: 0.9690\n",
       "Epoch 3/5\n",
       "250/250 [==============================] - 252s 1s/step - loss: 0.3910 - val_loss: 0.4303 - accuracy: 0.9820\n",
       "Epoch 4/5\n",
       "250/250 [==============================] - 251s 1s/step - loss: 0.2028 - val_loss: 0.3191 - accuracy: 0.9900\n",
       "Epoch 5/5\n",
       "250/250 [==============================] - 238s 949ms/step - loss: 0.1232 - val_loss: 0.3259 - accuracy: 0.9890"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶•í•˜í•©ë‹ˆë‹¤! ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê³  ğŸ¤— Hubì— ê³µìœ í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ìì„¸í•œ ì˜ˆì œëŠ” ë‹¤ìŒ [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)ì„ ì°¸ì¡°í•˜ì„¸ìš”.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ë¡ [[inference]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¢‹ì•„ìš”, ì´ì œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í–ˆìœ¼ë‹ˆ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³ ì í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ë´…ì‹œë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"food101\", split=\"validation[:10]\")\n",
    "image = ds[\"image\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\" alt=\"image of beignets\"/>\n",
    "</div>\n",
    "\n",
    "ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ë¡œ ì¶”ë¡ ì„ ì‹œë„í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ `pipeline()`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ `pipeline`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.31856709718704224, 'label': 'beignets'},\n",
       " {'score': 0.015232225880026817, 'label': 'bruschetta'},\n",
       " {'score': 0.01519392803311348, 'label': 'chicken_wings'},\n",
       " {'score': 0.013022331520915031, 'label': 'pork_chop'},\n",
       " {'score': 0.012728818692266941, 'label': 'prime_rib'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model\")\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì›í•œë‹¤ë©´, `pipeline`ì˜ ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ê°€ì ¸ì˜¤ê³  `input`ì„ PyTorch í…ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "import torch\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"my_awesome_food_model\")\n",
    "inputs = image_processor(image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  logitsì„ ë°˜í™˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"my_awesome_food_model\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ê°€ì ¸ì˜¤ê³ , ëª¨ë¸ì˜ `id2label` ë§¤í•‘ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beignets'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label = logits.argmax(-1).item()\n",
    "model.config.id2label[predicted_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ê°€ì ¸ì˜¤ê³  `input`ì„ TensorFlow í…ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MariaK/food_classifier\")\n",
    "inputs = image_processor(image, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  logitsì„ ë°˜í™˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\"MariaK/food_classifier\")\n",
    "logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ê°€ì ¸ì˜¤ê³ , ëª¨ë¸ì˜ `id2label` ë§¤í•‘ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beignets'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
