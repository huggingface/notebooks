{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë‘˜ëŸ¬ë³´ê¸°[[quick-tour]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformerë¥¼ ì‹œì‘í•´ë´ìš”! ë‘˜ëŸ¬ë³´ê¸°ëŠ” ê°œë°œìì™€ ì¼ë°˜ ì‚¬ìš©ì ëª¨ë‘ë¥¼ ìœ„í•´ ì“°ì—¬ì¡ŒìŠµë‹ˆë‹¤. `pipeline()`ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ë°©ë²•, [AutoClass](https://huggingface.co/docs/transformers/main/ko/./model_doc/auto)ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸°ë¥¼ ì ì¬í•˜ëŠ” ë°©ë²•ê³¼ PyTorch ë˜ëŠ” TensorFlowë¡œ ì‹ ì†í•˜ê²Œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ë³¸ì„ ë°°ìš°ê³  ì‹¶ë‹¤ë©´ íŠœí† ë¦¬ì–¼ì´ë‚˜ [course](https://huggingface.co/course/chapter1/1)ì—ì„œ ì—¬ê¸° ì†Œê°œëœ ê°œë…ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì„ í™•ì¸í•˜ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ ,\n",
    "\n",
    "```bash\n",
    "!pip install transformers datasets\n",
    "```\n",
    "\n",
    "ì¢‹ì•„í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "```bash\n",
    "pip install torch\n",
    "```\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline (íŒŒì´í”„ë¼ì¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline()`ì€ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì¶”ë¡ í•  ë•Œ ì œì¼ ì‰¬ìš´ ë°©ë²•ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°ì˜ ìˆ˜ë§ì€ íƒœìŠ¤í¬ì— `pipeline()`ì„ ì¦‰ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ì˜ ì˜ˆì‹œëŠ” ì•„ë˜ í‘œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n",
    "\n",
    "| **íƒœìŠ¤í¬**     | **ì„¤ëª…**                                                            | **ëª¨ë‹¬ë¦¬í‹°**     | **íŒŒì´í”„ë¼ì¸ ID**                             |\n",
    "|----------------|---------------------------------------------------------------------|------------------|-----------------------------------------------|\n",
    "| í…ìŠ¤íŠ¸ ë¶„ë¥˜    | í…ìŠ¤íŠ¸ì— ì•Œë§ì€ ë¼ë²¨ ë¶™ì´ê¸°                                         | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task=\"sentiment-analysis\")           |\n",
    "| í…ìŠ¤íŠ¸ ìƒì„±    | ì£¼ì–´ì§„ ë¬¸ìì—´ ì…ë ¥ê³¼ ì´ì–´ì§€ëŠ” í…ìŠ¤íŠ¸ ìƒì„±í•˜ê¸°                       | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task=\"text-generation\")              |\n",
    "| ê°œì²´ëª… ì¸ì‹    | ë¬¸ìì—´ì˜ ê° í† í°ë§ˆë‹¤ ì•Œë§ì€ ë¼ë²¨ ë¶™ì´ê¸° (ì¸ë¬¼, ì¡°ì§, ì¥ì†Œ ë“±ë“±)     | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task=\"ner\")                          |\n",
    "| ì§ˆì˜ì‘ë‹µ       | ì£¼ì–´ì§„ ë¬¸ë§¥ê³¼ ì§ˆë¬¸ì— ë”°ë¼ ì˜¬ë°”ë¥¸ ëŒ€ë‹µí•˜ê¸°                           | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task=\"question-answering\")           |\n",
    "| ë¹ˆì¹¸ ì±„ìš°ê¸°    | ë¬¸ìì—´ì˜ ë¹ˆì¹¸ì— ì•Œë§ì€ í† í° ë§ì¶”ê¸°                                  | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task=\"fill-mask\")                    |\n",
    "| ìš”ì•½           | í…ìŠ¤íŠ¸ë‚˜ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê¸°                                            | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task=\"summarization\")                |\n",
    "| ë²ˆì—­           | í…ìŠ¤íŠ¸ë¥¼ í•œ ì–¸ì–´ì—ì„œ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ê¸°                           | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task=\"translation\")                  |\n",
    "| ì´ë¯¸ì§€ ë¶„ë¥˜    | ì´ë¯¸ì§€ì— ì•Œë§ì€ ë¼ë²¨ ë¶™ì´ê¸°                                         | ì»´í“¨í„° ë¹„ì „(CV)  | pipeline(task=\"image-classification\")         |\n",
    "| ì´ë¯¸ì§€ ë¶„í•     | ì´ë¯¸ì§€ì˜ í”½ì…€ë§ˆë‹¤ ë¼ë²¨ ë¶™ì´ê¸°(ì‹œë§¨í‹±, íŒŒë†‰í‹± ë° ì¸ìŠ¤í„´ìŠ¤ ë¶„í•  í¬í•¨) | ì»´í“¨í„° ë¹„ì „(CV)  | pipeline(task=\"image-segmentation\")           |\n",
    "| ê°ì²´ íƒì§€      | ì´ë¯¸ì§€ ì† ê°ì²´ì˜ ê²½ê³„ ìƒìë¥¼ ê·¸ë¦¬ê³  í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ê¸°               | ì»´í“¨í„° ë¹„ì „(CV)  | pipeline(task=\"object-detection\")             |\n",
    "| ì˜¤ë””ì˜¤ ë¶„ë¥˜    | ì˜¤ë””ì˜¤ íŒŒì¼ì— ì•Œë§ì€ ë¼ë²¨ ë¶™ì´ê¸°                                    | ì˜¤ë””ì˜¤           | pipeline(task=\"audio-classification\")         |\n",
    "| ìë™ ìŒì„± ì¸ì‹ | ì˜¤ë””ì˜¤ íŒŒì¼ ì† ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¾¸ê¸°                               | ì˜¤ë””ì˜¤           | pipeline(task=\"automatic-speech-recognition\") |\n",
    "| ì‹œê° ì§ˆì˜ì‘ë‹µ  | ì£¼ì–´ì§„ ì´ë¯¸ì§€ì™€ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì— ë”°ë¼ ì˜¬ë°”ë¥´ê²Œ ëŒ€ë‹µí•˜ê¸°         | ë©€í‹°ëª¨ë‹¬         | pipeline(task=\"vqa\")                          |\n",
    "\n",
    "ë¨¼ì € `pipeline()`ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ì–´ ì ìš©í•  íƒœìŠ¤í¬ë¥¼ ê³ ë¥´ì„¸ìš”. ìœ„ íƒœìŠ¤í¬ë“¤ì€ ëª¨ë‘ `pipeline()`ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê³ , ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ì˜ ì „ì²´ ëª©ë¡ì„ ë³´ë ¤ë©´ [pipeline API ë ˆí¼ëŸ°ìŠ¤](https://huggingface.co/docs/transformers/main/ko/./main_classes/pipelines)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ê°ì • ë¶„ì„ íƒœìŠ¤í¬ì— `pipeline()`ë¥¼ ì ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline()`ì€ ê¸°ë³¸ [ì‚¬ì „í•™ìŠµëœ ëª¨ë¸(ì˜ì–´)](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)ì™€ ê°ì • ë¶„ì„ì„ í•˜ê¸° ìœ„í•œ tokenizerë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œí•´ë†“ìŠµë‹ˆë‹¤. ì´ì œ ì›í•˜ëŠ” í…ìŠ¤íŠ¸ì— `classifier`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"We are very happy to show you the ğŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì…ë ¥ì´ ì—¬ëŸ¬ ê°œë¼ë©´, ì…ë ¥ì„ `pipeline()`ì— ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•´ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label: POSITIVE, with score: 0.9998\n",
       "label: NEGATIVE, with score: 0.5309"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline()`ì€ íŠ¹ì • íƒœìŠ¤í¬ìš© ë°ì´í„°ì…‹ë¥¼ ì „ë¶€ ìˆœíšŒí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìë™ ìŒì„± ì¸ì‹ íƒœìŠ¤í¬ì— ì ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ìˆœíšŒí•  ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ë¥¼ ì ì¬í•˜ê² ìŠµë‹ˆë‹¤. (ìì„¸í•œ ë‚´ìš©ì€ ğŸ¤— Datasets [ì‹œì‘í•˜ê¸°](https://huggingface.co/docs/datasets/quickstart#audio)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”) [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) ë°ì´í„°ì…‹ë¡œ í•´ë³¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œë§ ë ˆì´íŠ¸ê°€ [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h)ì˜ í›ˆë ¨ ë‹¹ì‹œ ìƒ˜í”Œë§ ë ˆì´íŠ¸ì™€ ì¼ì¹˜í•´ì•¼ë§Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜¤ë””ì˜¤ íŒŒì¼ì€ `\"audio\"` ì—´ì„ í˜¸ì¶œí•  ë•Œ ìë™ìœ¼ë¡œ ì ì¬ë˜ê³  ë‹¤ì‹œ ìƒ˜í”Œë§ë©ë‹ˆë‹¤.\n",
    "ì²˜ìŒ 4ê°œ ìƒ˜í”Œì—ì„œ ìŒì„±ì„ ì¶”ì¶œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì— ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FODING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I THURN A JOIN A COUNT']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = speech_recognizer(dataset[:4][\"audio\"])\n",
    "print([d[\"text\"] for d in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ìŒì„±ì´ë‚˜ ë¹„ì „ì²˜ëŸ¼) ì…ë ¥ì´ í° ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì˜ ê²½ìš°, ë©”ëª¨ë¦¬ì— ì ì¬ì‹œí‚¤ê¸° ìœ„í•´ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ì œë„ˆë ˆì´í„°ë¡œ ì…ë ¥ì„ ëª¨ë‘ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [pipeline API ë ˆí¼ëŸ°ìŠ¤](https://huggingface.co/docs/transformers/main/ko/./main_classes/pipelines)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒì´í”„ë¼ì¸ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ì´ë‚˜ tokenizer ì‚¬ìš©í•˜ëŠ” ë°©ë²•[[use-another-model-and-tokenizer-in-the-pipeline]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline()`ì€ [Hub](https://huggingface.co/models) ì† ëª¨ë“  ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì–´, ì–¼ë§ˆë“ ì§€ `pipeline()`ì„ ì‚¬ìš©í•˜ê³  ì‹¶ì€ëŒ€ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í”„ë‘ìŠ¤ì–´ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë£° ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“œë ¤ë©´, Hubì˜ íƒœê·¸ë¡œ ì ì ˆí•œ ëª¨ë¸ì„ ì°¾ì•„ë³´ì„¸ìš”. ìƒìœ„ ê²€ìƒ‰ ê²°ê³¼ë¡œ ëœ¬ ê°ì • ë¶„ì„ì„ ìœ„í•´ íŒŒì¸íŠœë‹ëœ ë‹¤êµ­ì–´ [BERT ëª¨ë¸](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)ì´ í”„ë‘ìŠ¤ì–´ë¥¼ ì§€ì›í•˜ëŠ”êµ°ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoModelForSequenceClassification`ê³¼ `AutoTokenizer`ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ê³¼ í•¨ê»˜ ì—°ê´€ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (`AutoClass`ì— ëŒ€í•œ ë‚´ìš©ì€ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TFAutoModelForSequenceClassification`ê³¼ `AutoTokenizer`ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ê³¼ í•¨ê»˜ ì—°ê´€ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (`TFAutoClass`ì— ëŒ€í•œ ë‚´ìš©ì€ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline()`ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì…ë ¥í•˜ë©´ ì´ì œ (ê°ì • ë¶„ì„ê¸°ì¸) `classifier`ë¥¼ í”„ë‘ìŠ¤ì–´ í…ìŠ¤íŠ¸ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7273}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•˜ê³ ì‹¶ì€ ê²ƒì— ì ìš©í•  ë§ˆë•…í•œ ëª¨ë¸ì´ ì—†ë‹¤ë©´, ê°€ì§„ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•´ì•¼ í•©ë‹ˆë‹¤. ìì„¸í•œ ë°©ë²•ì€ [íŒŒì¸íŠœë‹ íŠœí† ë¦¬ì–¼](https://huggingface.co/docs/transformers/main/ko/./training)ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”. ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì˜ íŒŒì¸íŠœë‹ì„ ë§ˆì¹˜ì…¨ìœ¼ë©´, ëˆ„êµ¬ë‚˜ ë¨¸ì‹ ëŸ¬ë‹ì„ í•  ìˆ˜ ìˆë„ë¡ [ê³µìœ ](https://huggingface.co/docs/transformers/main/ko/./model_sharing)í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ì£¼ì„¸ìš”. ğŸ¤—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‚´ë¶€ì ìœ¼ë¡œ ë“¤ì–´ê°€ë©´ ìœ„ì—ì„œ ì‚¬ìš©í–ˆë˜ `pipeline()`ì€ `AutoModelForSequenceClassification`ê³¼ `AutoTokenizer` í´ë˜ìŠ¤ë¡œ ì‘ë™í•©ë‹ˆë‹¤. [AutoClass](https://huggingface.co/docs/transformers/main/ko/./model_doc/auto)ë€ ì´ë¦„ì´ë‚˜ ê²½ë¡œë¥¼ ë°›ìœ¼ë©´ ê·¸ì— ì•Œë§ëŠ” ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” 'ë°”ë¡œê°€ê¸°'ë¼ê³  ë³¼ ìˆ˜ ìˆëŠ”ë°ìš”. ì›í•˜ëŠ” íƒœìŠ¤í¬ì™€ ì „ì²˜ë¦¬ì— ì í•©í•œ `AutoClass`ë¥¼ ê³ ë¥´ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì „ì— ì‚¬ìš©í–ˆë˜ ì˜ˆì‹œë¡œ ëŒì•„ê°€ì„œ `AutoClass`ë¡œ `pipeline()`ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í† í¬ë‚˜ì´ì €ëŠ” ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ë©°, í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ë°›ì„ ìˆ«ì ë°°ì—´ë¡œ ë°”ê¿‰ë‹ˆë‹¤. í† í°í™” ê³¼ì •ì—ëŠ” ë‹¨ì–´ë¥¼ ì–´ë””ì—ì„œ ëŠì„ì§€, ì–¼ë§Œí¼ ë‚˜ëˆŒì§€ ë“±ì„ í¬í•¨í•œ ì—¬ëŸ¬ ê·œì¹™ì´ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [í† í¬ë‚˜ì´ì € ìš”ì•½](https://huggingface.co/docs/transformers/main/ko/./tokenizer_summary)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ì œì¼ ì¤‘ìš”í•œ ì ì€ ëª¨ë¸ì´ í›ˆë ¨ëì„ ë•Œì™€ ë™ì¼í•œ í† í°í™” ê·œì¹™ì„ ì“°ë„ë¡ ë™ì¼í•œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "`AutoTokenizer`ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í† í¬ë‚˜ì´ì €ì— í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ¬ë©´ ë‹¤ìŒì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬ê°€ ë°˜í™˜ë©ë‹ˆë‹¤.\n",
    "\n",
    "* [input_ids](https://huggingface.co/docs/transformers/main/ko/./glossary#input-ids): ìˆ«ìë¡œ í‘œí˜„ëœ í† í°ë“¤\n",
    "* [attention_mask](https://huggingface.co/docs/transformers/main/ko/.glossary#attention-mask): ì£¼ì‹œí•  í† í°ë“¤\n",
    "\n",
    "í† í¬ë‚˜ì´ì €ëŠ” ì…ë ¥ì„ ë¦¬ìŠ¤íŠ¸ë¡œë„ ë°›ì„ ìˆ˜ ìˆìœ¼ë©°, í…ìŠ¤íŠ¸ë¥¼ íŒ¨ë“œí•˜ê±°ë‚˜ ì˜ë¼ë‚´ì–´ ê· ì¼í•œ ê¸¸ì´ì˜ ë°°ì¹˜ë¥¼ ë°˜í™˜í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "[ì „ì²˜ë¦¬](https://huggingface.co/docs/transformers/main/ko/./preprocessing) íŠœí† ë¦¬ì–¼ì„ ë³´ì‹œë©´ í† í°í™”ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ì™€ ë©€í‹°ëª¨ë‹¬ ì…ë ¥ì„ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•œ `AutoFeatureExtractor`ê³¼ `AutoProcessor`ì˜ ì‚¬ìš©ë°©ë²•ë„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformersë¡œ ì‚¬ì „í•™ìŠµëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°„ë‹¨í•˜ê³  í†µì¼ëœ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬ë©´ `AutoTokenizer`ì²˜ëŸ¼ `AutoModel`ë„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ìœ ì¼í•œ ì°¨ì´ì ì€ íƒœìŠ¤í¬ì— ì í•©í•œ `AutoModel`ì„ ì„ íƒí•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸(ë˜ëŠ” ì‹œí€€ìŠ¤) ë¶„ë¥˜ì˜ ê²½ìš° `AutoModelForSequenceClassification`ì„ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "`AutoModel` í´ë˜ìŠ¤ì—ì„œ ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ë“¤ì€ [íƒœìŠ¤í¬ ì •ë¦¬](https://huggingface.co/docs/transformers/main/ko/./task_summary) ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ì´ì œ ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°°ì¹˜ë¥¼ ëª¨ë¸ë¡œ ì§ì ‘ ë³´ë‚´ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ì²˜ëŸ¼ `**`ë¥¼ ì•ì— ë¶™ì—¬ ë”•ì…”ë„ˆë¦¬ë¥¼ í’€ì–´ì£¼ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì˜ activation ê²°ê³¼ëŠ” `logits` ì†ì„±ì— ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. `logits`ì— Softmax í•¨ìˆ˜ë¥¼ ì ìš©í•´ì„œ í™•ë¥  í˜•íƒœë¡œ ë°›ìœ¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
       "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "print(pt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— TransformersëŠ” ì‚¬ì „í•™ìŠµëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°„ë‹¨í•˜ê³  í†µì¼ëœ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬ë©´ `AutoTokenizer`ì²˜ëŸ¼ `TFAutoModel`ë„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ìœ ì¼í•œ ì°¨ì´ì ì€ íƒœìŠ¤í¬ì— ì í•©í•œ `TFAutoModel`ë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸(ë˜ëŠ” ì‹œí€€ìŠ¤) ë¶„ë¥˜ì˜ ê²½ìš° `TFAutoModelForSequenceClassification`ì„ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "`AutoModel` í´ë˜ìŠ¤ì—ì„œ ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ë“¤ì€ [íƒœìŠ¤í¬ ì •ë¦¬](https://huggingface.co/docs/transformers/main/ko/./task_summary) ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ì´ì œ ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°°ì¹˜ë¥¼ ëª¨ë¸ë¡œ ì§ì ‘ ë³´ë‚´ì•¼ í•©ë‹ˆë‹¤. ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ë¥¼ í…ì„œì— ì§ì ‘ ë„£ì–´ì£¼ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_outputs = tf_model(tf_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì˜ activation ê²°ê³¼ëŠ” `logits` ì†ì„±ì— ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. `logits`ì— Softmax í•¨ìˆ˜ë¥¼ ì ìš©í•´ì„œ í™•ë¥  í˜•íƒœë¡œ ë°›ìœ¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
    "tf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ëª¨ë“  (PyTorch ë˜ëŠ” TensorFlow) ğŸ¤— Transformers ëª¨ë¸ì€ (softmax ë“±ì˜) ìµœì¢… activation í•¨ìˆ˜ *ì´ì „ì—* í…ì„œë¥¼ ë‚´ë†“ìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ ìµœì¢… activation í•¨ìˆ˜ë¥¼ ì¢…ì¢… loss í•¨ìˆ˜ì™€ ë™ì¼ì‹œí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ëª¨ë¸ ì¶œë ¥ì€ íŠ¹ìˆ˜ ë°ì´í„° í´ë˜ìŠ¤ì´ë¯€ë¡œ í•´ë‹¹ ì†ì„±ì€ IDEì—ì„œ ìë™ìœ¼ë¡œ ì™„ì„±ë©ë‹ˆë‹¤. ëª¨ë¸ ì¶œë ¥ì€ íŠœí”Œ ë˜ëŠ” (ì •ìˆ˜, ìŠ¬ë¼ì´ìŠ¤ ë˜ëŠ” ë¬¸ìì—´ë¡œ ì¸ë±ì‹±í•˜ëŠ”) ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì£¼ì–´ì§€ê³  ì´ëŸ° ê²½ìš° Noneì¸ ì†ì„±ì€ ë¬´ì‹œë©ë‹ˆë‹¤.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ ì €ì¥í•˜ê¸°[[save-a-model]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•œ ë’¤ì—ëŠ” `PreTrainedModel.save_pretrained()`ë¡œ ëª¨ë¸ì„ í† í¬ë‚˜ì´ì €ì™€ í•¨ê»˜ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_directory = \"./pt_save_pretrained\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "pt_model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•  ë•ŒëŠ” `PreTrainedModel.from_pretrained()`ë¡œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•œ ë’¤ì—ëŠ” `TFPreTrainedModel.save_pretrained()`ë¡œ ëª¨ë¸ì„ í† í¬ë‚˜ì´ì €ì™€ í•¨ê»˜ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_save_directory = \"./tf_save_pretrained\"\n",
    "tokenizer.save_pretrained(tf_save_directory)\n",
    "tf_model.save_pretrained(tf_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•  ë•ŒëŠ” `TFPreTrainedModel.from_pretrained()`ë¡œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformers ê¸°ëŠ¥ ì¤‘ íŠ¹íˆ ì¬ë¯¸ìˆëŠ” í•œ ê°€ì§€ëŠ” ëª¨ë¸ì„ ì €ì¥í•˜ê³  PyTorchë‚˜ TensorFlow ëª¨ë¸ë¡œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. 'from_pt' ë˜ëŠ” 'from_tf' ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ê¸°ì¡´ê³¼ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë¡œ ë³€í™˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì¶•í•˜ê¸°[[custom-model-builds]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì˜ êµ¬ì„± í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•˜ì—¬ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì€ë‹‰ì¸µ, ì–´í…ì…˜ í—¤ë“œ ìˆ˜ì™€ ê°™ì€ ëª¨ë¸ì˜ ì†ì„±ì„ êµ¬ì„±ì—ì„œ ì§€ì •í•©ë‹ˆë‹¤. ì»¤ìŠ¤í…€ êµ¬ì„± í´ë˜ìŠ¤ì—ì„œ ëª¨ë¸ì„ ë§Œë“¤ë©´ ì²˜ìŒë¶€í„° ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë¸ ì†ì„±ì€ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”ë˜ë¯€ë¡œ ì˜ë¯¸ ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë¨¼ì € `AutoConfig`ë¥¼ ì„í¬íŠ¸í•˜ê³ , ìˆ˜ì •í•˜ê³  ì‹¶ì€ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ì„¸ìš”. `AutoConfig.from_pretrained()`ì—ì„œ ì–´í…ì…˜ í—¤ë“œ ìˆ˜ ê°™ì€ ì†ì„±ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "my_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", n_heads=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoModel.from_config()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ìŠ¤í…€ êµ¬ì„±ëŒ€ë¡œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "my_model = AutoModel.from_config(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TFAutoModel.from_config()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ìŠ¤í…€ êµ¬ì„±ëŒ€ë¡œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "my_model = TFAutoModel.from_config(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì»¤ìŠ¤í…€ êµ¬ì„±ì„ ì‘ì„±í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì»¤ìŠ¤í…€ ì•„í‚¤í…ì²˜ ë§Œë“¤ê¸°](https://huggingface.co/docs/transformers/main/ko/./create_a_model) ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer - PyTorchì— ìµœì í™”ëœ í›ˆë ¨ ë°˜ë³µ ë£¨í”„[[trainer-a-pytorch-optimized-training-loop]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë“  ëª¨ë¸ì€ [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ì´ì–´ì„œ ëŒ€ë‹¤ìˆ˜ì˜ í›ˆë ¨ ë°˜ë³µ ë£¨í”„ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì§ì ‘ í›ˆë ¨ ë°˜ë³µ ë£¨í”„ë¥¼ ì‘ì„±í•´ë„ ë˜ì§€ë§Œ, ğŸ¤— TransformersëŠ” PyTorchìš© `Trainer` í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê¸°ë³¸ì ì¸ í›ˆë ¨ ë°˜í­ ë£¨í”„ê°€ í¬í•¨ë˜ì–´ ìˆê³ , ë¶„ì‚° í›ˆë ¨ì´ë‚˜ í˜¼í•© ì •ë°€ë„ ë“±ì˜ ì¶”ê°€ ê¸°ëŠ¥ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "íƒœìŠ¤í¬ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ `Trainer`ì— ì „ë‹¬í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "1. `PreTrainedModel` ë˜ëŠ” [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "   >>> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "2. `TrainingArguments`ë¡œ í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°ë‚˜ í›ˆë ¨í•  epoch ìˆ˜ì™€ ê°™ì´ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ í›ˆë ¨ ì¸ìˆ˜ë¥¼ ì „í˜€ ì§€ì •í•˜ì§€ ì•Šì€ ê²½ìš° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import TrainingArguments\n",
    "\n",
    "   >>> training_args = TrainingArguments(\n",
    "   ...     output_dir=\"path/to/save/folder/\",\n",
    "   ...     learning_rate=2e-5,\n",
    "   ...     per_device_train_batch_size=8,\n",
    "   ...     per_device_eval_batch_size=8,\n",
    "   ...     num_train_epochs=2,\n",
    "   ... )\n",
    "   ```\n",
    "\n",
    "3. í† í¬ë‚˜ì´ì €, íŠ¹ì§•ì¶”ì¶œê¸°(feature extractor), ì „ì²˜ë¦¬ê¸°(processor) í´ë˜ìŠ¤ ë“±ìœ¼ë¡œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoTokenizer\n",
    "\n",
    "   >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "4. ë°ì´í„°ì…‹ë¥¼ ì ì¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> from datasets import load_dataset\n",
    "\n",
    "   >>> dataset = load_dataset(\"rotten_tomatoes\")  # doctest: +IGNORE_RESULT\n",
    "   ```\n",
    "\n",
    "5. ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ê³  `map`ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ì…‹ì— ì ìš©ì‹œí‚µë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> def tokenize_dataset(dataset):\n",
    "   ...     return tokenizer(dataset[\"text\"])\n",
    "\n",
    "\n",
    "   >>> dataset = dataset.map(tokenize_dataset, batched=True)\n",
    "   ```\n",
    "\n",
    "6. `DataCollatorWithPadding`ë¡œ ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° í‘œë³¸ìœ¼ë¡œ ì‚¼ì„ ë°°ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import DataCollatorWithPadding\n",
    "\n",
    "   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "   ```\n",
    "\n",
    "ì´ì œ ìœ„ì˜ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ `Trainer`ë¡œ ëª¨ìœ¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¤€ë¹„ë˜ì—ˆìœ¼ë©´ `train()`ìœ¼ë¡œ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "sequence-to-sequence ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” (ë²ˆì—­ì´ë‚˜ ìš”ì•½ ê°™ì€) íƒœìŠ¤í¬ì˜ ê²½ìš° `Seq2SeqTrainer`ì™€ `Seq2SeqTrainingArguments` í´ë˜ìŠ¤ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "`Trainer` ë‚´ë¶€ì˜ ë©”ì„œë“œë¥¼ êµ¬í˜„ ìƒì†(subclassing)í•´ì„œ í›ˆë ¨ ë°˜ë³µ ë£¨í”„ë¥¼ ê°œì¡°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬ë©´ loss í•¨ìˆ˜, optimizer, scheduler ë“±ì˜ ê¸°ëŠ¥ë„ ê°œì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë©”ì„œë“œë¥¼ êµ¬í˜„ ìƒì†í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³´ë ¤ë©´ `Trainer`ë¥¼ ì°¸ê³ í•˜ì„¸ìš”. \n",
    "\n",
    "í›ˆë ¨ ë°˜ë³µ ë£¨í”„ë¥¼ ê°œì¡°í•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•ì€ [Callbacks](https://huggingface.co/docs/transformers/main/ko/./main_classes/callbacks)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. Callbacksë¡œ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í†µí•©í•˜ê³ , í›ˆë ¨ ë°˜ë³µ ë£¨í”„ë¥¼ ìˆ˜ì‹œë¡œ ì²´í¬í•˜ì—¬ ì§„í–‰ ìƒí™©ì„ ë³´ê³ ë°›ê±°ë‚˜, í›ˆë ¨ì„ ì¡°ê¸°ì— ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Callbacksì€ í›ˆë ¨ ë°˜ë³µ ë£¨í”„ ìì²´ë¥¼ ì „í˜€ ìˆ˜ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë§Œì•½ loss í•¨ìˆ˜ ë“±ì„ ê°œì¡°í•˜ê³  ì‹¶ë‹¤ë©´ `Trainer`ë¥¼ êµ¬í˜„ ìƒì†í•´ì•¼ë§Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlowë¡œ í›ˆë ¨ì‹œí‚¤ê¸°[[train-with-tensorflow]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë“  ëª¨ë¸ì€ [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ì´ì–´ì„œ [Keras](https://keras.io/) APIë¥¼ í†µí•´ TensorFlowì—ì„œ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ¤— Transformersì—ì„œ ë°ì´í„°ì…‹ë¥¼ `tf.data.Dataset` í˜•íƒœë¡œ ì‰½ê²Œ ì ì¬í•  ìˆ˜ ìˆëŠ” `prepare_tf_dataset()` ë©”ì„œë“œë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì—, Kerasì˜ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ë° [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) ë©”ì„œë“œë¡œ ì¦‰ì‹œ í›ˆë ¨ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. `TFPreTrainedModel` ë˜ëŠ” [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "   >>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "2. í† í¬ë‚˜ì´ì €, íŠ¹ì§•ì¶”ì¶œê¸°(feature extractor), ì „ì²˜ë¦¬ê¸°(processor) í´ë˜ìŠ¤ ë“±ìœ¼ë¡œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoTokenizer\n",
    "\n",
    "   >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "3. ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> def tokenize_dataset(dataset):\n",
    "   ...     return tokenizer(dataset[\"text\"])  # doctest: +SKIP\n",
    "   ```\n",
    "\n",
    "4. `map`ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ì…‹ì— ìœ„ í•¨ìˆ˜ë¥¼ ì ìš©ì‹œí‚¨ ë‹¤ìŒ, ë°ì´í„°ì…‹ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ `prepare_tf_dataset()`ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ë³€ê²½í•´ë³´ê±°ë‚˜ ë°ì´í„°ì…‹ë¥¼ ì„ì–´ë´ë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "\n",
    "   ```py\n",
    "   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP\n",
    "   >>> tf_dataset = model.prepare_tf_dataset(\n",
    "   ...     dataset, batch_size=16, shuffle=True, tokenizer=tokenizer\n",
    "   ... )  # doctest: +SKIP\n",
    "   ```\n",
    "\n",
    "5. ì¤€ë¹„ë˜ì—ˆìœ¼ë©´ `compile`ê³¼ `fit`ìœ¼ë¡œ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”.\n",
    "\n",
    "   ```py\n",
    "   >>> from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "   >>> model.compile(optimizer=Adam(3e-5))\n",
    "   >>> model.fit(dataset)  # doctest: +SKIP\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì´ì œ ë¬´ì–¼ í•˜ë©´ ë ê¹Œìš”?[[whats-next]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformers ë‘˜ëŸ¬ë³´ê¸°ë¥¼ ëª¨ë‘ ì½ìœ¼ì…¨ë‹¤ë©´, ê°€ì´ë“œë¥¼ í†µí•´ íŠ¹ì • ê¸°ìˆ ì„ ë°°ìš¸ ìˆ˜ ìˆì–´ìš”. ì˜ˆë¥¼ ë“¤ì–´ ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ì‘ì„±í•˜ëŠ” ë°©ë²•, íƒœìŠ¤í¬ìš© ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•, ìŠ¤í¬ë¦½íŠ¸ë¡œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²• ë“±ì´ ìˆìŠµë‹ˆë‹¤. ğŸ¤— Transformersì˜ í•µì‹¬ ê°œë…ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ ì»¤í”¼ í•œ ì”ì„ ë§ˆì‹  ë’¤ ê°œë… ê°€ì´ë“œë¥¼ ì‚´í´ë³´ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
