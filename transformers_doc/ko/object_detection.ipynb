{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ì„¤ì¹˜ ë°©ë²•\n",
    "! pip install transformers datasets\n",
    "# ë§ˆì§€ë§‰ ë¦´ë¦¬ìŠ¤ ëŒ€ì‹  ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ë ¤ë©´, ìœ„ ëª…ë ¹ì„ ì£¼ì„ìœ¼ë¡œ ë°”ê¾¸ê³  ì•„ë˜ ëª…ë ¹ì„ í•´ì œí•˜ì„¸ìš”.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê°ì²´ íƒì§€ [[object-detection]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°ì²´ íƒì§€ëŠ” ì´ë¯¸ì§€ì—ì„œ ì¸ìŠ¤í„´ìŠ¤(ì˜ˆ: ì‚¬ëŒ, ê±´ë¬¼ ë˜ëŠ” ìë™ì°¨)ë¥¼ ê°ì§€í•˜ëŠ” ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì…ë‹ˆë‹¤. ê°ì²´ íƒì§€ ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³  íƒì§€ëœ ë°”ìš´ë”© ë°•ìŠ¤ì˜ ì¢Œí‘œì™€ ê´€ë ¨ëœ ë ˆì´ë¸”ì„ ì¶œë ¥í•©ë‹ˆë‹¤. \n",
    "í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì—ëŠ” ì—¬ëŸ¬ ê°ì²´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë©° ê°ê°ì€ ìì²´ì ì¸ ë°”ìš´ë”© ë°•ìŠ¤ì™€ ë ˆì´ë¸”ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì˜ˆ: ì°¨ì™€ ê±´ë¬¼ì´ ìˆëŠ” ì´ë¯¸ì§€). \n",
    "ë˜í•œ ê° ê°ì²´ëŠ” ì´ë¯¸ì§€ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì— ì¡´ì¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì˜ˆ: ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ëŒ€ì˜ ì°¨ê°€ ìˆì„ ìˆ˜ ìˆìŒ). \n",
    "ì´ ì‘ì—…ì€ ë³´í–‰ì, ë„ë¡œ í‘œì§€íŒ, ì‹ í˜¸ë“±ê³¼ ê°™ì€ ê²ƒë“¤ì„ ê°ì§€í•˜ëŠ” ììœ¨ ì£¼í–‰ì— ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. \n",
    "ë‹¤ë¥¸ ì‘ìš© ë¶„ì•¼ë¡œëŠ” ì´ë¯¸ì§€ ë‚´ ê°ì²´ ìˆ˜ ê³„ì‚° ë° ì´ë¯¸ì§€ ê²€ìƒ‰ ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê°€ì´ë“œì—ì„œ ë‹¤ìŒì„ ë°°ìš¸ ê²ƒì…ë‹ˆë‹¤:\n",
    "\n",
    " 1. í•©ì„±ê³± ë°±ë³¸(ì¸í’‹ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” í•©ì„±ê³± ë„¤íŠ¸ì›Œí¬)ê³¼ ì¸ì½”ë”-ë””ì½”ë” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ê²°í•©í•œ [DETR](https://huggingface.co/docs/transformers/model_doc/detr) ëª¨ë¸ì„ [CPPE-5](https://huggingface.co/datasets/cppe-5) ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ë¯¸ì„¸ì¡°ì • í•˜ê¸°\n",
    " 2. ë¯¸ì„¸ì¡°ì • í•œ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ê¸°.\n",
    "\n",
    "<Tip>\n",
    "ì´ íŠœí† ë¦¬ì–¼ì˜ íƒœìŠ¤í¬ëŠ” ë‹¤ìŒ ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤:\n",
    "\n",
    "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
    "\n",
    "[Conditional DETR](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/conditional_detr), [Deformable DETR](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/deformable_detr), [DETA](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/deta), [DETR](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/detr), [Table Transformer](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/table-transformer), [YOLOS](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/yolos)\n",
    "\n",
    "<!--End of the generated tip-->\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:\n",
    "```bash\n",
    "pip install -q datasets transformers evaluate timm albumentations\n",
    "```\n",
    "\n",
    "í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ğŸ¤— Datasetsê³¼ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ğŸ¤— Transformers, ë°ì´í„°ë¥¼ ì¦ê°•í•˜ê¸° ìœ„í•œ `albumentations`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. \n",
    "DETR ëª¨ë¸ì˜ í•©ì„±ê³± ë°±ë³¸ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ì„œëŠ” í˜„ì¬ `timm`ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì»¤ë®¤ë‹ˆí‹°ì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ê³µìœ í•  ìˆ˜ ìˆë„ë¡ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚˜ë©´ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPPE-5 ë°ì´í„° ì„¸íŠ¸ ê°€ì ¸ì˜¤ê¸° [[load-the-CPPE-5-dataset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CPPE-5](https://huggingface.co/datasets/cppe-5) ë°ì´í„° ì„¸íŠ¸ëŠ” COVID-19 ëŒ€ìœ í–‰ ìƒí™©ì—ì„œ ì˜ë£Œ ì „ë¬¸ì¸ë ¥ ë³´í˜¸ ì¥ë¹„(PPE)ë¥¼ ì‹ë³„í•˜ëŠ” ì–´ë…¸í…Œì´ì…˜ì´ í¬í•¨ëœ ì´ë¯¸ì§€ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„° ì„¸íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
       "        num_rows: 29\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cppe5 = load_dataset(\"cppe-5\")\n",
    "cppe5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ë°ì´í„° ì„¸íŠ¸ëŠ” í•™ìŠµ ì„¸íŠ¸ ì´ë¯¸ì§€ 1,000ê°œì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì´ë¯¸ì§€ 29ê°œë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„°ì— ìµìˆ™í•´ì§€ê¸° ìœ„í•´, ì˜ˆì‹œê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 15,\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7F9EC9E77C10>,\n",
       " 'width': 943,\n",
       " 'height': 663,\n",
       " 'objects': {'id': [114, 115, 116, 117],\n",
       "  'area': [3796, 1596, 152768, 81002],\n",
       "  'bbox': [[302.0, 109.0, 73.0, 52.0],\n",
       "   [810.0, 100.0, 57.0, 28.0],\n",
       "   [160.0, 31.0, 248.0, 616.0],\n",
       "   [741.0, 68.0, 202.0, 401.0]],\n",
       "  'category': [4, 4, 0, 0]}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cppe5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì„¸íŠ¸ì— ìˆëŠ” ì˜ˆì‹œëŠ” ë‹¤ìŒì˜ ì˜ì—­ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- `image_id`: ì˜ˆì‹œ ì´ë¯¸ì§€ id\n",
    "- `image`: ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ëŠ” `PIL.Image.Image` ê°ì²´\n",
    "- `width`: ì´ë¯¸ì§€ì˜ ë„ˆë¹„\n",
    "- `height`: ì´ë¯¸ì§€ì˜ ë†’ì´\n",
    "- `objects`: ì´ë¯¸ì§€ ì•ˆì˜ ê°ì²´ë“¤ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬:\n",
    "  - `id`: ì–´ë…¸í…Œì´ì…˜ id\n",
    "  - `area`: ë°”ìš´ë”© ë°•ìŠ¤ì˜ ë©´ì \n",
    "  - `bbox`: ê°ì²´ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ([COCO í¬ë§·](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco)ìœ¼ë¡œ)\n",
    "  - `category`: ê°ì²´ì˜ ì¹´í…Œê³ ë¦¬, ê°€ëŠ¥í•œ ê°’ìœ¼ë¡œëŠ” `Coverall (0)`, `Face_Shield (1)`, `Gloves (2)`, `Goggles (3)` ë° `Mask (4)` ê°€ í¬í•¨ë©ë‹ˆë‹¤.\n",
    "\n",
    "`bbox` í•„ë“œê°€ DETR ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” COCO í˜•ì‹ì„ ë”°ë¥¸ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "ê·¸ëŸ¬ë‚˜ `objects` ë‚´ë¶€ì˜ í•„ë“œ ê·¸ë£¹ì€ DETRì´ ìš”êµ¬í•˜ëŠ” ì–´ë…¸í…Œì´ì…˜ í˜•ì‹ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ê¸° ì „ì— ì „ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„°ë¥¼ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í•œ ê°€ì§€ ì˜ˆì‹œë¥¼ ì‹œê°í™”í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "image = cppe5[\"train\"][0][\"image\"]\n",
    "annotations = cppe5[\"train\"][0][\"objects\"]\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "for i in range(len(annotations[\"id\"])):\n",
    "    box = annotations[\"bbox\"][i - 1]\n",
    "    class_idx = annotations[\"category\"][i - 1]\n",
    "    x, y, w, h = tuple(box)\n",
    "    draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n",
    "    draw.text((x, y), id2label[class_idx], fill=\"white\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://i.imgur.com/TdaqPJO.png\" alt=\"CPPE-5 Image Example\"/>\n",
    "</div>\n",
    "\n",
    "ë°”ìš´ë”© ë°•ìŠ¤ì™€ ì—°ê²°ëœ ë ˆì´ë¸”ì„ ì‹œê°í™”í•˜ë ¤ë©´ ë°ì´í„° ì„¸íŠ¸ì˜ ë©”íƒ€ ë°ì´í„°, íŠ¹íˆ `category` í•„ë“œì—ì„œ ë ˆì´ë¸”ì„ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤. \n",
    "ë˜í•œ ë ˆì´ë¸” IDë¥¼ ë ˆì´ë¸” í´ë˜ìŠ¤ì— ë§¤í•‘í•˜ëŠ” `id2label`ê³¼ ë°˜ëŒ€ë¡œ ë§¤í•‘í•˜ëŠ” `label2id` ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. \n",
    "ëª¨ë¸ì„ ì„¤ì •í•  ë•Œ ì´ëŸ¬í•œ ë§¤í•‘ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë§¤í•‘ì€ í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ëª¨ë¸ì„ ê³µìœ í–ˆì„ ë•Œ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë°ì´í„°ë¥¼ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•œ ìµœì¢… ë‹¨ê³„ë¡œ, ì ì¬ì ì¸ ë¬¸ì œë¥¼ ì°¾ì•„ë³´ì„¸ìš”. \n",
    "ê°ì²´ ê°ì§€ë¥¼ ìœ„í•œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ ì¤‘ í•˜ë‚˜ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ê°€ ì´ë¯¸ì§€ì˜ ê°€ì¥ìë¦¬ë¥¼ ë„˜ì–´ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤. \n",
    "ì´ëŸ¬í•œ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ \"ë„˜ì–´ê°€ëŠ” ê²ƒ(run away)\"ì€ í›ˆë ¨ ì¤‘ì— ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆê¸°ì— ì´ ë‹¨ê³„ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "ì´ ë°ì´í„° ì„¸íŠ¸ì—ë„ ê°™ì€ ë¬¸ì œê°€ ìˆëŠ” ëª‡ ê°€ì§€ ì˜ˆê°€ ìˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” ê°„ë‹¨í•˜ê²Œí•˜ê¸° ìœ„í•´ ë°ì´í„°ì—ì„œ ì´ëŸ¬í•œ ì´ë¯¸ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_idx = [590, 821, 822, 875, 876, 878, 879]\n",
    "keep = [i for i in range(len(cppe5[\"train\"])) if i not in remove_idx]\n",
    "cppe5[\"train\"] = cppe5[\"train\"].select(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸° [[preprocess-the-data]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • í•˜ë ¤ë©´, ë¯¸ë¦¬ í•™ìŠµëœ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œ ì „ì²˜ë¦¬ ë°©ì‹ê³¼ ì •í™•í•˜ê²Œ ì¼ì¹˜í•˜ë„ë¡ ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "`AutoImageProcessor`ëŠ” ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ DETR ëª¨ë¸ì´ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `pixel_values`, `pixel_mask`, ê·¸ë¦¬ê³  `labels`ë¥¼ ìƒì„±í•˜ëŠ” ì‘ì—…ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. \n",
    "ì´ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì—ëŠ” ê±±ì •í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” ëª‡ ê°€ì§€ ì†ì„±ì´ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- `image_mean = [0.485, 0.456, 0.406 ]`\n",
    "- `image_std = [0.229, 0.224, 0.225]`\n",
    "\n",
    "\n",
    "ì´ ê°’ë“¤ì€ ëª¨ë¸ ì‚¬ì „ í›ˆë ¨ ì¤‘ ì´ë¯¸ì§€ë¥¼ ì •ê·œí™”í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í‰ê· ê³¼ í‘œì¤€ í¸ì°¨ì…ë‹ˆë‹¤. \n",
    "ì´ ê°’ë“¤ì€ ì¶”ë¡  ë˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì´ë¯¸ì§€ ëª¨ë¸ì„ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•  ë•Œ ë³µì œí•´ì•¼ í•˜ëŠ” ì¤‘ìš”í•œ ê°’ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"facebook/detr-resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`image_processor`ì— ì´ë¯¸ì§€ë¥¼ ì „ë‹¬í•˜ê¸° ì „ì—, ë°ì´í„° ì„¸íŠ¸ì— ë‘ ê°€ì§€ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "- ì´ë¯¸ì§€ ì¦ê°•\n",
    "- DETR ëª¨ë¸ì˜ ìš”êµ¬ì— ë§ê²Œ ì–´ë…¸í…Œì´ì…˜ì„ ë‹¤ì‹œ í¬ë§·íŒ…\n",
    "\n",
    "ì²«ì§¸ë¡œ, ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ê³¼ì í•© ë˜ì§€ ì•Šë„ë¡ ë°ì´í„° ì¦ê°• ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ ì•„ë¬´ê±°ë‚˜ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” [Albumentations](https://albumentations.ai/docs/) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤...\n",
    "ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë³€í™˜ì„ ì´ë¯¸ì§€ì— ì ìš©í•˜ê³  ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì ì ˆí•˜ê²Œ ì—…ë°ì´íŠ¸í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n",
    "ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì„œì—ëŠ” [ê°ì²´ íƒì§€ë¥¼ ìœ„í•´ ì´ë¯¸ì§€ë¥¼ ë³´ê°•í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ê°€ì´ë“œ](https://huggingface.co/docs/datasets/object_detection)ê°€ ìˆìœ¼ë©°, \n",
    "ì´ ì˜ˆì œì™€ ì •í™•íˆ ë™ì¼í•œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê° ì´ë¯¸ì§€ë¥¼ (480, 480) í¬ê¸°ë¡œ ì¡°ì •í•˜ê³ , ì¢Œìš°ë¡œ ë’¤ì§‘ê³ , ë°ê¸°ë¥¼ ë†’ì´ëŠ” ë™ì¼í•œ ì ‘ê·¼ë²•ì„ ì ìš©í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(480, 480),\n",
    "        albumentations.HorizontalFlip(p=1.0),\n",
    "        albumentations.RandomBrightnessContrast(p=1.0),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì–´ë…¸í…Œì´ì…˜ì´ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•©ë‹ˆë‹¤: `{'image_id': int, 'annotations': List[Dict]}`, ì—¬ê¸°ì„œ ê° ë”•ì…”ë„ˆë¦¬ëŠ” COCO ê°ì²´ ì–´ë…¸í…Œì´ì…˜ì…ë‹ˆë‹¤. ë‹¨ì¼ ì˜ˆì œì— ëŒ€í•´ ì–´ë…¸í…Œì´ì…˜ì˜ í˜•ì‹ì„ ë‹¤ì‹œ ì§€ì •í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì¶”ê°€í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ ì „ì²˜ë¦¬ ë³€í™˜ì„ ê²°í•©í•˜ì—¬ ì˜ˆì œ ë°°ì¹˜ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming a batch\n",
    "def transform_aug_ann(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì „ ë‹¨ê³„ì—ì„œ ë§Œë“  ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ğŸ¤— Datasetsì˜ `with_transform` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ ì „ì²´ì— ì ìš©í•©ë‹ˆë‹¤.\n",
    "ì´ ë©”ì†Œë“œëŠ” ë°ì´í„° ì„¸íŠ¸ì˜ ìš”ì†Œë¥¼ ê°€ì ¸ì˜¬ ë•Œë§ˆë‹¤ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì‹œì ì—ì„œëŠ” ì „ì²˜ë¦¬ í›„ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì˜ˆì‹œ í•˜ë‚˜ë¥¼ ê°€ì ¸ì™€ì„œ ë³€í™˜ í›„ ëª¨ì–‘ì´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì´ë•Œ, `pixel_values` í…ì„œ, `pixel_mask` í…ì„œ, ê·¸ë¦¬ê³  `labels`ë¡œ êµ¬ì„±ëœ í…ì„œê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],\n",
       "          [ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],\n",
       "          [ 0.9132,  0.9132,  0.9132,  ..., -1.9638, -1.9638, -1.9638],\n",
       "          ...,\n",
       "          [-1.5699, -1.5699, -1.5699,  ..., -1.9980, -1.9980, -1.9980],\n",
       "          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809],\n",
       "          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809]],\n",
       "\n",
       "         [[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],\n",
       "          [ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],\n",
       "          [ 1.3081,  1.3081,  1.3081,  ..., -1.8256, -1.8256, -1.8256],\n",
       "          ...,\n",
       "          [-1.3179, -1.3179, -1.3179,  ..., -1.8606, -1.8606, -1.8606],\n",
       "          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431],\n",
       "          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431]],\n",
       "\n",
       "         [[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\n",
       "          [ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\n",
       "          [ 1.4200,  1.4200,  1.4200,  ..., -1.6302, -1.6302, -1.6302],\n",
       "          ...,\n",
       "          [-1.0201, -1.0201, -1.0201,  ..., -1.5604, -1.5604, -1.5604],\n",
       "          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430],\n",
       "          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430]]]),\n",
       " 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': {'size': tensor([800, 800]), 'image_id': tensor([756]), 'class_labels': tensor([4]), 'boxes': tensor([[0.7340, 0.6986, 0.3414, 0.5944]]), 'area': tensor([519544.4375]), 'iscrowd': tensor([0]), 'orig_size': tensor([480, 480])}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cppe5[\"train\"] = cppe5[\"train\"].with_transform(transform_aug_ann)\n",
    "cppe5[\"train\"][15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°ê°ì˜ ì´ë¯¸ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¦ê°•í•˜ê³  ì´ë¯¸ì§€ì˜ ì–´ë…¸í…Œì´ì…˜ì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤. \n",
    "ê·¸ëŸ¬ë‚˜ ì „ì²˜ë¦¬ëŠ” ì•„ì§ ëë‚˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ë‹¨ê³„ë¡œ, ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ ë§Œë“¤ ì‚¬ìš©ì ì •ì˜ `collate_fn`ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "í•´ë‹¹ ë°°ì¹˜ì—ì„œ ê°€ì¥ í° ì´ë¯¸ì§€ì— ì´ë¯¸ì§€(í˜„ì¬ `pixel_values` ì¸)ë¥¼ íŒ¨ë“œí•˜ê³ , ì‹¤ì œ í”½ì…€(1)ê³¼ íŒ¨ë”©(0)ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ê·¸ì— í•´ë‹¹í•˜ëŠ” ìƒˆë¡œìš´ `pixel_mask`ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸° [[training-the-DETR-model]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì „ ì„¹ì…˜ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì—¬ ì´ì œ ëª¨ë¸ì„ í•™ìŠµí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
    "ì´ ë°ì´í„° ì„¸íŠ¸ì˜ ì´ë¯¸ì§€ëŠ” ë¦¬ì‚¬ì´ì¦ˆ í›„ì—ë„ ì—¬ì „íˆ ìš©ëŸ‰ì´ í¬ê¸° ë•Œë¬¸ì—, ì´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • í•˜ë ¤ë©´ ì ì–´ë„ í•˜ë‚˜ì˜ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•™ìŠµì€ ë‹¤ìŒì˜ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. `AutoModelForObjectDetection`ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬ì™€ ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "2. `TrainingArguments`ì—ì„œ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "3. ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ë° ë°ì´í„° ì½œë ˆì´í„°ì™€ í•¨ê»˜ `Trainer`ì— í›ˆë ¨ ì¸ìˆ˜ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "4. `train()`ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²˜ë¦¬ì— ì‚¬ìš©í•œ ì²´í¬í¬ì¸íŠ¸ì™€ ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ë•Œ, ë°ì´í„° ì„¸íŠ¸ì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ë§Œë“  `label2id`ì™€ `id2label` ë§¤í•‘ì„ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "ë˜í•œ, `ignore_mismatched_sizes=True`ë¥¼ ì§€ì •í•˜ì—¬ ê¸°ì¡´ ë¶„ë¥˜ í—¤ë“œ(ëª¨ë¸ì—ì„œ ë¶„ë¥˜ì— ì‚¬ìš©ë˜ëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´)ë¥¼ ìƒˆ ë¶„ë¥˜ í—¤ë“œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TrainingArguments`ì—ì„œ `output_dir`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•œ ë‹¤ìŒ, í•„ìš”ì— ë”°ë¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ì„±í•˜ì„¸ìš”.\n",
    "ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì—´ì„ ì œê±°í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ `remove_unused_columns`ê°€ `True`ì¼ ê²½ìš° ì´ë¯¸ì§€ ì—´ì´ ì‚­ì œë©ë‹ˆë‹¤. \n",
    "ì´ë¯¸ì§€ ì—´ì´ ì—†ëŠ” ê²½ìš° `pixel_values`ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— `remove_unused_columns`ë¥¼ `False`ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œí•˜ì—¬ ê³µìœ í•˜ë ¤ë©´ `push_to_hub`ë¥¼ `True`ë¡œ ì„¤ì •í•˜ì‹­ì‹œì˜¤(í—ˆê¹…í˜ì´ìŠ¤ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"detr-resnet-50_finetuned_cppe5\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ `model`, `training_args`, `collate_fn`, `image_processor`ì™€ ë°ì´í„° ì„¸íŠ¸(`cppe5`)ë¥¼ ëª¨ë‘ ê°€ì ¸ì˜¨ í›„, `train()`ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=cppe5[\"train\"],\n",
    "    tokenizer=image_processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`training_args`ì—ì„œ `push_to_hub`ë¥¼ `True`ë¡œ ì„¤ì •í•œ ê²½ìš°, í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ëŠ” í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œë©ë‹ˆë‹¤. \n",
    "í•™ìŠµ ì™„ë£Œ í›„, `push_to_hub()` ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í‰ê°€í•˜ê¸° [[evaluate]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°ì²´ íƒì§€ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ì¼ë ¨ì˜ <a href=\"https://cocodataset.org/#detection-eval\">COCO-ìŠ¤íƒ€ì¼ ì§€í‘œ</a>ë¡œ í‰ê°€ë©ë‹ˆë‹¤. \n",
    "ê¸°ì¡´ì— êµ¬í˜„ëœ í‰ê°€ ì§€í‘œ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì—¬ê¸°ì—ì„œëŠ” í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— í‘¸ì‹œí•œ ìµœì¢… ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ë° `torchvision`ì—ì„œ ì œê³µí•˜ëŠ” í‰ê°€ ì§€í‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "`torchvision` í‰ê°€ì(evaluator)ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì‹¤ì¸¡ê°’ì¸ COCO ë°ì´í„° ì„¸íŠ¸ë¥¼ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "COCO ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¹Œë“œí•˜ëŠ” APIëŠ” ë°ì´í„°ë¥¼ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ì €ì¥í•´ì•¼ í•˜ë¯€ë¡œ, ë¨¼ì € ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ì„ ë””ìŠ¤í¬ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "í•™ìŠµì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì¤€ë¹„í•  ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ, cppe5[\"test\"]ì—ì„œì˜ ì–´ë…¸í…Œì´ì…˜ì€ í¬ë§·ì„ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "í‰ê°€ ë‹¨ê³„ëŠ” ì•½ê°„ì˜ ì‘ì—…ì´ í•„ìš”í•˜ì§€ë§Œ, í¬ê²Œ ì„¸ ê°€ì§€ ì£¼ìš” ë‹¨ê³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "ë¨¼ì €, `cppe5[\"test\"]` ì„¸íŠ¸ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤: ì–´ë…¸í…Œì´ì…˜ì„ í¬ë§·ì— ë§ê²Œ ë§Œë“¤ê³  ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# format annotations the same as for training, no need for data augmentation\n",
    "def val_formatted_anns(image_id, objects):\n",
    "    annotations = []\n",
    "    for i in range(0, len(objects[\"id\"])):\n",
    "        new_ann = {\n",
    "            \"id\": objects[\"id\"][i],\n",
    "            \"category_id\": objects[\"category\"][i],\n",
    "            \"iscrowd\": 0,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": objects[\"area\"][i],\n",
    "            \"bbox\": objects[\"bbox\"][i],\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "# Save images and annotations into the files torchvision.datasets.CocoDetection expects\n",
    "def save_cppe5_annotation_file_images(cppe5):\n",
    "    output_json = {}\n",
    "    path_output_cppe5 = f\"{os.getcwd()}/cppe5/\"\n",
    "\n",
    "    if not os.path.exists(path_output_cppe5):\n",
    "        os.makedirs(path_output_cppe5)\n",
    "\n",
    "    path_anno = os.path.join(path_output_cppe5, \"cppe5_ann.json\")\n",
    "    categories_json = [{\"supercategory\": \"none\", \"id\": id, \"name\": id2label[id]} for id in id2label]\n",
    "    output_json[\"images\"] = []\n",
    "    output_json[\"annotations\"] = []\n",
    "    for example in cppe5:\n",
    "        ann = val_formatted_anns(example[\"image_id\"], example[\"objects\"])\n",
    "        output_json[\"images\"].append(\n",
    "            {\n",
    "                \"id\": example[\"image_id\"],\n",
    "                \"width\": example[\"image\"].width,\n",
    "                \"height\": example[\"image\"].height,\n",
    "                \"file_name\": f\"{example['image_id']}.png\",\n",
    "            }\n",
    "        )\n",
    "        output_json[\"annotations\"].extend(ann)\n",
    "    output_json[\"categories\"] = categories_json\n",
    "\n",
    "    with open(path_anno, \"w\") as file:\n",
    "        json.dump(output_json, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    for im, img_id in zip(cppe5[\"image\"], cppe5[\"image_id\"]):\n",
    "        path_img = os.path.join(path_output_cppe5, f\"{img_id}.png\")\n",
    "        im.save(path_img)\n",
    "\n",
    "    return path_output_cppe5, path_anno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ, `cocoevaluator`ì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `CocoDetection` í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, feature_extractor, ann_file):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target: converting target to DETR format,\n",
    "        # resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {\"image_id\": image_id, \"annotations\": target}\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()  # remove batch dimension\n",
    "        target = encoding[\"labels\"][0]  # remove batch dimension\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": target}\n",
    "\n",
    "\n",
    "im_processor = AutoImageProcessor.from_pretrained(\"MariaK/detr-resnet-50_finetuned_cppe5\")\n",
    "\n",
    "path_output_cppe5, path_anno = save_cppe5_annotation_file_images(cppe5[\"test\"])\n",
    "test_ds_coco_format = CocoDetection(path_output_cppe5, im_processor, path_anno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ, í‰ê°€ ì§€í‘œë¥¼ ê°€ì ¸ì™€ì„œ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulating evaluation results...\n",
       "DONE (t=0.08s).\n",
       "IoU metric: bbox\n",
       " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.150\n",
       " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.280\n",
       " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130\n",
       " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038\n",
       " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.036\n",
       " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.182\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.317\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.335\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.104\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.146\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"MariaK/detr-resnet-50_finetuned_cppe5\")\n",
    "module = evaluate.load(\"ybelkada/cocoevaluate\", coco=test_ds_coco_format.coco)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_ds_coco_format, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "\n",
    "        labels = [\n",
    "            {k: v for k, v in t.items()} for t in batch[\"labels\"]\n",
    "        ]  # these are in DETR format, resized + normalized\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "        results = im_processor.post_process(outputs, orig_target_sizes)  # convert outputs of model to COCO api\n",
    "\n",
    "        module.add(prediction=results, reference=labels)\n",
    "        del batch\n",
    "\n",
    "results = module.compute()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ëŸ¬í•œ ê²°ê³¼ëŠ” `TrainingArguments`ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ë”ìš± ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œë²ˆ ì‹œë„í•´ ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ë¡ í•˜ê¸° [[inference]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETR ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • ë° í‰ê°€í•˜ê³ , í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œ í–ˆìœ¼ë¯€ë¡œ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ `pipeline()`ì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \n",
    "ëª¨ë¸ê³¼ í•¨ê»˜ ê°ì²´ íƒì§€ë¥¼ ìœ„í•œ íŒŒì´í”„ë¼ì¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³ , ì´ë¯¸ì§€ë¥¼ ì „ë‹¬í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import requests\n",
    "\n",
    "url = \"https://i.imgur.com/2lnWoly.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "obj_detector = pipeline(\"object-detection\", model=\"MariaK/detr-resnet-50_finetuned_cppe5\")\n",
    "obj_detector(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§Œì•½ ì›í•œë‹¤ë©´ ìˆ˜ë™ìœ¼ë¡œ `pipeline`ì˜ ê²°ê³¼ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Detected Coverall with confidence 0.566 at location [1215.32, 147.38, 4401.81, 3227.08]\n",
       "Detected Mask with confidence 0.584 at location [2449.06, 823.19, 3256.43, 1413.9]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"MariaK/detr-resnet-50_finetuned_cppe5\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"MariaK/detr-resnet-50_finetuned_cppe5\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê² ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://i.imgur.com/4QZnf9A.png\" alt=\"Object detection result on a new image\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
