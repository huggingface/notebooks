{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ ğŸ¤— PEFT åŠ è½½adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•](https://huggingface.co/blog/peft)åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œå¹¶åœ¨å…¶é¡¶éƒ¨æ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ˆadaptersï¼‰ã€‚adaptersè¢«è®­ç»ƒä»¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•å·²è¢«è¯æ˜éå¸¸èŠ‚çœå†…å­˜ï¼ŒåŒæ—¶å…·æœ‰è¾ƒä½çš„è®¡ç®—ä½¿ç”¨é‡ï¼ŒåŒæ—¶äº§ç”Ÿä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸å½“çš„ç»“æœã€‚\n",
    "\n",
    "ä½¿ç”¨PEFTè®­ç»ƒçš„adaptersé€šå¸¸æ¯”å®Œæ•´æ¨¡å‹å°ä¸€ä¸ªæ•°é‡çº§ï¼Œä½¿å…¶æ–¹ä¾¿å…±äº«ã€å­˜å‚¨å’ŒåŠ è½½ã€‚\n",
    "\n",
    "<div class=\"flex flex-col justify-center\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png\"/>\n",
    "  <figcaption class=\"text-center\">ä¸å®Œæ•´å°ºå¯¸çš„æ¨¡å‹æƒé‡ï¼ˆçº¦ä¸º700MBï¼‰ç›¸æ¯”ï¼Œå­˜å‚¨åœ¨Hubä¸Šçš„OPTForCausalLMæ¨¡å‹çš„adapteræƒé‡ä»…ä¸º~6MBã€‚</figcaption>\n",
    "</div>\n",
    "\n",
    "å¦‚æœæ‚¨å¯¹å­¦ä¹ æ›´å¤šå…³äºğŸ¤— PEFTåº“æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®¾ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆå®‰è£… ğŸ¤— PEFTï¼š\n",
    "\n",
    "```bash\n",
    "pip install peft\n",
    "```\n",
    "\n",
    "å¦‚æœä½ æƒ³å°è¯•å…¨æ–°çš„ç‰¹æ€§ï¼Œä½ å¯èƒ½ä¼šæœ‰å…´è¶£ä»æºä»£ç å®‰è£…è¿™ä¸ªåº“ï¼š\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/huggingface/peft.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ”¯æŒçš„ PEFT æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransformersåŸç”Ÿæ”¯æŒä¸€äº›PEFTæ–¹æ³•ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åŠ è½½æœ¬åœ°å­˜å‚¨æˆ–åœ¨Hubä¸Šçš„adapteræƒé‡ï¼Œå¹¶ä½¿ç”¨å‡ è¡Œä»£ç è½»æ¾è¿è¡Œæˆ–è®­ç»ƒå®ƒä»¬ã€‚ä»¥ä¸‹æ˜¯å—æ”¯æŒçš„æ–¹æ³•ï¼š\n",
    "\n",
    "- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)\n",
    "- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)\n",
    "- [AdaLoRA](https://huggingface.co/papers/2303.10512)\n",
    "\n",
    "å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–PEFTæ–¹æ³•ï¼Œä¾‹å¦‚æç¤ºå­¦ä¹ æˆ–æç¤ºå¾®è°ƒï¼Œæˆ–è€…å…³äºé€šç”¨çš„ ğŸ¤— PEFTåº“ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŠ è½½ PEFT adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¦ä»huggingfaceçš„Transformersåº“ä¸­åŠ è½½å¹¶ä½¿ç”¨PEFTadapteræ¨¡å‹ï¼Œè¯·ç¡®ä¿Hubä»“åº“æˆ–æœ¬åœ°ç›®å½•åŒ…å«ä¸€ä¸ª`adapter_config.json`æ–‡ä»¶å’Œadapteræƒé‡ï¼Œå¦‚ä¸Šä¾‹æ‰€ç¤ºã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»åŠ è½½PEFT adapteræ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œè¦ä¸ºå› æœè¯­è¨€å»ºæ¨¡åŠ è½½ä¸€ä¸ªPEFT adapteræ¨¡å‹ï¼š\n",
    "\n",
    "1. æŒ‡å®šPEFTæ¨¡å‹id\n",
    "2. å°†å…¶ä¼ é€’ç»™`AutoModelForCausalLM`ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ä½ å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»æˆ–åŸºç¡€æ¨¡å‹ç±»ï¼ˆå¦‚`OPTForCausalLM`æˆ–`LlamaForCausalLM`ï¼‰æ¥åŠ è½½ä¸€ä¸ªPEFT adapterã€‚\n",
    "\n",
    "\n",
    "</Tip>\n",
    "\n",
    "æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡`load_adapter`æ–¹æ³•æ¥åŠ è½½ PEFT adapterã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸºäº8bitæˆ–4bitè¿›è¡ŒåŠ è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bitsandbytes`é›†æˆæ”¯æŒ8bitå’Œ4bitç²¾åº¦æ•°æ®ç±»å‹ï¼Œè¿™å¯¹äºåŠ è½½å¤§æ¨¡å‹éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥èŠ‚çœå†…å­˜ï¼ˆè¯·å‚é˜…`bitsandbytes`[æŒ‡å—](https://huggingface.co/docs/transformers/main/zh/./quantization#bitsandbytes-integration)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ï¼‰ã€‚è¦æœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†é…åˆ°æ‚¨çš„ç¡¬ä»¶ï¼Œè¯·åœ¨[from_pretrained()](https://huggingface.co/docs/transformers/main/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained)ä¸­æ·»åŠ `load_in_8bit`æˆ–`load_in_4bit`å‚æ•°ï¼Œå¹¶å°†`device_map=\"auto\"`è®¾ç½®ä¸ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ·»åŠ æ–°çš„adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯ä»¥ä½¿ç”¨`add_adapter`æ–¹æ³•ä¸ºä¸€ä¸ªå·²æœ‰adapterçš„æ¨¡å‹æ·»åŠ ä¸€ä¸ªæ–°çš„adapterï¼Œåªè¦æ–°adapterçš„ç±»å‹ä¸å½“å‰adapterç›¸åŒå³å¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªé™„åŠ åˆ°æ¨¡å‹ä¸Šçš„LoRA adapterï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    init_lora_weights=False\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ·»åŠ ä¸€ä¸ªæ–°çš„adapterï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach new adapter with same config\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨`set_adapter`æ¥è®¾ç½®è¦ä½¿ç”¨çš„adapterã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use adapter_1\n",
    "model.set_adapter(\"adapter_1\")\n",
    "output = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n",
    "\n",
    "# use adapter_2\n",
    "model.set_adapter(\"adapter_2\")\n",
    "output_enabled = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯ç”¨å’Œç¦ç”¨adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€æ—¦æ‚¨å°†adapteræ·»åŠ åˆ°æ¨¡å‹ä¸­ï¼Œæ‚¨å¯ä»¥å¯ç”¨æˆ–ç¦ç”¨adapteræ¨¡å—ã€‚è¦å¯ç”¨adapteræ¨¡å—ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "adapter_model_id = \"ybelkada/opt-350m-lora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "text = \"Hello\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "peft_config = PeftConfig.from_pretrained(adapter_model_id)\n",
    "\n",
    "# to initiate with random weights\n",
    "peft_config.init_lora_weights = False\n",
    "\n",
    "model.add_adapter(peft_config)\n",
    "model.enable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¦ç¦ç”¨adapteræ¨¡å—ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.disable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒä¸€ä¸ª PEFT adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFTé€‚é…å™¨å—`Trainer`ç±»æ”¯æŒï¼Œå› æ­¤æ‚¨å¯ä»¥ä¸ºæ‚¨çš„ç‰¹å®šç”¨ä¾‹è®­ç»ƒé€‚é…å™¨ã€‚å®ƒåªéœ€è¦æ·»åŠ å‡ è¡Œä»£ç å³å¯ã€‚ä¾‹å¦‚ï¼Œè¦è®­ç»ƒä¸€ä¸ªLoRA adapterï¼š\n",
    "\n",
    "\n",
    "<Tip>\n",
    "\n",
    "å¦‚æœä½ ä¸ç†Ÿæ‚‰å¦‚ä½•ä½¿ç”¨`Trainer`å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/docs/transformers/main/zh/training)æ•™ç¨‹ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "1. ä½¿ç”¨ä»»åŠ¡ç±»å‹å’Œè¶…å‚æ•°å®šä¹‰adapteré…ç½®ï¼ˆå‚è§`LoraConfig`ä»¥äº†è§£è¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å°†adapteræ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ç°åœ¨å¯ä»¥å°†æ¨¡å‹ä¼ é€’ç»™`Trainer`äº†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, ...)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¦ä¿å­˜è®­ç»ƒå¥½çš„adapterå¹¶é‡æ–°åŠ è½½å®ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "TODO: (@younesbelkada @stevhliu)\n",
    "-   Link to PEFT docs for further details\n",
    "-   Trainer  \n",
    "-   8-bit / 4-bit examples ?\n",
    "-->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
