{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æ‚¨å¯ä»¥åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ•°æ®éœ€è¦è¢«é¢„å¤„ç†ä¸ºæœŸæœ›çš„æ¨¡å‹è¾“å…¥æ ¼å¼ã€‚æ— è®ºæ‚¨çš„æ•°æ®æ˜¯æ–‡æœ¬ã€å›¾åƒè¿˜æ˜¯éŸ³é¢‘ï¼Œå®ƒä»¬éƒ½éœ€è¦è¢«è½¬æ¢å¹¶ç»„åˆæˆæ‰¹é‡çš„å¼ é‡ã€‚ğŸ¤— Transformers æä¾›äº†ä¸€ç»„é¢„å¤„ç†ç±»æ¥å¸®åŠ©å‡†å¤‡æ•°æ®ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†äº†è§£ä»¥ä¸‹å†…å®¹ï¼š\n",
    "\n",
    "* å¯¹äºæ–‡æœ¬ï¼Œä½¿ç”¨[åˆ†è¯å™¨](https://huggingface.co/docs/transformers/main/zh/./main_classes/tokenizer)(`Tokenizer`)å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç³»åˆ—æ ‡è®°(`tokens`)ï¼Œå¹¶åˆ›å»º`tokens`çš„æ•°å­—è¡¨ç¤ºï¼Œå°†å®ƒä»¬ç»„åˆæˆå¼ é‡ã€‚\n",
    "* å¯¹äºè¯­éŸ³å’ŒéŸ³é¢‘ï¼Œä½¿ç”¨[ç‰¹å¾æå–å™¨](https://huggingface.co/docs/transformers/main/zh/./main_classes/feature_extractor)(`Feature extractor`)ä»éŸ³é¢‘æ³¢å½¢ä¸­æå–é¡ºåºç‰¹å¾å¹¶å°†å…¶è½¬æ¢ä¸ºå¼ é‡ã€‚\n",
    "* å›¾åƒè¾“å…¥ä½¿ç”¨[å›¾åƒå¤„ç†å™¨](https://huggingface.co/docs/transformers/main/zh/./main_classes/image)(`ImageProcessor`)å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚\n",
    "* å¤šæ¨¡æ€è¾“å…¥ï¼Œä½¿ç”¨[å¤„ç†å™¨](https://huggingface.co/docs/transformers/main/zh/./main_classes/processors)(`Processor`)ç»“åˆäº†`Tokenizer`å’Œ`ImageProcessor`æˆ–`Processor`ã€‚\n",
    "\n",
    "<Tip>\n",
    "\n",
    "`AutoProcessor` **å§‹ç»ˆ**æœ‰æ•ˆçš„è‡ªåŠ¨é€‰æ‹©é€‚ç”¨äºæ‚¨ä½¿ç”¨çš„æ¨¡å‹çš„æ­£ç¡®`class`ï¼Œæ— è®ºæ‚¨ä½¿ç”¨çš„æ˜¯`Tokenizer`ã€`ImageProcessor`ã€`Feature extractor`è¿˜æ˜¯`Processor`ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·å®‰è£…ğŸ¤— Datasetsï¼Œä»¥ä¾¿æ‚¨å¯ä»¥åŠ è½½ä¸€äº›æ•°æ®é›†æ¥è¿›è¡Œå®éªŒï¼š\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªç„¶è¯­è¨€å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Yffk5aydLzg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Yffk5aydLzg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤„ç†æ–‡æœ¬æ•°æ®çš„ä¸»è¦å·¥å…·æ˜¯[Tokenizer](https://huggingface.co/docs/transformers/main/zh/main_classes/tokenizer)ã€‚`Tokenizer`æ ¹æ®ä¸€ç»„è§„åˆ™å°†æ–‡æœ¬æ‹†åˆ†ä¸º`tokens`ã€‚ç„¶åå°†è¿™äº›`tokens`è½¬æ¢ä¸ºæ•°å­—ï¼Œç„¶åè½¬æ¢ä¸ºå¼ é‡ï¼Œæˆä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚æ¨¡å‹æ‰€éœ€çš„ä»»ä½•é™„åŠ è¾“å…¥éƒ½ç”±`Tokenizer`æ·»åŠ ã€‚\n",
    "\n",
    "<Tip>\n",
    "\n",
    "å¦‚æœæ‚¨è®¡åˆ’ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œé‡è¦çš„æ˜¯ä½¿ç”¨ä¸ä¹‹å…³è”çš„é¢„è®­ç»ƒ`Tokenizer`ã€‚è¿™ç¡®ä¿æ–‡æœ¬çš„æ‹†åˆ†æ–¹å¼ä¸é¢„è®­ç»ƒè¯­æ–™åº“ç›¸åŒï¼Œå¹¶åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨ç›¸åŒçš„æ ‡è®°-ç´¢å¼•çš„å¯¹åº”å…³ç³»ï¼ˆé€šå¸¸ç§°ä¸º*è¯æ±‡è¡¨*-`vocab`ï¼‰ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "å¼€å§‹ä½¿ç”¨`AutoTokenizer.from_pretrained()`æ–¹æ³•åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒ`tokenizer`ã€‚è¿™å°†ä¸‹è½½æ¨¡å‹é¢„è®­ç»ƒçš„`vocab`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åå°†æ‚¨çš„æ–‡æœ¬ä¼ é€’ç»™`tokenizer`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer`è¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé‡è¦å¯¹è±¡çš„å­—å…¸ï¼š\n",
    "\n",
    "* [input_ids](https://huggingface.co/docs/transformers/main/zh/glossary#input-ids) æ˜¯ä¸å¥å­ä¸­æ¯ä¸ª`token`å¯¹åº”çš„ç´¢å¼•ã€‚\n",
    "* [attention_mask](https://huggingface.co/docs/transformers/main/zh/glossary#attention-mask) æŒ‡ç¤ºæ˜¯å¦åº”è¯¥å…³æ³¨ä¸€ä¸ª`token`ã€‚\n",
    "* [token_type_ids](https://huggingface.co/docs/transformers/main/zh/glossary#token-type-ids) åœ¨å­˜åœ¨å¤šä¸ªåºåˆ—æ—¶æ ‡è¯†ä¸€ä¸ª`token`å±äºå“ªä¸ªåºåˆ—ã€‚\n",
    "\n",
    "é€šè¿‡è§£ç  `input_ids` æ¥è¿”å›æ‚¨çš„è¾“å…¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æ‚¨æ‰€è§ï¼Œ`tokenizer`å‘å¥å­ä¸­æ·»åŠ äº†ä¸¤ä¸ªç‰¹æ®Š`token` - `CLS` å’Œ `SEP`ï¼ˆåˆ†ç±»å™¨å’Œåˆ†éš”ç¬¦ï¼‰ã€‚å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½éœ€è¦ç‰¹æ®Š`token`ï¼Œä½†å¦‚æœéœ€è¦ï¼Œ`tokenizer`ä¼šè‡ªåŠ¨ä¸ºæ‚¨æ·»åŠ ã€‚\n",
    "\n",
    "å¦‚æœæœ‰å¤šä¸ªå¥å­éœ€è¦é¢„å¤„ç†ï¼Œå°†å®ƒä»¬ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™`tokenizer`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],\n",
       "               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n",
       "               [101, 1327, 1164, 5450, 23434, 136, 102]],\n",
       " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    [0, 0, 0, 0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "                    [1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¡«å……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¥å­çš„é•¿åº¦å¹¶ä¸æ€»æ˜¯ç›¸åŒï¼Œè¿™å¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ¨¡å‹è¾“å…¥çš„å¼ é‡éœ€è¦å…·æœ‰ç»Ÿä¸€çš„å½¢çŠ¶ã€‚å¡«å……æ˜¯ä¸€ç§ç­–ç•¥ï¼Œé€šè¿‡åœ¨è¾ƒçŸ­çš„å¥å­ä¸­æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„`padding token`ï¼Œä»¥ç¡®ä¿å¼ é‡æ˜¯çŸ©å½¢çš„ã€‚\n",
    "\n",
    "å°† `padding` å‚æ•°è®¾ç½®ä¸º `True`ï¼Œä»¥ä½¿æ‰¹æ¬¡ä¸­è¾ƒçŸ­çš„åºåˆ—å¡«å……åˆ°ä¸æœ€é•¿åºåˆ—ç›¸åŒ¹é…çš„é•¿åº¦ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n",
       "               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n",
       "               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_input = tokenizer(batch_sentences, padding=True)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¬¬ä¸€å¥å’Œç¬¬ä¸‰å¥å› ä¸ºè¾ƒçŸ­ï¼Œé€šè¿‡`0`è¿›è¡Œå¡«å……ï¼Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æˆªæ–­"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦ä¸€æ–¹é¢ï¼Œæœ‰æ—¶å€™ä¸€ä¸ªåºåˆ—å¯èƒ½å¯¹æ¨¡å‹æ¥è¯´å¤ªé•¿äº†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦å°†åºåˆ—æˆªæ–­ä¸ºæ›´çŸ­çš„é•¿åº¦ã€‚\n",
    "\n",
    "å°† `truncation` å‚æ•°è®¾ç½®ä¸º `True`ï¼Œä»¥å°†åºåˆ—æˆªæ–­ä¸ºæ¨¡å‹æ¥å—çš„æœ€å¤§é•¿åº¦ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n",
       "               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n",
       "               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "æŸ¥çœ‹[å¡«å……å’Œæˆªæ–­](https://huggingface.co/docs/transformers/main/zh/./pad_truncation)æ¦‚å¿µæŒ‡å—ï¼Œäº†è§£æ›´å¤šæœ‰å…³å¡«å……å’Œæˆªæ–­å‚æ•°çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ„å»ºå¼ é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åï¼Œ`tokenizer`å¯ä»¥è¿”å›å®é™…è¾“å…¥åˆ°æ¨¡å‹çš„å¼ é‡ã€‚\n",
    "\n",
    "å°† `return_tensors` å‚æ•°è®¾ç½®ä¸º `pt`ï¼ˆå¯¹äºPyTorchï¼‰æˆ– `tf`ï¼ˆå¯¹äºTensorFlowï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n",
       "                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n",
       "                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
       "array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n",
       "       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
       " 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## éŸ³é¢‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äºéŸ³é¢‘ä»»åŠ¡ï¼Œæ‚¨éœ€è¦[feature extractor](https://huggingface.co/docs/transformers/main/zh/main_classes/feature_extractor)æ¥å‡†å¤‡æ‚¨çš„æ•°æ®é›†ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚`feature extractor`æ—¨åœ¨ä»åŸå§‹éŸ³é¢‘æ•°æ®ä¸­æå–ç‰¹å¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå¼ é‡ã€‚\n",
    "\n",
    "åŠ è½½[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ä»¥äº†è§£å¦‚ä½•åœ¨éŸ³é¢‘æ•°æ®é›†ä¸­ä½¿ç”¨`feature extractor`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®¿é—® `audio` åˆ—çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä»¥æŸ¥çœ‹è¾“å…¥ã€‚è°ƒç”¨ `audio` åˆ—ä¼šè‡ªåŠ¨åŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n",
       "         0.        ,  0.        ], dtype=float32),\n",
       " 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n",
       " 'sampling_rate': 8000}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ä¼šè¿”å›ä¸‰ä¸ªå¯¹è±¡ï¼š\n",
    "\n",
    "* `array` æ˜¯åŠ è½½çš„è¯­éŸ³ä¿¡å· - å¹¶åœ¨å¿…è¦æ—¶é‡æ–°é‡‡ä¸º`1D array`ã€‚\n",
    "* `path` æŒ‡å‘éŸ³é¢‘æ–‡ä»¶çš„ä½ç½®ã€‚\n",
    "* `sampling_rate` æ˜¯æ¯ç§’æµ‹é‡çš„è¯­éŸ³ä¿¡å·æ•°æ®ç‚¹æ•°é‡ã€‚\n",
    "\n",
    "å¯¹äºæœ¬æ•™ç¨‹ï¼Œæ‚¨å°†ä½¿ç”¨[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)æ¨¡å‹ã€‚æŸ¥çœ‹æ¨¡å‹å¡ç‰‡ï¼Œæ‚¨å°†äº†è§£åˆ°Wav2Vec2æ˜¯åœ¨16kHzé‡‡æ ·çš„è¯­éŸ³éŸ³é¢‘æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ã€‚é‡è¦çš„æ˜¯ï¼Œæ‚¨çš„éŸ³é¢‘æ•°æ®çš„é‡‡æ ·ç‡è¦ä¸ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†çš„é‡‡æ ·ç‡åŒ¹é…ã€‚å¦‚æœæ‚¨çš„æ•°æ®çš„é‡‡æ ·ç‡ä¸åŒï¼Œé‚£ä¹ˆæ‚¨éœ€è¦å¯¹æ•°æ®è¿›è¡Œé‡æ–°é‡‡æ ·ã€‚\n",
    "\n",
    "1. ä½¿ç”¨ğŸ¤— Datasetsçš„`cast_column`æ–¹æ³•å°†é‡‡æ ·ç‡æå‡åˆ°16kHzï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å†æ¬¡è°ƒç”¨ `audio` åˆ—ä»¥é‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n",
       "         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n",
       " 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ª`feature extractor`ä»¥å¯¹è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–å’Œå¡«å……ã€‚å½“å¡«å……æ–‡æœ¬æ•°æ®æ—¶ï¼Œä¼šä¸ºè¾ƒçŸ­çš„åºåˆ—æ·»åŠ  `0`ã€‚ç›¸åŒçš„ç†å¿µé€‚ç”¨äºéŸ³é¢‘æ•°æ®ã€‚`feature extractor`æ·»åŠ  `0` - è¢«è§£é‡Šä¸ºé™éŸ³ - åˆ°`array` ã€‚\n",
    "\n",
    "ä½¿ç”¨ `AutoFeatureExtractor.from_pretrained()` åŠ è½½`feature extractor`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†éŸ³é¢‘ `array` ä¼ é€’ç»™`feature extractor`ã€‚æˆ‘ä»¬è¿˜å»ºè®®åœ¨`feature extractor`ä¸­æ·»åŠ  `sampling_rate` å‚æ•°ï¼Œä»¥æ›´å¥½åœ°è°ƒè¯•å¯èƒ½å‘ç”Ÿçš„é™éŸ³é”™è¯¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,\n",
       "        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_input = [dataset[0][\"audio\"][\"array\"]]\n",
    "feature_extractor(audio_input, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°±åƒ`tokenizer`ä¸€æ ·ï¼Œæ‚¨å¯ä»¥åº”ç”¨å¡«å……æˆ–æˆªæ–­æ¥å¤„ç†æ‰¹æ¬¡ä¸­çš„å¯å˜åºåˆ—ã€‚è¯·æŸ¥çœ‹è¿™ä¸¤ä¸ªéŸ³é¢‘æ ·æœ¬çš„åºåˆ—é•¿åº¦ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173398,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"audio\"][\"array\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106496,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1][\"audio\"][\"array\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é¢„å¤„ç†æ•°æ®é›†ï¼Œä»¥ä½¿éŸ³é¢‘æ ·æœ¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚é€šè¿‡æŒ‡å®šæœ€å¤§æ ·æœ¬é•¿åº¦ï¼Œ`feature extractor`å°†å¡«å……æˆ–æˆªæ–­åºåˆ—ä»¥ä½¿å…¶åŒ¹é…ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        max_length=100000,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†`preprocess_function`åº”ç”¨äºæ•°æ®é›†ä¸­çš„å‰å‡ ä¸ªç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = preprocess_function(dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æ ·æœ¬é•¿åº¦æ˜¯ç›¸åŒçš„ï¼Œå¹¶ä¸”ä¸æŒ‡å®šçš„æœ€å¤§é•¿åº¦åŒ¹é…ã€‚æ‚¨ç°åœ¨å¯ä»¥å°†ç»è¿‡å¤„ç†çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"input_values\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"input_values\"][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®¡ç®—æœºè§†è§‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œæ‚¨éœ€è¦ä¸€ä¸ª[ image processor](https://huggingface.co/docs/transformers/main/zh/main_classes/image_processor)æ¥å‡†å¤‡æ•°æ®é›†ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚å›¾åƒé¢„å¤„ç†åŒ…æ‹¬å¤šä¸ªæ­¥éª¤å°†å›¾åƒè½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›è¾“å…¥çš„æ ¼å¼ã€‚è¿™äº›æ­¥éª¤åŒ…æ‹¬ä½†ä¸é™äºè°ƒæ•´å¤§å°ã€æ ‡å‡†åŒ–ã€é¢œè‰²é€šé“æ ¡æ­£ä»¥åŠå°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚\n",
    "\n",
    "<Tip>\n",
    "\n",
    "å›¾åƒé¢„å¤„ç†é€šå¸¸éµå¾ªæŸç§å½¢å¼çš„å›¾åƒå¢å¼ºã€‚å›¾åƒé¢„å¤„ç†å’Œå›¾åƒå¢å¼ºéƒ½ä¼šæ”¹å˜å›¾åƒæ•°æ®ï¼Œä½†å®ƒä»¬æœ‰ä¸åŒçš„ç›®çš„ï¼š\n",
    "\n",
    "* å›¾åƒå¢å¼ºå¯ä»¥å¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶å¢åŠ æ¨¡å‹çš„é²æ£’æ€§ã€‚æ‚¨å¯ä»¥åœ¨æ•°æ®å¢å¼ºæ–¹é¢å……åˆ†å‘æŒ¥åˆ›é€ æ€§ - è°ƒæ•´äº®åº¦å’Œé¢œè‰²ã€è£å‰ªã€æ—‹è½¬ã€è°ƒæ•´å¤§å°ã€ç¼©æ”¾ç­‰ã€‚ä½†è¦æ³¨æ„ä¸è¦æ”¹å˜å›¾åƒçš„å«ä¹‰ã€‚\n",
    "* å›¾åƒé¢„å¤„ç†ç¡®ä¿å›¾åƒä¸æ¨¡å‹é¢„æœŸçš„è¾“å…¥æ ¼å¼åŒ¹é…ã€‚åœ¨å¾®è°ƒè®¡ç®—æœºè§†è§‰æ¨¡å‹æ—¶ï¼Œå¿…é¡»å¯¹å›¾åƒè¿›è¡Œä¸æ¨¡å‹è®­ç»ƒæ—¶ç›¸åŒçš„é¢„å¤„ç†ã€‚\n",
    "\n",
    "æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„å›¾åƒå¢å¼ºåº“ã€‚å¯¹äºå›¾åƒé¢„å¤„ç†ï¼Œè¯·ä½¿ç”¨ä¸æ¨¡å‹ç›¸å…³è”çš„`ImageProcessor`ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "åŠ è½½[food101](https://huggingface.co/datasets/food101)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ä»¥äº†è§£å¦‚ä½•åœ¨è®¡ç®—æœºè§†è§‰æ•°æ®é›†ä¸­ä½¿ç”¨å›¾åƒå¤„ç†å™¨ï¼š\n",
    "\n",
    "<Tip>\n",
    "\n",
    "å› ä¸ºæ•°æ®é›†ç›¸å½“å¤§ï¼Œè¯·ä½¿ç”¨ğŸ¤— Datasetsçš„`split`å‚æ•°åŠ è½½è®­ç»ƒé›†ä¸­çš„å°‘é‡æ ·æœ¬ï¼\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"food101\", split=\"train[:100]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ğŸ¤— Datasetsçš„[`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)åŠŸèƒ½æŸ¥çœ‹å›¾åƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png\"/>\n",
    "</div>\n",
    "\n",
    "ä½¿ç”¨ `AutoImageProcessor.from_pretrained()` åŠ è½½`image processor`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆï¼Œè®©æˆ‘ä»¬è¿›è¡Œå›¾åƒå¢å¼ºã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„åº“ï¼Œä½†åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨torchvisionçš„[`transforms`](https://pytorch.org/vision/stable/transforms.html)æ¨¡å—ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£ä½¿ç”¨å…¶ä»–æ•°æ®å¢å¼ºåº“ï¼Œè¯·å‚é˜…[Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)æˆ–[Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)ä¸­çš„ç¤ºä¾‹ã€‚\n",
    "\n",
    "1. åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨[`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)å°†[`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)å’Œ [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)å˜æ¢è¿æ¥åœ¨ä¸€èµ·ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºè°ƒæ•´å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥ä»`image_processor`ä¸­è·å–å›¾åƒå°ºå¯¸è¦æ±‚ã€‚å¯¹äºä¸€äº›æ¨¡å‹ï¼Œç²¾ç¡®çš„é«˜åº¦å’Œå®½åº¦éœ€è¦è¢«å®šä¹‰ï¼Œå¯¹äºå…¶ä»–æ¨¡å‹åªéœ€å®šä¹‰`shortest_edge`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose\n",
    "\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "\n",
    "_transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. æ¨¡å‹æ¥å— [`pixel_values`](https://huggingface.co/docs/transformers/main/zh/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values) ä½œä¸ºè¾“å…¥ã€‚`ImageProcessor` å¯ä»¥è¿›è¡Œå›¾åƒçš„æ ‡å‡†åŒ–ï¼Œå¹¶ç”Ÿæˆé€‚å½“çš„å¼ é‡ã€‚åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†å›¾åƒå¢å¼ºå’Œå›¾åƒé¢„å¤„ç†æ­¥éª¤ç»„åˆèµ·æ¥å¤„ç†æ‰¹é‡å›¾åƒï¼Œå¹¶ç”Ÿæˆ `pixel_values`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    images = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    examples[\"pixel_values\"] = image_processor(images, do_resize=False, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®`do_resize=False`ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨å›¾åƒå¢å¼ºè½¬æ¢ä¸­è°ƒæ•´äº†å›¾åƒçš„å¤§å°ï¼Œå¹¶åˆ©ç”¨äº†é€‚å½“çš„`image_processor`çš„`size`å±æ€§ã€‚å¦‚æœæ‚¨åœ¨å›¾åƒå¢å¼ºæœŸé—´ä¸è°ƒæ•´å›¾åƒçš„å¤§å°ï¼Œè¯·å°†æ­¤å‚æ•°æ’é™¤åœ¨å¤–ã€‚é»˜è®¤æƒ…å†µä¸‹`ImageProcessor`å°†å¤„ç†è°ƒæ•´å¤§å°ã€‚\n",
    "\n",
    "å¦‚æœå¸Œæœ›å°†å›¾åƒæ ‡å‡†åŒ–æ­¥éª¤ä¸ºå›¾åƒå¢å¼ºçš„ä¸€éƒ¨åˆ†ï¼Œè¯·ä½¿ç”¨`image_processor.image_mean`å’Œ`image_processor.image_std`ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "3. ç„¶åä½¿ç”¨ğŸ¤— Datasetsçš„[`set_transform`](https://huggingface.co/docs/datasets/process#format-transform)åœ¨è¿è¡Œæ—¶åº”ç”¨è¿™äº›å˜æ¢ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ç°åœ¨ï¼Œå½“æ‚¨è®¿é—®å›¾åƒæ—¶ï¼Œæ‚¨å°†æ³¨æ„åˆ°`image processor`å·²æ·»åŠ äº† `pixel_values`ã€‚æ‚¨ç°åœ¨å¯ä»¥å°†ç»è¿‡å¤„ç†çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™æ˜¯åœ¨åº”ç”¨å˜æ¢åçš„å›¾åƒæ ·å­ã€‚å›¾åƒå·²è¢«éšæœºè£å‰ªï¼Œå¹¶å…¶é¢œè‰²å±æ€§å‘ç”Ÿäº†å˜åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = dataset[0][\"pixel_values\"]\n",
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png\"/>\n",
    "</div>\n",
    "\n",
    "<Tip>\n",
    "\n",
    "å¯¹äºè¯¸å¦‚ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ç­‰ä»»åŠ¡ï¼Œ`ImageProcessor`æä¾›äº†è®­ç»ƒåå¤„ç†æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å°†æ¨¡å‹çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºæœ‰æ„ä¹‰çš„é¢„æµ‹ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–åˆ†å‰²åœ°å›¾ã€‚\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¡«å……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚ï¼Œåœ¨å¾®è°ƒ[DETR](https://huggingface.co/docs/transformers/main/zh/./model_doc/detr)æ—¶ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ—¶åº”ç”¨äº†å°ºåº¦å¢å¼ºã€‚è¿™å¯èƒ½å¯¼è‡´æ‰¹å¤„ç†ä¸­çš„å›¾åƒå¤§å°ä¸åŒã€‚æ‚¨å¯ä»¥ä½¿ç”¨`DetrImageProcessor.pad()`æ¥æŒ‡å®šè‡ªå®šä¹‰çš„`collate_fn`å°†å›¾åƒæ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¤šæ¨¡æ€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äºæ¶‰åŠå¤šæ¨¡æ€è¾“å…¥çš„ä»»åŠ¡ï¼Œæ‚¨éœ€è¦[processor](https://huggingface.co/docs/transformers/main/zh/main_classes/processors)æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®é›†ã€‚`processor`å°†ä¸¤ä¸ªå¤„ç†å¯¹è±¡-ä¾‹å¦‚`tokenizer`å’Œ`feature extractor`-ç»„åˆåœ¨ä¸€èµ·ã€‚\n",
    "\n",
    "åŠ è½½[LJ Speech](https://huggingface.co/datasets/lj_speech)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasets æ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ä»¥äº†è§£å¦‚ä½•ä½¿ç”¨`processor`è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "lj_speech = load_dataset(\"lj_speech\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äºASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰ï¼Œä¸»è¦å…³æ³¨`audio`å’Œ`text`ï¼Œå› æ­¤å¯ä»¥åˆ é™¤å…¶ä»–åˆ—ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_speech = lj_speech.map(remove_columns=[\"file\", \"id\", \"normalized_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æŸ¥çœ‹`audio`å’Œ`text`åˆ—ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,\n",
       "         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),\n",
       " 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',\n",
       " 'sampling_rate': 22050}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lj_speech[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lj_speech[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯·è®°ä½ï¼Œæ‚¨åº”å§‹ç»ˆ[é‡æ–°é‡‡æ ·](https://huggingface.co/docs/transformers/main/zh/preprocessing#audio)éŸ³é¢‘æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼Œä»¥åŒ¹é…ç”¨äºé¢„è®­ç»ƒæ¨¡å‹æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_speech = lj_speech.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨`AutoProcessor.from_pretrained()`åŠ è½½ä¸€ä¸ª`processor`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå°†åŒ…å«åœ¨ `array` ä¸­çš„éŸ³é¢‘æ•°æ®å¤„ç†ä¸º `input_values`ï¼Œå¹¶å°† `text` æ ‡è®°ä¸º `labels`ã€‚è¿™äº›å°†æ˜¯è¾“å…¥æ¨¡å‹çš„æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example.update(processor(audio=audio[\"array\"], text=example[\"text\"], sampling_rate=16000))\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å°† `prepare_dataset` å‡½æ•°åº”ç”¨äºä¸€ä¸ªç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset(lj_speech[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`processor`ç°åœ¨å·²ç»æ·»åŠ äº† `input_values` å’Œ `labels`ï¼Œå¹¶ä¸”é‡‡æ ·ç‡ä¹Ÿæ­£ç¡®é™ä½ä¸ºä¸º16kHzã€‚ç°åœ¨å¯ä»¥å°†å¤„ç†åçš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹ï¼"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
