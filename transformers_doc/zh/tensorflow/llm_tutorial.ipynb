{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨LLMsè¿›è¡Œç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMsï¼Œå³å¤§è¯­è¨€æ¨¡å‹ï¼Œæ˜¯æ–‡æœ¬ç”ŸæˆèƒŒåçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒä»¬åŒ…å«ç»è¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒçš„transformeræ¨¡å‹ï¼Œç”¨äºæ ¹æ®ç»™å®šçš„è¾“å…¥æ–‡æœ¬é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼ˆæˆ–æ›´å‡†ç¡®åœ°è¯´ï¼Œä¸‹ä¸€ä¸ª`token`ï¼‰ã€‚ç”±äºå®ƒä»¬ä¸€æ¬¡åªé¢„æµ‹ä¸€ä¸ª`token`ï¼Œå› æ­¤é™¤äº†è°ƒç”¨æ¨¡å‹ä¹‹å¤–ï¼Œæ‚¨éœ€è¦æ‰§è¡Œæ›´å¤æ‚çš„æ“ä½œæ¥ç”Ÿæˆæ–°çš„å¥å­â€”â€”æ‚¨éœ€è¦è¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚\n",
    "\n",
    "è‡ªå›å½’ç”Ÿæˆæ˜¯åœ¨ç»™å®šä¸€äº›åˆå§‹è¾“å…¥ï¼Œé€šè¿‡è¿­ä»£è°ƒç”¨æ¨¡å‹åŠå…¶è‡ªèº«çš„ç”Ÿæˆè¾“å‡ºæ¥ç”Ÿæˆæ–‡æœ¬çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨ğŸ¤— Transformersä¸­ï¼Œè¿™ç”±[generate()](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationMixin.generate)æ–¹æ³•å¤„ç†ï¼Œæ‰€æœ‰å…·æœ‰ç”Ÿæˆèƒ½åŠ›çš„æ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨è¯¥æ–¹æ³•ã€‚\n",
    "\n",
    "æœ¬æ•™ç¨‹å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š\n",
    "\n",
    "* ä½¿ç”¨LLMç”Ÿæˆæ–‡æœ¬\n",
    "* é¿å…å¸¸è§çš„é™·é˜±\n",
    "* å¸®åŠ©æ‚¨å……åˆ†åˆ©ç”¨LLMä¸‹ä¸€æ­¥æŒ‡å¯¼\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install transformers bitsandbytes>=0.39.0 -q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆæ–‡æœ¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€ä¸ªç”¨äº[å› æœè¯­è¨€å»ºæ¨¡](https://huggingface.co/docs/transformers/main/zh/tasks/language_modeling)è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå°†æ–‡æœ¬`tokens`åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ª`token`çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "\n",
    "<!-- [GIF 1 -- FWD PASS] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"LLMçš„å‰å‘ä¼ é€’\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "ä½¿ç”¨LLMè¿›è¡Œè‡ªå›å½’ç”Ÿæˆçš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯å¦‚ä½•ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ª`token`ã€‚è¿™ä¸ªæ­¥éª¤å¯ä»¥éšæ„è¿›è¡Œï¼Œåªè¦æœ€ç»ˆå¾—åˆ°ä¸‹ä¸€ä¸ªè¿­ä»£çš„`token`ã€‚è¿™æ„å‘³ç€å¯ä»¥ç®€å•çš„ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©æœ€å¯èƒ½çš„`token`ï¼Œä¹Ÿå¯ä»¥å¤æ‚çš„åœ¨å¯¹ç»“æœåˆ†å¸ƒè¿›è¡Œé‡‡æ ·ä¹‹å‰åº”ç”¨å¤šç§å˜æ¢ï¼Œè¿™å–å†³äºä½ çš„éœ€æ±‚ã€‚\n",
    "\n",
    "<!-- [GIF 2 -- TEXT GENERATION] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"è‡ªå›å½’ç”Ÿæˆè¿­ä»£åœ°ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªtokenä»¥ç”Ÿæˆæ–‡æœ¬\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "ä¸Šè¿°è¿‡ç¨‹æ˜¯è¿­ä»£é‡å¤çš„ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªåœæ­¢æ¡ä»¶ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœæ­¢æ¡ä»¶ç”±æ¨¡å‹å†³å®šï¼Œè¯¥æ¨¡å‹åº”å­¦ä¼šåœ¨ä½•æ—¶è¾“å‡ºä¸€ä¸ªç»“æŸåºåˆ—ï¼ˆ`EOS`ï¼‰æ ‡è®°ã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œç”Ÿæˆå°†åœ¨è¾¾åˆ°æŸä¸ªé¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æ—¶åœæ­¢ã€‚\n",
    "\n",
    "æ­£ç¡®è®¾ç½®`token`é€‰æ‹©æ­¥éª¤å’Œåœæ­¢æ¡ä»¶å¯¹äºè®©ä½ çš„æ¨¡å‹æŒ‰ç…§é¢„æœŸçš„æ–¹å¼æ‰§è¡Œä»»åŠ¡è‡³å…³é‡è¦ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ª[~generation.GenerationConfig]æ–‡ä»¶ï¼Œå®ƒåŒ…å«ä¸€ä¸ªæ•ˆæœä¸é”™çš„é»˜è®¤ç”Ÿæˆå‚æ•°é…ç½®ï¼Œå¹¶ä¸æ‚¨æ¨¡å‹ä¸€èµ·åŠ è½½ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬è°ˆè°ˆä»£ç ï¼\n",
    "\n",
    "<Tip>\n",
    "\n",
    "å¦‚æœæ‚¨å¯¹åŸºæœ¬çš„LLMä½¿ç”¨æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬é«˜çº§çš„[`Pipeline`](https://huggingface.co/docs/transformers/main/zh/pipeline_tutorial)æ¥å£æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚ç„¶è€Œï¼ŒLLMsé€šå¸¸éœ€è¦åƒ`quantization`å’Œ`tokené€‰æ‹©æ­¥éª¤çš„ç²¾ç»†æ§åˆ¶`ç­‰é«˜çº§åŠŸèƒ½ï¼Œè¿™æœ€å¥½é€šè¿‡[generate()](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationMixin.generate)æ¥å®Œæˆã€‚ä½¿ç”¨LLMè¿›è¡Œè‡ªå›å½’ç”Ÿæˆä¹Ÿæ˜¯èµ„æºå¯†é›†å‹çš„æ“ä½œï¼Œåº”è¯¥åœ¨GPUä¸Šæ‰§è¡Œä»¥è·å¾—è¶³å¤Ÿçš„ååé‡ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "é¦–å…ˆï¼Œæ‚¨éœ€è¦åŠ è½½æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨å°†ä¼šæ³¨æ„åˆ°åœ¨`from_pretrained`è°ƒç”¨ä¸­çš„ä¸¤ä¸ªæ ‡å¿—ï¼š\n",
    "\n",
    "- `device_map`ç¡®ä¿æ¨¡å‹è¢«ç§»åŠ¨åˆ°æ‚¨çš„GPU(s)ä¸Š\n",
    "- `load_in_4bit`åº”ç”¨[4ä½åŠ¨æ€é‡åŒ–](https://huggingface.co/docs/transformers/main/zh/main_classes/quantization)æ¥æå¤§åœ°å‡å°‘èµ„æºéœ€æ±‚\n",
    "\n",
    "è¿˜æœ‰å…¶ä»–æ–¹å¼æ¥åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªå¼€å§‹ä½¿ç”¨LLMå¾ˆå¥½çš„èµ·ç‚¹ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œä½ éœ€è¦ä½¿ç”¨ä¸€ä¸ª[tokenizer](https://huggingface.co/docs/transformers/main/zh/tokenizer_summary)æ¥é¢„å¤„ç†ä½ çš„æ–‡æœ¬è¾“å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_inputs`å˜é‡ä¿å­˜ç€åˆ†è¯åçš„æ–‡æœ¬è¾“å…¥ä»¥åŠæ³¨æ„åŠ›æ©ç ã€‚å°½ç®¡[generate()](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationMixin.generate)åœ¨æœªä¼ é€’æ³¨æ„åŠ›æ©ç æ—¶ä¼šå°½å…¶æ‰€èƒ½æ¨æ–­å‡ºæ³¨æ„åŠ›æ©ç ï¼Œä½†å»ºè®®å°½å¯èƒ½ä¼ é€’å®ƒä»¥è·å¾—æœ€ä½³ç»“æœã€‚\n",
    "\n",
    "åœ¨å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯åï¼Œå¯ä»¥è°ƒç”¨[generate()](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationMixin.generate)æ–¹æ³•æ¥è¿”å›ç”Ÿæˆçš„`tokens`ã€‚ç”Ÿæˆçš„`tokens`åº”è¯¥åœ¨æ‰“å°ä¹‹å‰è½¬æ¢ä¸ºæ–‡æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A list of colors: red, blue, green, yellow, orange, purple, pink,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åï¼Œæ‚¨ä¸éœ€è¦ä¸€æ¬¡å¤„ç†ä¸€ä¸ªåºåˆ—ï¼æ‚¨å¯ä»¥æ‰¹é‡è¾“å…¥ï¼Œè¿™å°†åœ¨å°å»¶è¿Ÿå’Œä½å†…å­˜æˆæœ¬ä¸‹æ˜¾è‘—æé«˜ååé‡ã€‚æ‚¨åªéœ€è¦ç¡®ä¿æ­£ç¡®åœ°å¡«å……æ‚¨çš„è¾“å…¥ï¼ˆè¯¦è§ä¸‹æ–‡ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A list of colors: red, blue, green, yellow, orange, purple, pink,',\n",
       "'Portugal is a country in southwestern Europe, on the Iber']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n",
    ").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°±æ˜¯è¿™æ ·ï¼åœ¨å‡ è¡Œä»£ç ä¸­ï¼Œæ‚¨å°±å¯ä»¥åˆ©ç”¨LLMçš„å¼ºå¤§åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¸¸è§é™·é˜±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ‰è®¸å¤š[ç”Ÿæˆç­–ç•¥](https://huggingface.co/docs/transformers/main/zh/generation_strategies)ï¼Œæœ‰æ—¶é»˜è®¤å€¼å¯èƒ½ä¸é€‚åˆæ‚¨çš„ç”¨ä¾‹ã€‚å¦‚æœæ‚¨çš„è¾“å‡ºä¸æ‚¨æœŸæœ›çš„ç»“æœä¸åŒ¹é…ï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªæœ€å¸¸è§çš„é™·é˜±åˆ—è¡¨ä»¥åŠå¦‚ä½•é¿å…å®ƒä»¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç”Ÿæˆçš„è¾“å‡ºå¤ªçŸ­/å¤ªé•¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœåœ¨[GenerationConfig](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationConfig)æ–‡ä»¶ä¸­æ²¡æœ‰æŒ‡å®šï¼Œ`generate`é»˜è®¤è¿”å›20ä¸ªtokensã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®åœ¨æ‚¨çš„`generate`è°ƒç”¨ä¸­æ‰‹åŠ¨è®¾ç½®`max_new_tokens`ä»¥æ§åˆ¶å®ƒå¯ä»¥è¿”å›çš„æœ€å¤§æ–°tokensæ•°é‡ã€‚è¯·æ³¨æ„ï¼ŒLLMsï¼ˆæ›´å‡†ç¡®åœ°è¯´ï¼Œä»…[è§£ç å™¨æ¨¡å‹](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)ï¼‰ä¹Ÿå°†è¾“å…¥æç¤ºä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†è¿”å›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# By default, the output will contain up to 20 tokens\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting `max_new_tokens` allows you to control the maximum length\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é”™è¯¯çš„ç”Ÿæˆæ¨¡å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é»˜è®¤æƒ…å†µä¸‹ï¼Œé™¤éåœ¨[GenerationConfig](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationConfig)æ–‡ä»¶ä¸­æŒ‡å®šï¼Œå¦åˆ™`generate`ä¼šåœ¨æ¯ä¸ªè¿­ä»£ä¸­é€‰æ‹©æœ€å¯èƒ½çš„tokenï¼ˆè´ªå©ªè§£ç ï¼‰ã€‚å¯¹äºæ‚¨çš„ä»»åŠ¡ï¼Œè¿™å¯èƒ½æ˜¯ä¸ç†æƒ³çš„ï¼›åƒèŠå¤©æœºå™¨äººæˆ–å†™ä½œæ–‡ç« è¿™æ ·çš„åˆ›é€ æ€§ä»»åŠ¡å—ç›Šäºé‡‡æ ·ã€‚å¦ä¸€æ–¹é¢ï¼ŒåƒéŸ³é¢‘è½¬å½•æˆ–ç¿»è¯‘è¿™æ ·çš„åŸºäºè¾“å…¥çš„ä»»åŠ¡å—ç›Šäºè´ªå©ªè§£ç ã€‚é€šè¿‡å°†`do_sample=True`å¯ç”¨é‡‡æ ·ï¼Œæ‚¨å¯ä»¥åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/how-to-generate)ä¸­äº†è§£æ›´å¤šå…³äºè¿™ä¸ªè¯é¢˜çš„ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat. I am a cat. I am a cat. I am a cat'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed or reproducibility -- you don't need this unless you want full reproducibility\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# LLM + greedy decoding = repetitive, boring output\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat.  Specifically, I am an indoor-only cat.  I'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With sampling, the output becomes more creative!\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é”™è¯¯çš„å¡«å……ä½ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMsæ˜¯[ä»…è§£ç å™¨](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)æ¶æ„ï¼Œæ„å‘³ç€å®ƒä»¬ä¼šæŒç»­è¿­ä»£æ‚¨çš„è¾“å…¥æç¤ºã€‚å¦‚æœæ‚¨çš„è¾“å…¥é•¿åº¦ä¸ç›¸åŒï¼Œåˆ™éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œå¡«å……ã€‚ç”±äºLLMsæ²¡æœ‰æ¥å—è¿‡ä»`pad tokens`ç»§ç»­è®­ç»ƒï¼Œå› æ­¤æ‚¨çš„è¾“å…¥éœ€è¦å·¦å¡«å……ã€‚ç¡®ä¿åœ¨ç”Ÿæˆæ—¶ä¸è¦å¿˜è®°ä¼ é€’æ³¨æ„åŠ›æ©ç ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 33333333333'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer initialized above has right-padding active by default: the 1st sequence,\n",
    "# which is shorter, has padding on the right side. Generation fails to capture the logic.\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 3, 4, 5, 6,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With left-padding, it works as expected!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é”™è¯¯çš„æç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€äº›æ¨¡å‹å’Œä»»åŠ¡æœŸæœ›æŸç§è¾“å…¥æç¤ºæ ¼å¼æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚å½“æœªåº”ç”¨æ­¤æ ¼å¼æ—¶ï¼Œæ‚¨å°†è·å¾—æ‚„ç„¶çš„æ€§èƒ½ä¸‹é™ï¼šæ¨¡å‹èƒ½å·¥ä½œï¼Œä½†ä¸å¦‚é¢„æœŸæç¤ºé‚£æ ·å¥½ã€‚æœ‰å…³æç¤ºçš„æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ªäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦å°å¿ƒï¼Œå¯åœ¨[æŒ‡å—](https://huggingface.co/docs/transformers/main/zh/tasks/prompting)ä¸­æ‰¾åˆ°ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä½¿ç”¨[èŠå¤©æ¨¡æ¿](https://huggingface.co/docs/transformers/main/zh/chat_templating)çš„èŠå¤©LLMç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not a thug, but i can tell you that a human cannot eat\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")\n",
    "set_seed(0)\n",
    "prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "input_length = model_inputs.input_ids.shape[1]\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None, you thug. How bout you try to focus on more useful questions?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write\n",
    "# a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)\n",
    "\n",
    "set_seed(0)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "input_length = model_inputs.shape[1]\n",
    "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, it followed a proper thug style ğŸ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ›´å¤šèµ„æº"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è™½ç„¶è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ç›¸å¯¹ç®€å•ï¼Œä½†è¦å……åˆ†åˆ©ç”¨LLMå¯èƒ½æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå¾ˆå¤šç»„ä»¶å¤æ‚ä¸”å¯†åˆ‡å…³è”ã€‚ä»¥ä¸‹æ˜¯å¸®åŠ©æ‚¨æ·±å…¥äº†è§£LLMä½¿ç”¨å’Œç†è§£çš„ä¸‹ä¸€æ­¥ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é«˜çº§ç”Ÿæˆç”¨æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [æŒ‡å—](https://huggingface.co/docs/transformers/main/zh/generation_strategies)ï¼Œä»‹ç»å¦‚ä½•æ§åˆ¶ä¸åŒçš„ç”Ÿæˆæ–¹æ³•ã€å¦‚ä½•è®¾ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶ä»¥åŠå¦‚ä½•è¿›è¡Œè¾“å‡ºæµå¼ä¼ è¾“ï¼›\n",
    "2. [æŒ‡å—](https://huggingface.co/docs/transformers/main/zh/chat_templating)ï¼Œä»‹ç»èŠå¤©LLMsçš„æç¤ºæ¨¡æ¿ï¼›\n",
    "3. [æŒ‡å—](https://huggingface.co/docs/transformers/main/zh/tasks/prompting)ï¼Œä»‹ç»å¦‚ä½•å……åˆ†åˆ©ç”¨æç¤ºè®¾è®¡ï¼›\n",
    "4. APIå‚è€ƒæ–‡æ¡£ï¼ŒåŒ…æ‹¬[GenerationConfig](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationConfig)ã€[generate()](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationMixin.generate)å’Œ[ä¸ç”Ÿæˆç›¸å…³çš„ç±»](https://huggingface.co/docs/transformers/main/zh/internal/generation_utils)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMæ’è¡Œæ¦œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), ä¾§é‡äºå¼€æºæ¨¡å‹çš„è´¨é‡;\n",
    "2. [Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard), ä¾§é‡äºLLMçš„ååé‡."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å»¶è¿Ÿã€ååé‡å’Œå†…å­˜åˆ©ç”¨ç‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [æŒ‡å—](https://huggingface.co/docs/transformers/main/zh/llm_tutorial_optimization),å¦‚ä½•ä¼˜åŒ–LLMsä»¥æé«˜é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨ï¼›\n",
    "2. [æŒ‡å—](https://huggingface.co/docs/transformers/main/zh/main_classes/quantization), å…³äº`quantization`ï¼Œå¦‚bitsandbyteså’Œautogptqçš„æŒ‡å—ï¼Œæ•™æ‚¨å¦‚ä½•å¤§å¹…é™ä½å†…å­˜éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç›¸å…³åº“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`text-generation-inference`](https://github.com/huggingface/text-generation-inference), ä¸€ä¸ªé¢å‘ç”Ÿäº§çš„LLMæœåŠ¡å™¨ï¼›\n",
    "2. [`optimum`](https://github.com/huggingface/optimum), ä¸€ä¸ªğŸ¤— Transformersçš„æ‰©å±•ï¼Œä¼˜åŒ–ç‰¹å®šç¡¬ä»¶è®¾å¤‡çš„æ€§èƒ½"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
