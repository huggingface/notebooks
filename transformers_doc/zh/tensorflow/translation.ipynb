{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¿»è¯‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1JvfrvZgi6c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1JvfrvZgi6c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¿»è¯‘å°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€ã€‚å®ƒæ˜¯å¯ä»¥è¡¨è¿°ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜çš„å‡ ä¸ªä»»åŠ¡ä¹‹ä¸€â€”â€”è¿™æ˜¯ä¸€ç§ä»è¾“å…¥è¿”å›æŸäº›è¾“å‡ºçš„å¼ºå¤§æ¡†æ¶ï¼Œé€‚ç”¨äºç¿»è¯‘æˆ–æ‘˜è¦ç­‰ä»»åŠ¡ã€‚ç¿»è¯‘ç³»ç»Ÿé€šå¸¸ç”¨äºä¸åŒè¯­è¨€æ–‡æœ¬ä¹‹é—´çš„è½¬æ¢ï¼Œä½†ä¹Ÿå¯ä»¥ç”¨äºè¯­éŸ³ï¼Œæˆ–è€…æ–‡æœ¬è½¬è¯­éŸ³ã€è¯­éŸ³è½¬æ–‡æœ¬ç­‰ç»„åˆåœºæ™¯ã€‚\n",
    "\n",
    "æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š\n",
    "\n",
    "1. åœ¨ [OPUS Books](https://huggingface.co/datasets/opus_books) æ•°æ®é›†çš„è‹±æ³•å­é›†ä¸Šå¾®è°ƒ [T5](https://huggingface.co/google-t5/t5-small)ï¼Œå°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆæ³•æ–‡ã€‚\n",
    "2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚\n",
    "\n",
    "<Tip>\n",
    "\n",
    "å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ‰€æœ‰ä¸æœ¬ä»»åŠ¡å…¼å®¹çš„æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œæœ€å¥½æŸ¥çœ‹[ä»»åŠ¡é¡µ](https://huggingface.co/tasks/translation)ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate sacrebleu\n",
    "```\n",
    "\n",
    "å»ºè®®æ‚¨ç™»å½• Hugging Face è´¦æˆ·ï¼Œä»¥ä¾¿å°†æ¨¡å‹ä¸Šä¼ å¹¶åˆ†äº«ç»™ç¤¾åŒºã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œè¿›è¡Œç™»å½•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŠ è½½ OPUS Books æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆä» ğŸ¤— Datasets åº“ä¸­åŠ è½½ [OPUS Books](https://huggingface.co/datasets/opus_books) æ•°æ®é›†çš„è‹±æ³•å­é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ `train_test_split` æ–¹æ³•å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '90560',\n",
       " 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',\n",
       "  'fr': 'Mais ce plateau Ã©levÃ© ne mesurait que quelques toises, et bientÃ´t nous fÃ»mes rentrÃ©s dans notre Ã©lÃ©ment.'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`translation`ï¼šæ–‡æœ¬çš„è‹±æ–‡å’Œæ³•æ–‡ç¿»è¯‘ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XAR8jnZZuUs?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XAR8jnZZuUs?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ T5 åˆ†è¯å™¨ï¼Œå¤„ç†è‹±æ³•è¯­è¨€å¯¹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨è¦åˆ›å»ºçš„é¢„å¤„ç†å‡½æ•°éœ€è¦ï¼š\n",
    "\n",
    "1. åœ¨è¾“å…¥å‰æ·»åŠ æç¤ºè¯ï¼Œè®© T5 çŸ¥é“è¿™æ˜¯ä¸€ä¸ªç¿»è¯‘ä»»åŠ¡ã€‚æŸäº›èƒ½å¤Ÿå¤„ç†å¤šç§ NLP ä»»åŠ¡çš„æ¨¡å‹éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡æç¤ºã€‚\n",
    "2. åœ¨ `text_target` å‚æ•°ä¸­è®¾ç½®ç›®æ ‡è¯­è¨€ï¼ˆæ³•è¯­ï¼‰ï¼Œä»¥ç¡®ä¿åˆ†è¯å™¨èƒ½æ­£ç¡®å¤„ç†ç›®æ ‡æ–‡æœ¬ã€‚å¦‚æœä¸è®¾ç½® `text_target`ï¼Œåˆ†è¯å™¨ä¼šå°†ç›®æ ‡æ–‡æœ¬ä½œä¸ºè‹±è¯­å¤„ç†ã€‚\n",
    "3. å°†åºåˆ—æˆªæ–­è‡³ä¸è¶…è¿‡ `max_length` å‚æ•°è®¾ç½®çš„æœ€å¤§é•¿åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ ğŸ¤— Datasets çš„ `map` æ–¹æ³•å°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚é€šè¿‡è®¾ç½® `batched=True` ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œå¯ä»¥åŠ é€Ÿ `map` å‡½æ•°ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ä½¿ç”¨ [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main/zh/main_classes/data_collator#transformers.DataCollatorForSeq2Seq) åˆ›å»ºä¸€æ‰¹æ ·æœ¬ã€‚åœ¨æ•´ç†æ—¶å°†å¥å­*åŠ¨æ€å¡«å……*è‡³æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ï¼Œæ¯”å°†æ•´ä¸ªæ•°æ®é›†å¡«å……è‡³æœ€å¤§é•¿åº¦æ›´é«˜æ•ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥è¯„ä¼°æŒ‡æ ‡æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½ [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) æŒ‡æ ‡ï¼ˆå‚é˜… ğŸ¤— Evaluate [å¿«é€Ÿæ•™ç¨‹](https://huggingface.co/docs/evaluate/a_quick_tour)ï¼Œäº†è§£æ›´å¤šå…³äºåŠ è½½å’Œè®¡ç®—æŒ‡æ ‡çš„ä¿¡æ¯ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹ç»“æœå’Œæ ‡ç­¾ä¼ é€’ç»™ `compute` æ¥è®¡ç®— SacreBLEU åˆ†æ•°ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨çš„ `compute_metrics` å‡½æ•°å·²å‡†å¤‡å°±ç»ªï¼Œåœ¨è®¾ç½®è®­ç»ƒæ—¶ä¼šç”¨åˆ°å®ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ `Trainer` å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](https://huggingface.co/docs/transformers/main/zh/tasks/../training#train-with-pytorch-trainer)çš„åŸºç¡€æ•™ç¨‹ï¼\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨ `AutoModelForSeq2SeqLM` åŠ è½½ T5ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤æ—¶ï¼Œåªå‰©ä¸‰ä¸ªæ­¥éª¤ï¼š\n",
    "\n",
    "1. åœ¨ `Seq2SeqTrainingArguments` ä¸­å®šä¹‰è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œå®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½® `push_to_hub=True`ï¼Œå°†æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ`Trainer` å°†è¯„ä¼° SacreBLEU æŒ‡æ ‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚\n",
    "2. å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™ `Seq2SeqTrainer`ï¼ŒåŒæ—¶ä¼ å…¥æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ã€‚\n",
    "3. è°ƒç”¨ `train()` å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_opus_books_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True for XPU\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_books[\"train\"],\n",
    "    eval_dataset=tokenized_books[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ `push_to_hub()` æ–¹æ³•å°†æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œè®©æ‰€æœ‰äººéƒ½èƒ½ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "å¦‚éœ€äº†è§£å¦‚ä½•å¾®è°ƒç¿»è¯‘æ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·å‚é˜…ç›¸åº”çš„\n",
    "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)ã€‚\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨æ–­"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†æ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨æ–­äº†ï¼\n",
    "\n",
    "å‡†å¤‡ä¸€äº›æ‚¨æƒ³è¦ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ã€‚å¯¹äº T5ï¼Œæ‚¨éœ€è¦æ ¹æ®æ‰€å¤„ç†çš„ä»»åŠ¡ä¸ºè¾“å…¥æ·»åŠ å‰ç¼€ã€‚å¯¹äºä»è‹±è¯­åˆ°æ³•è¯­çš„ç¿»è¯‘ï¼Œå‰ç¼€å¦‚ä¸‹æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶å°† `input_ids` ä½œä¸º PyTorch å¼ é‡è¿”å›ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_opus_books_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ [generate()](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationMixin.generate) æ–¹æ³•åˆ›å»ºç¿»è¯‘ç»“æœã€‚æœ‰å…³ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆå‚æ•°çš„æ›´å¤šè¯¦æƒ…ï¼Œè¯·æŸ¥é˜…[æ–‡æœ¬ç”Ÿæˆ](https://huggingface.co/docs/transformers/main/zh/tasks/../main_classes/text_generation) APIã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_opus_books_model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†ç”Ÿæˆçš„è¯å…ƒ id è§£ç å›æ–‡æœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Les lignÃ©es partagent des ressources avec des bactÃ©ries enfixant l'azote.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
