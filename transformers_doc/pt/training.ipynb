{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning de um modelo pr√©-treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O uso de um modelo pr√©-treinado tem importantes vantagens. Redu√ß√£o do custo computacional, a pegada de carbono, e te\n",
    "permite utilizar modelos de √∫ltima gera√ß√£o sem ter que treinar um novo desde o in√≠cio.\n",
    "O ü§ó Transformers proporciona acesso a milhares de modelos pr√©-treinados numa ampla gama de tarefas.\n",
    "Quando utilizar um modelo pr√©-treinado, treine-o com um dataset espec√≠fico para a sua tarefa.\n",
    "Isto √© chamado de fine-tuning, uma t√©cnica de treinamento incrivelmente poderosa. Neste tutorial faremos o fine-tuning\n",
    "de um modelo pr√©-treinado com um framework de Deep Learning da sua escolha:\n",
    "\n",
    "* Fine-tuning de um modelo pr√©-treinado com o ü§ó Transformers `Trainer`.\n",
    "* Fine-tuning de um modelo pr√©-treinado no TensorFlow com o Keras.\n",
    "* Fine-tuning de um modelo pr√©-treinado em PyTorch nativo.\n",
    "\n",
    "<a id='data-processing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando um dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de aplicar o fine-tuning a um modelo pr√©-treinado, baixe um dataset e prepare-o para o treinamento.\n",
    "O tutorial anterior ensinar√° a processar os dados para o treinamento, e ent√£o poder√° ter a oportunidade de testar\n",
    "esse novo conhecimento em algo pr√°tico.\n",
    "\n",
    "Comece carregando o dataset [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como j√° sabe, √© necess√°rio ter um tokenizador para processar o texto e incluir uma estrat√©gia de padding e truncamento,\n",
    "para manejar qualquer tamanho var√≠avel de sequ√™ncia. Para processar o seu dataset em apenas um passo, utilize o m√©todo de\n",
    "ü§ó Datasets [`map`](https://huggingface.co/docs/datasets/process#map) para aplicar uma fun√ß√£o de preprocessamento sobre\n",
    "todo o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desejar, √© poss√≠vel criar um subconjunto menor do dataset completo para aplicar o fine-tuning e assim reduzir o tempo necess√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trainer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning com o `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nvBXf7s7vTI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nvBXf7s7vTI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ü§ó Transformers proporciona uma classe `Trainer` otimizada para o treinamento de modelos de ü§ó Transformers,\n",
    "facilitando os primeiros passos do treinamento sem a necessidade de escrever manualmente o seu pr√≥prio ciclo.\n",
    "A API do `Trainer` suporta um grande conjunto de op√ß√µes de treinamento e funcionalidades, como o logging,\n",
    "o gradient accumulation e o mixed precision.\n",
    "\n",
    "Comece carregando seu modelo e especifique o n√∫mero de labels de previs√£o.\n",
    "A partir do [Card Dataset](https://huggingface.co/datasets/yelp_review_full#data-fields) do Yelp Reveiw, que ja\n",
    "sabemos ter 5 labels usamos o seguinte c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "    Voc√™ ver√° um alerta sobre alguns pesos pr√©-treinados que n√£o est√£o sendo utilizados e que alguns pesos est√£o\n",
    "    sendo inicializados aleatoriamente. N√£o se preocupe, essa mensagem √© completamente normal.\n",
    "    O header/cabe√ß√°rio pr√©-treinado do modelo BERT √© descartado e substitui-se por um header de classifica√ß√£o\n",
    "    inicializado aleatoriamente. Assim, pode aplicar o fine-tuning a este novo header do modelo em sua tarefa\n",
    "    de classifica√ß√£o de sequ√™ncias fazendo um transfer learning do modelo pr√©-treinado.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperpar√¢metros de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, crie uma classe `TrainingArguments` que contenha todos os hiperpar√¢metros que possam ser ajustados, assim\n",
    "como os indicadores para ativar as diferentes op√ß√µes de treinamento. Para este tutorial, voc√™ pode come√ßar o treinamento\n",
    "usando os [hiperpar√°metros](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) padr√£o,\n",
    "por√©m, sinta-se livre para experimentar com eles e encontrar uma configura√ß√£o √≥tima.\n",
    "\n",
    "Especifique onde salvar os checkpoints do treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√©tricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `Trainer` n√£o avalia automaticamente o rendimento do modelo durante o treinamento. Ser√° necess√°rio passar ao\n",
    "`Trainer` uma fun√ß√£o para calcular e fazer um diagn√≥stico sobre as m√©tricas. A biblioteca ü§ó Datasets proporciona\n",
    "uma fun√ß√£o de [`accuracy`](https://huggingface.co/metrics/accuracy) simples que pode ser carregada com a fun√ß√£o\n",
    "`load_metric` (ver este [tutorial](https://huggingface.co/docs/datasets/metrics) para mais informa√ß√µes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina a fun√ß√£o `compute` dentro de `metric` para calcular a precis√£o das suas predi√ß√µes.\n",
    "Antes de passar as suas predi√ß√µes ao `compute`, √© necess√°rio converter as predi√ß√µes √† logits (lembre-se que\n",
    "todos os modelos de ü§ó Transformers retornam logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quiser controlar as suas m√©tricas de avalia√ß√£o durante o fine-tuning, especifique o par√¢metro `eval_strategy`\n",
    "nos seus argumentos de treinamento para que o modelo considere a m√©trica de avalia√ß√£o ao final de cada √©poca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um objeto `Trainer` com o seu modelo, argumentos de treinamento, conjuntos de dados de treinamento e de teste, e a sua fun√ß√£o de avalia√ß√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, aplique o fine-tuning a seu modelo chamado `train()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='keras'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning com Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rnTGBy2ax1c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rnTGBy2ax1c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos de ü§ó Transformers tamb√©m permitem realizar o treinamento com o TensorFlow com a API do Keras.\n",
    "Contudo, ser√° necess√°rio fazer algumas mudan√ßas antes de realizar o fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convers√£o do dataset ao formato do TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `DefaultDataCollator` junta os tensores em um batch para que o modelo possa ser treinado em cima deles.\n",
    "Assegure-se de especificar os `return_tensors` para retornar os tensores do TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "    O `Trainer` utiliza `DataCollatorWithPadding` por padr√£o, ent√£o voc√™ n√£o precisa especificar explicitamente um\n",
    "    colador de dados (data collator).\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Em seguida, converta os datasets tokenizados em datasets do TensorFlow com o m√©todo\n",
    "[`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset).\n",
    "Especifique suas entradas em `columns` e seu r√≥tulo em `label_cols`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = small_train_dataset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=\"labels\",\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = small_eval_dataset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=\"labels\",\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compila√ß√£o e ajustes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregue um modelo do TensorFlow com o n√∫mero esperado de r√≥tulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, compile e ajuste o fine-tuning a seu modelo com [`fit`](https://keras.io/api/models/model_training_apis/) como\n",
    "faria com qualquer outro modelo do Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pytorch_native'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune em PyTorch nativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh9CL8fyG80?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh9CL8fyG80?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `Trainer` se encarrega do ciclo de treinamento e permite aplicar o fine-tuning a um modelo em uma linha de c√≥digo apenas.\n",
    "Para os usu√°rios que preferirem escrever o seu pr√≥prio ciclo de treinamento, tamb√©m √© poss√≠vel aplicar o fine-tuning a um\n",
    "modelo de ü§ó Transformers em PyTorch nativo.\n",
    "\n",
    "Neste momento, talvez ocorra a necessidade de reinicar seu notebook ou executar a seguinte linha de c√≥digo para liberar\n",
    "mem√≥ria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del pytorch_model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em sequ√™ncia, faremos um post-processing manual do `tokenized_dataset` e assim prepar√°-lo para o treinamento.\n",
    "\n",
    "1. Apague a coluna de `text` porque o modelo n√£o aceita texto cru como entrada:\n",
    "\n",
    "    ```py\n",
    "    >>> tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "    ```\n",
    "\n",
    "2. Troque o nome da coluna `label` para `labels`, pois o modelo espera um argumento de mesmo nome:\n",
    "\n",
    "    ```py\n",
    "    >>> tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "    ```\n",
    "\n",
    "3. Defina o formato do dataset para retornar tensores do PyTorch no lugar de listas:\n",
    "\n",
    "    ```py\n",
    "    >>> tokenized_datasets.set_format(\"torch\")\n",
    "    ```\n",
    "\n",
    "Em sequ√™ncia, crie um subconjunto menor do dataset, como foi mostrado anteriormente, para aceler√°-lo o fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um `DataLoader` para os seus datasets de treinamento e de teste para poder iterar sobre batches de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregue seu modelo com o n√∫mero de labels esperados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimiza√ß√£o e configura√ß√£o do Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um otimizador e um learning rate para aplicar o fine-tuning ao modelo.\n",
    "Iremos utilizar o otimizador [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) do PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina o learning rate do `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo, especifique o `device` do ambiente para utilizar uma GPU se tiver acesso √† alguma. Caso contr√°rio, o treinamento\n",
    "em uma CPU pode acabar levando v√°rias horas em vez de minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "    Se necess√°rio, voc√™ pode obter o acesso gratuito a uma GPU na n√∫vem por meio de um notebook no\n",
    "    [Colaboratory](https://colab.research.google.com/) ou [SageMaker StudioLab](https://studiolab.sagemaker.aws/)\n",
    "    se n√£o tiver esse recurso de forma local.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Perfeito, agora estamos prontos para come√ßar o treinamento! ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar melhor o processo de treinamento, utilize a biblioteca [tqdm](https://tqdm.github.io/) para adicionar\n",
    "uma barra de progresso sobre o n√∫mero de passos percorridos no treinamento atual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√©tricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma que √© necess√°rio adicionar uma fun√ß√£o de avalia√ß√£o ao `Trainer`, √© necess√°rio fazer o mesmo quando\n",
    "escrevendo o pr√≥prio ciclo de treinamento. Contudo, em vez de calcular e retornar a m√©trica final de cada √©poca,\n",
    "voc√™ dever√° adicionar todos os batches com [`add_batch`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=add_batch#datasets.Metric.add_batch)\n",
    "e calcular a m√©trica apenas no final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos adicionais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mais exemplos de fine-tuning acesse:\n",
    "\n",
    "- [ü§ó Transformers Examples](https://github.com/huggingface/transformers/tree/main/examples) inclui scripts\n",
    "para treinas tarefas comuns de NLP em PyTorch e TensorFlow.\n",
    "\n",
    "- [ü§ó Transformers Notebooks](https://huggingface.co/docs/transformers/main/pt/notebooks) cont√©m v√°rios notebooks sobre como aplicar o fine-tuning a um modelo\n",
    "para tarefas espec√≠ficas no PyTorch e TensorFlow."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
