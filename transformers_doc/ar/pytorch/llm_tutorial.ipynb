{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ØªØ¹Ø¯ LLMsØŒ Ø£Ùˆ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø©ØŒ Ø§Ù„Ù…ÙƒÙˆÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ÙˆØ±Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ. ÙˆØ¨Ø§Ø®ØªØµØ§Ø±ØŒ ØªØªÙƒÙˆÙ† Ù…Ù† Ù†Ù…Ø§Ø°Ø¬ Ù…Ø­ÙˆÙ„ ÙƒØ¨ÙŠØ±Ø© Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© (Ø£ÙˆØŒ Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©ØŒ Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ù„ØºÙˆÙŠ) Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ù†Øµ Ù…Ø¹ÙŠÙ†. Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù†Ù‡Ø§ ØªØªÙ†Ø¨Ø£ Ø¨Ø±Ù…Ø² ÙˆØ§Ø­Ø¯ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©ØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø´ÙŠØ¡ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ù‹Ø§ Ù„ØªÙˆÙ„ÙŠØ¯ Ø¬Ù…Ù„ Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø®Ù„Ø§Ù Ù…Ø¬Ø±Ø¯ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ - ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ.\n",
    "\n",
    "Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù‡Ùˆ Ø¥Ø¬Ø±Ø§Ø¡ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠØªØ¶Ù…Ù† Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ Ù…ØªÙƒØ±Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø®Ø±Ø¬Ø§ØªÙ‡ Ø§Ù„Ø®Ø§ØµØ©ØŒ Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©. ÙÙŠ ğŸ¤— TransformersØŒ ÙŠØªÙ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù‡Ø°Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¯Ø§Ù„Ø© `generate()`ØŒ ÙˆØ§Ù„ØªÙŠ ØªØªÙˆÙØ± Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø°Ø§Øª Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠØ©.\n",
    "\n",
    "Ø³ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ ÙƒÙŠÙÙŠØ©:\n",
    "\n",
    "* ØªØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (LLM)\n",
    "* ØªØ¬Ù†Ø¨ Ø§Ù„ÙˆÙ‚ÙˆØ¹ ÙÙŠ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©\n",
    "* Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ Ù…Ù† LLM Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ\n",
    "\n",
    "Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:\n",
    "\n",
    "```bash\n",
    "pip install transformers bitsandbytes>=0.39.0 -q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙŠØ£Ø®Ø° Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù„Ù€ [Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©](https://huggingface.co/docs/transformers/main/ar/tasks/language_modeling) ÙŠØ£Ø®Ø° ØªØ³Ù„Ø³Ù„Ù‹Ø§ Ù…Ù† Ø±Ù…ÙˆØ² Ù†ØµÙŠØ© ÙƒÙ…Ø¯Ø®Ù„ ÙˆÙŠØ¹ÙŠØ¯ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù„Ù„Ø±Ù…Ø² Ø§Ù„ØªØ§Ù„ÙŠ.\n",
    "\n",
    "<!-- [GIF 1 -- FWD PASS] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© (LLM)\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "Ù‡Ù†Ø§Ùƒ Ø¬Ø§Ù†Ø¨ Ø¨Ø§Ù„Øº Ø§Ù„Ø£Ù‡Ù…ÙŠØ© ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLMs ÙˆÙ‡Ùˆ ÙƒÙŠÙÙŠØ© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø±Ù…Ø² Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù‡Ø°Ø§. ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø·ÙˆØ© Ø·Ø§Ù„Ù…Ø§ Ø£Ù†Ùƒ ØªÙ†ØªÙ‡ÙŠ Ø¨Ø±Ù…Ø² Ù„Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØ§Ù„ÙŠ. ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨Ø³ÙŠØ·Ù‹Ø§ Ù…Ø«Ù„ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø­ØªÙ…Ø§Ù„Ù‹Ø§ Ù…Ù† ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø£Ùˆ Ù…Ø¹Ù‚Ø¯Ù‹Ø§ Ù…Ø«Ù„ ØªØ·Ø¨ÙŠÙ‚ Ø¹Ø´Ø±Ø§Øª Ø§Ù„ØªØ­ÙˆÙ„Ø§Øª Ù‚Ø¨Ù„ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†Ø§ØªØ¬.\n",
    "\n",
    "<!-- [GIF 2 -- TEXT GENERATION] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "ØªØªÙƒØ±Ø± Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù…ÙˆØ¶Ø­Ø© Ø£Ø¹Ù„Ø§Ù‡ Ø¨Ø´ÙƒÙ„ ØªÙƒØ±Ø§Ø±ÙŠ Ø­ØªÙ‰ ÙŠØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø´Ø±Ø· Ø§Ù„ØªÙˆÙ‚Ù. ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØŒ ÙŠØ­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø´Ø±Ø· Ø§Ù„ØªÙˆÙ‚ÙØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªØ¹Ù„Ù… Ø¹Ù†Ø¯ Ø¥Ø®Ø±Ø§Ø¬ Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ (`EOS`). Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø£Ù…Ø± ÙƒØ°Ù„ÙƒØŒ ÙŠØªÙˆÙ‚Ù Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¹Ù†Ø¯ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø·ÙˆÙ„ Ø£Ù‚ØµÙ‰ Ù…Ø­Ø¯Ø¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§.\n",
    "\n",
    "Ù…Ù† Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø·ÙˆØ© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø±Ù…Ø² ÙˆØ´Ø±Ø· Ø§Ù„ØªÙˆÙ‚Ù Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù„Ø¬Ø¹Ù„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙŠØªØµØ±Ù ÙƒÙ…Ø§ ØªØªÙˆÙ‚Ø¹ ÙÙŠ Ù…Ù‡Ù…ØªÙƒ. ÙˆÙ„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¨Ø¨ Ù„Ø¯ÙŠÙ†Ø§ `GenerationConfig` Ù…Ù„Ù Ù…Ø±ØªØ¨Ø· Ø¨ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„Ù…Ø© ØªÙˆÙ„ÙŠØ¯ÙŠØ© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¬ÙŠØ¯Ø© ÙˆÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ.\n",
    "\n",
    "Ø¯Ø¹Ù†Ø§ Ù†ØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„ÙƒÙˆØ¯!\n",
    "\n",
    "\n",
    "<Tip>\n",
    "\n",
    "Ø¥Ø°Ø§ ÙƒÙ†Øª Ù…Ù‡ØªÙ…Ù‹Ø§ Ø¨Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù€ LLMØŒ ÙØ¥Ù† ÙˆØ§Ø¬Ù‡Ø© [`Pipeline`](https://huggingface.co/docs/transformers/main/ar/pipeline_tutorial) Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ù‡ÙŠ Ù†Ù‚Ø·Ø© Ø§Ù†Ø·Ù„Ø§Ù‚ Ø±Ø§Ø¦Ø¹Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ØªØªØ·Ù„Ø¨ LLMs Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø«Ù„ Ø§Ù„ØªÙƒÙ…ÙŠÙ… ÙˆØ§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙÙŠ Ø®Ø·ÙˆØ© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø±Ù…Ø²ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠØªÙ… ØªÙ†ÙÙŠØ°Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ù…Ù† Ø®Ù„Ø§Ù„ `generate()`. Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLMs  ÙŠØ³ØªÙ‡Ù„Ùƒ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø±Ø¯Ø¯ ÙˆÙŠØ¬Ø¨ ØªÙ†ÙÙŠØ°Ù‡ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ ÙƒØ§ÙÙ.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Ø£ÙˆÙ„Ø§Ù‹ØŒ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø³ØªÙ„Ø§Ø­Ø¸ ÙˆØ¬ÙˆØ¯ Ù…Ø¹Ø§Ù…Ù„ÙŠÙ† ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ `from_pretrained`:\n",
    "\n",
    " - `device_map` ÙŠØ¶Ù…Ù† Ø§Ù†ØªÙ‚Ø§Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ\n",
    " - `load_in_4bit` ÙŠØ·Ø¨Ù‚ [4-bit dynamic quantization](https://huggingface.co/docs/transformers/main/ar/main_classes/quantization) Ù„Ø®ÙØ¶ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±\n",
    "\n",
    "Ù‡Ù†Ø§Ùƒ Ø·Ø±Ù‚ Ø£Ø®Ø±Ù‰ Ù„ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆÙ„ÙƒÙ† Ù‡Ø°Ø§ Ø®Ø· Ø£Ø³Ø§Ø³ Ø¬ÙŠØ¯ Ù„Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM.\n",
    "\n",
    "Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Ù…ÙØ¬Ø²Ù‘Ø¦ Ø§Ù„Ù„ØºÙˆÙŠ](https://huggingface.co/docs/transformers/main/ar/tokenizer_summary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙŠØ­ØªÙˆÙŠ Ù…ØªØºÙŠØ± `model_inputs` Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„ Ø¨Ø¹Ø¯ ØªÙ‚Ø³ÙŠÙ…Ù‡ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù„ØºÙˆÙŠØ© (tokens)ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡. ÙÙŠ Ø­ÙŠÙ† Ø£Ù† `generate()` ØªØ¨Ø°Ù„ Ù‚ØµØ§Ø±Ù‰ Ø¬Ù‡Ø¯Ù‡Ø§ Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ÙŠØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡ØŒ Ù†ÙˆØµÙŠ Ø¨ØªÙ…Ø±ÙŠØ±Ù‡ ÙƒÙ„Ù…Ø§ Ø£Ù…ÙƒÙ† Ø°Ù„Ùƒ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ø«Ø§Ù„ÙŠØ©.\n",
    "\n",
    "Ø¨Ø¹Ø¯ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù„ØºÙˆÙŠØ©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© `generate()` Ù„Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ù†Ø§ØªØ¬Ø©. ÙŠØ¬Ø¨ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ø¥Ù„Ù‰ Ù†Øµ Ù‚Ø¨Ù„ Ø·Ø¨Ø§Ø¹ØªÙ‡."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A list of colors: red, blue, green, yellow, orange, purple, pink,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ù„ÙŠØ³ Ø¹Ù„ÙŠÙƒ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ§Øª Ø§Ù„ÙˆØ§Ø­Ø¯Ø© ØªÙ„Ùˆ Ø§Ù„Ø£Ø®Ø±Ù‰! ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ø¨ØªÙƒÙ„ÙØ© ØµØºÙŠØ±Ø© ÙÙŠ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙˆØ§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±. ÙƒÙ„ Ù…Ø§ Ø¹Ù„ÙŠÙƒ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù†Ù‡ Ù‡Ùˆ  ØªØ¹Ø¨Ø¦Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ (Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ Ø°Ù„Ùƒ Ø£Ø¯Ù†Ø§Ù‡)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A list of colors: red, blue, green, yellow, orange, purple, pink,',\n",
       "'Portugal is a country in southwestern Europe, on the Iber']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙˆÙ‡Ø°Ø§ ÙƒÙ„ Ø´ÙŠØ¡! ÙÙŠ Ø¨Ø¶Ø¹ Ø³Ø·ÙˆØ± Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø®ÙŠØ± Ù‚ÙˆØ© LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† [Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯](https://huggingface.co/docs/transformers/main/ar/generation_strategies)ØŒ ÙˆÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø­ÙŠØ§Ù† Ù‚Ø¯ Ù„Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø­Ø§Ù„ØªÙƒ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù…Ø§ ØªØªÙˆÙ‚Ø¹Ù‡ØŒ ÙÙ‚Ø¯ Ù‚Ù…Ù†Ø§ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£ÙƒØ«Ø± Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙˆÙƒÙŠÙÙŠØ© ØªØ¬Ù†Ø¨Ù‡Ø§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§/Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ù‹Ø§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±Ù…ÙˆØ² ÙÙŠ `GenerationConfig` Ø§Ù„Ù…Ù„ÙØŒ `generate` ÙŠØ¹ÙŠØ¯ Ù…Ø§ ÙŠØµÙ„ Ø¥Ù„Ù‰ 20 Ø±Ù…Ø²Ù‹Ø§ Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ. Ù†ÙˆØµÙŠ Ø¨Ø´Ø¯Ø© Ø¨ØªØ¹ÙŠÙŠÙ† `max_new_tokens` ÙŠØ¯ÙˆÙŠÙ‹Ø§ ÙÙŠ Ù…ÙƒØ§Ù„Ù…Ø© `generate` Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ¹ÙŠØ¯Ù‡Ø§. Ø¶Ø¹ ÙÙŠ Ø§Ø¹ØªØ¨Ø§Ø±Ùƒ Ø£Ù† LLMs (Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©ØŒ [Ù†Ù…Ø§Ø°Ø¬ ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± ÙÙ‚Ø·](https://huggingface.co/learn/nlp-course/chapter1/6ØŸfw=pt)) ØªØ¹ÙŠØ¯ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙƒØ¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Ø§ØªØ¬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# By default, the output will contain up to 20 tokens\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting `max_new_tokens` allows you to control the maximum length\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÙˆØ¶Ø¹ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ ÙˆÙ…Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡ ÙÙŠ `GenerationConfig` Ø§Ù„Ù…Ù„ÙØŒ `generate` ÙŠØ­Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø­ØªÙ…Ø§Ù„Ù‹Ø§ ÙÙ‰ ÙƒÙ„ Ø®Ø·ÙˆØ© Ù…Ù† Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (ÙˆÙ‡Ø°Ø§ ÙŠÙØ¹Ø±Ù Ø¨Ø§Ù„ØªØ´ÙÙŠØ± Ø§Ù„Ø¬Ø´Ø¹). Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ù…Ù‡Ù…ØªÙƒØŒ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ ØºÙŠØ± Ù…Ø±ØºÙˆØ¨ ÙÙŠÙ‡Ø› ØªØ³ØªÙÙŠØ¯ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ© Ù…Ø«Ù„ Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø£Ùˆ ÙƒØªØ§Ø¨Ø© Ù…Ù‚Ø§Ù„ Ø³ØªÙÙŠØ¯ Ù…Ù† Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¹ÙŠÙ†Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙÙŠ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§ØªØŒ ØªÙ…Ù† Ù†Ø§Ø­ÙŠØ© Ø£Ø®Ø±Ù‰ØŒ ÙØ¥Ù† Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù…Ø¯Ø®Ù„Ø§Øª Ù…Ø­Ø¯Ø¯Ø©  Ù…Ø«Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ Ø£Ùˆ Ø§Ù„ØªØ±Ø¬Ù… Ù…Ù† ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± Ø§Ù„Ø¬Ø´Ø¹. Ù‚Ù… Ø¨ØªÙØ¹ÙŠÙ„ Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `do_sample=True`ØŒ ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ [ØªØ¯ÙˆÙŠÙ†Ø© Ø§Ù„Ù…Ø¯ÙˆÙ†Ø©](https://huggingface.co/blog/how-to-generate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat. I am a cat. I am a cat. I am a cat'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed or reproducibility -- you don't need this unless you want full reproducibility\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# LLM + greedy decoding = repetitive, boring output\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat.  Specifically, I am an indoor-only cat.  I'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With sampling, the output becomes more creative!\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ù…Ø´ÙƒÙ„Ø© Ø­Ø´Ùˆ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙÙ‰ Ø§Ù„Ø§ØªØ¬Ø§Ø© Ø§Ù„Ø®Ø·Ø£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs Ù‡ÙŠ [Ù…Ø¹Ù…Ø§Ø±ÙŠØ§Øª ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± ÙÙ‚Ø·](https://huggingface.co/learn/nlp-course/chapter1/6ØŸfw=pt)ØŒ Ù…Ù…Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡Ø§ ØªØ³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¹Ù„Ù‰ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ. ÙØ¥Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¨Ù†ÙØ³ Ø§Ù„Ø·ÙˆÙ„. Ù„Ø­Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø©ØŒ ÙŠØªÙ… Ø¥Ø¶Ø§ÙØ© Ø±Ù…ÙˆØ² Ø­Ø´Ùˆ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø£Ù‚ØµØ±. Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† LLMs  Ù„Ø§ ØªÙˆÙ„ÙŠ Ø§Ù‡ØªÙ…Ø§Ù…Ù‹Ø§ Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø­Ø´Ùˆ Ù‡Ø°Ù‡ØŒ Ø°Ù„ÙƒØŒ ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù… Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ±ÙƒØ² Ø¹Ù„ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆÙ‡Ø°Ø§ ÙŠØªÙ… Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ù…Ø§ ÙŠØ³Ù…Ù‰ Ø¨Ù€ \"Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡\". ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø­Ø´Ùˆ ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ø¯Ø®Ù„ (Ø§Ù„Ø­Ø´Ùˆ Ù…Ù† Ø§Ù„ÙŠØ³Ø§Ø±)ØŒ ÙˆÙ„ÙŠØ³ ÙÙŠ Ù†Ù‡Ø§ÙŠØªÙ‡."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 33333333333'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer initialized above has right-padding active by default: the 1st sequence,\n",
    "# which is shorter, has padding on the right side. Generation fails to capture the logic.\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 3, 4, 5, 6,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With left-padding, it works as expected!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ù…ÙˆØ¬Ù‡ ØºÙŠØ± ØµØ­ÙŠØ­"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ØªØªÙˆÙ‚Ø¹ Ø¨Ø¹Ø¶ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ ØµÙŠØºØ© Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª  Ù„Ù„Ø¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­. Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§ØªØ¨Ø§Ø¹ Ù‡Ø°Ù‡ Ø§Ù„ØµÙŠØºØ©ØŒ ÙØ¥Ù† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØªØ£Ø«Ø± Ø³Ù„Ø¨Ù‹Ø§: Ù„ÙƒÙ† Ù‡Ø°Ø§ Ø§Ù„ØªØ¯Ù‡ÙˆØ± Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† ÙˆØ§Ø¶Ø­Ù‹Ø§ Ù„Ù„Ø¹ÙŠØ§Ù†. ØªØªÙˆÙØ± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø­ÙˆÙ„ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙˆØ®ÙŠ Ø§Ù„Ø­Ø°Ø±ØŒ ÙÙŠ [Ø§Ù„Ø¯Ù„ÙŠÙ„](https://huggingface.co/docs/transformers/main/ar/tasks/prompting). Ø¯Ø¹Ù†Ø§ Ù†Ø±Ù‰ Ù…Ø«Ø§Ù„Ø§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù„Ù„Ø¯Ø±Ø¯Ø´Ø©ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØ³ØªØ®Ø¯Ù… [Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©](https://huggingface.co/docs/transformers/main/ar/chat_templating):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not a thug, but i can tell you that a human cannot eat\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")\n",
    "set_seed(0)\n",
    "prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.input_ids.shape[1]\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None, you thug. How bout you try to focus on more useful questions?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write\n",
    "# a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)\n",
    "\n",
    "set_seed(0)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.shape[1]\n",
    "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, it followed a proper thug style ğŸ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ù…ÙˆØ§Ø±Ø¯ Ø¥Ø¶Ø§ÙÙŠØ©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙÙŠ Ø­ÙŠÙ† Ø£Ù† Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø³ÙŠØ·Ø© Ù†Ø³Ø¨ÙŠÙ‹Ø§ØŒ ÙØ¥Ù† Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ Ù…Ù† LLM Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ù…Ù‡Ù…Ø© ØµØ¹Ø¨Ø© Ù„Ø£Ù† Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©. Ù„Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„ØºÙˆØµ Ø¨Ø´ÙƒÙ„ Ø£Ø¹Ù…Ù‚ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM ÙˆÙÙ‡Ù…Ù‡:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªÙˆÙ„ÙŠØ¯ ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ø¯Ù„ÙŠÙ„ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© [Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø·Ø±Ù‚ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©](https://huggingface.co/docs/transformers/main/ar/generation_strategies)ØŒ ÙˆÙƒÙŠÙÙŠØ© Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ ÙˆÙƒÙŠÙÙŠØ© Ø¨Ø« Ø§Ù„Ù†Ø§ØªØ¬Ø›\n",
    "2. [ØªØ³Ø±ÙŠØ¹ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ](https://huggingface.co/docs/transformers/main/ar/llm_optims)Ø›\n",
    "3.[Ù‚ÙˆØ§Ù„Ø¨ Ù…ÙˆØ¬Ù‡Ø§Øª Ù„Ù„Ø¯Ø±Ø¯Ø´Ø© LLMs](https://huggingface.co/docs/transformers/main/ar/chat_\n",
    "4. [Ø¯Ù„ÙŠÙ„ ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ¬Ù‡](tasks/prompting);\n",
    "5. Ù…Ø±Ø¬Ø¹ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª (API)  `GenerationConfig`, `generate()`, Ùˆ  [generate-related classes](https://huggingface.co/docs/transformers/main/ar/internal/generation_utils). ÙˆØ§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙˆÙ„ÙŠØ¯.!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ù„ÙˆØ­Ø§Øª ØµØ¯Ø§Ø±Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ù„ÙˆØ­Ø© ØµØ¯Ø§Ø±Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ø§Ù„Ù…ØµØ¯Ø± (Open LLM Leaderboard): ØªØ±ÙƒØ² Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…ÙØªÙˆØ­Ø© Ø§Ù„Ù…ØµØ¯Ø± [Ø±Ø§Ø¨Ø· Ù„ÙˆØ­Ø© Ø§Ù„ØµØ¯Ø§Ø±Ø©](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
    "2. Ù„ÙˆØ­Ø© ØµØ¯Ø§Ø±Ø© Ø£Ø¯Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ø§Ù„Ù…ØµØ¯Ø± (Open LLM-Perf Leaderboard): ØªØ±ÙƒØ² Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© [Ø±Ø§Ø¨Ø· Ù„ÙˆØ­Ø© Ø§Ù„ØµØ¯Ø§Ø±Ø©](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙˆØ§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆØ§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ø¯Ù„ÙŠÙ„ ØªØ­Ø³ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©: Ø¯Ù„ÙŠÙ„ ØªØ­Ø³ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©.\n",
    "2. Ø§Ù„ØªÙƒÙ…ÙŠÙ… (Quantization): Ø¯Ù„ÙŠÙ„ Ø­ÙˆÙ„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ù…Ø«Ù„ ØªÙ‚Ù†ÙŠØªÙŠ bitsandbytes Ùˆ autogptqØŒ ÙˆØ§Ù„ØªÙŠ ØªÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© ØªÙ‚Ù„ÙŠÙ„ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ù…ÙƒØªØ¨Ø§Øª Ù…Ø±ØªØ¨Ø·Ø©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`optimum`](https://github.com/huggingface/optimum), Ø§Ù…ØªØ¯Ø§Ø¯ Ù„Ù…ÙƒØªØ¨Ø© Transformers ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ø£Ø¬Ù‡Ø²Ø© Ù…Ø¹ÙŠÙ†Ø©.\n",
    "2. [`outlines`](https://github.com/outlines-dev/outlines), Ù…ÙƒØªØ¨Ø© Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ (Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ù„ÙØ§Øª JSON).\n",
    "3. [`SynCode`](https://github.com/uiuc-focal-lab/syncode), Ù…ÙƒØªØ¨Ø© Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø®Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ JSONØŒ SQLØŒ Python).\n",
    "4. [`text-generation-inference`](https://github.com/huggingface/text-generation-inference), Ø®Ø§Ø¯Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ù†ØªØ§Ø¬ Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©.\n",
    "5. [`text-generation-webui`](https://github.com/oobabooga/text-generation-webui), ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
