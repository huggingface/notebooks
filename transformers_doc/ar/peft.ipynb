{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ÙˆÙ‘Ù„Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ØªÙ‚Ù†ÙŠØ© \"Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø°Ùˆ Ø§Ù„ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ØªÙŠØ±ÙŠØ©\" (PEFT)](https://huggingface.co/blog/peft) ØªÙ‚ÙˆÙ… Ø¨ØªØ¬Ù…ÙŠØ¯ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙØ¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙˆØªØ¶ÙŠÙ Ø¹Ø¯Ø¯ ØµØºÙŠØ± Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ (Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª) ÙÙˆÙ‚Ù‡. ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­ÙˆÙ‘Ù„Ø§Øª Ù„ØªØ¹Ù„Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…Ù‡Ø§Ù…. ÙˆÙ‚Ø¯ Ø«Ø¨Øª Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬ ÙØ¹Ø§Ù„ Ù„Ù„ØºØ§ÙŠØ© Ù…Ù† Ø­ÙŠØ« Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ø¹ Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†ØªØ§Ø¬ Ù†ØªØ§Ø¦Ø¬ Ù‚Ù…Ù…Ø§Ø«Ù„Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¶Ø¨ÙˆØ· Ø¯Ù‚ÙŠÙ‚Ù‹Ø§ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„.\n",
    "\n",
    "Ø¹Ø§Ø¯Ø© Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PEFT Ø£ØµØºØ± Ø¨Ù…Ù‚Ø¯Ø§Ø± ÙƒØ¨ÙŠØ± Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø­Ø¬Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„ØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„ Ù…Ù† Ø§Ù„Ø³Ù‡Ù„ Ù…Ø´Ø§Ø±ÙƒØªÙ‡Ø§ ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙˆØªØ­Ù…ÙŠÙ„Ù‡Ø§.\n",
    "\n",
    "<div class=\"flex flex-col justify-center\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png\"/>\n",
    "  <figcaption class=\"text-center\">ØªØ¨Ù„Øº Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙˆÙ„ Ù„Ø·Ø±Ø§Ø² OPTForCausalLM Ø§Ù„Ù…Ø®Ø²Ù† Ø¹Ù„Ù‰ Hub Ø­ÙˆØ§Ù„ÙŠ 6 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ø­ÙˆØ§Ù„ÙŠ 700 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª.</figcaption>\n",
    "</div>\n",
    "\n",
    "Ø¥Ø°Ø§ ÙƒÙ†Øª Ù…Ù‡ØªÙ…Ù‹Ø§ Ø¨Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— PEFTØŒ ÙØ±Ø§Ø¬Ø¹ [Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚](https://huggingface.co/docs/peft/index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø§Ø¨Ø¯Ø£ Ø¨ØªØ«Ø¨ÙŠØª ğŸ¤— PEFT:\n",
    "\n",
    "```bash\n",
    "pip install peft\n",
    "```\n",
    "\n",
    "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ØªÙ…Ø§Ù…Ù‹Ø§ØŒ ÙÙ‚Ø¯ ØªÙƒÙˆÙ† Ù…Ù‡ØªÙ…Ù‹Ø§ Ø¨ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±:\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/huggingface/peft.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ù†Ù…Ø§Ø°Ø¬ PEFT Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙŠØ¯Ø¹Ù… ğŸ¤— Transformers Ø¨Ø´ÙƒÙ„Ù Ø£ØµÙ„ÙŠ Ø¨Ø¹Ø¶ Ø·Ø±Ù‚ PEFTØŒ Ù…Ù…Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙˆÙ„ Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ø­Ù„ÙŠÙ‹Ø§ Ø£Ùˆ Ø¹Ù„Ù‰ Hub ÙˆØªØ´ØºÙŠÙ„Ù‡Ø§ Ø£Ùˆ ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ø¨Ø¨Ø¶Ø¹ Ø³Ø·ÙˆØ± Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©. Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù‡ÙŠ:\n",
    "\n",
    "- [Ù…Ø­ÙˆÙ„Ø§Øª Ø§Ù„Ø±ØªØ¨Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©](https://huggingface.co/docs/peft/conceptual_guides/lora)\n",
    "- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)\n",
    "- [AdaLoRA](https://huggingface.co/papers/2303.10512)\n",
    "\n",
    "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±Ù‚ PEFT Ø§Ù„Ø£Ø®Ø±Ù‰ØŒ Ù…Ø«Ù„ ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø­Ø« Ø£Ùˆ Ø¶Ø¨Ø· Ø§Ù„Ù…Ø­Ø«ØŒ Ø£Ùˆ Ø­ÙˆÙ„ Ù…ÙƒØªØ¨Ø© ğŸ¤— PEFT Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ [Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚](https://huggingface.co/docs/peft/index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ØªØ­Ù…ÙŠÙ„ Ù…Ø­ÙˆÙ„ PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙˆÙ„ PEFT ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù…Ù† ğŸ¤— TransformersØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù…Ø³ØªÙˆØ¯Ø¹ Hub Ø£Ùˆ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„Ù `adapter_config.json` ÙˆØ£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙˆÙ‘Ù„ØŒ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ ÙÙŠ ØµÙˆØ±Ø© Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙˆÙ‘Ù„ PEFT Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙØ¦Ø© `AutoModelFor`. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙˆÙ„ PEFT Ù„Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©:\n",
    "\n",
    "1. Ø­Ø¯Ø¯ Ù…Ø¹Ø±Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬  Ù„PEFT\n",
    "2. Ù…Ø±Ø±Ù‡ Ø¥Ù„Ù‰ ÙØ¦Ø© `AutoModelForCausalLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ù…Ø­ÙˆÙ„ PEFT Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙØ¦Ø© `AutoModelFor` Ø£Ùˆ ÙØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù…Ø«Ù„ `OPTForCausalLM` Ø£Ùˆ `LlamaForCausalLM`.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ù…ÙŠÙ„ Ù…Ø­ÙˆÙ„ PEFT Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø·Ø±ÙŠÙ‚Ø© `load_adapter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø±Ø§Ø¬Ø¹ Ù‚Ø³Ù… [ÙˆØ«Ø§Ø¦Ù‚ API](#transformers.integrations.PeftAdapterMixin) Ø£Ø¯Ù†Ø§Ù‡ Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙÙŠ 8 Ø¨Øª Ø£Ùˆ 4 Ø¨Øª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø±Ø§Ø¬Ø¹ Ù‚Ø³Ù… [ÙˆØ«Ø§Ø¦Ù‚ API](#transformers.integrations.PeftAdapterMixin) Ø£Ø¯Ù†Ø§Ù‡ Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙÙŠ 8 Ø¨Øª Ø£Ùˆ 4 Ø¨Øª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙŠØ¯Ø¹Ù… ØªÙƒØ§Ù…Ù„ `bitsandbytes` Ø£Ù†ÙˆØ§Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯Ù‚Ø© 8 Ø¨Øª Ùˆ4 Ø¨ØªØŒ ÙˆØ§Ù„ØªÙŠ ØªÙƒÙˆÙ† Ù…ÙÙŠØ¯Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ù„Ø£Ù†Ù‡Ø§ ØªÙˆÙØ±  Ù…Ø³Ø§Ø­Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ø±Ø§Ø¬Ø¹ Ø¯Ù„ÙŠÙ„ ØªÙƒØ§Ù…Ù„ `bitsandbytes` [guide](https://huggingface.co/docs/transformers/main/ar/./quantization#bitsandbytes-integration) Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯). Ø£Ø¶Ù Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª`load_in_8bit` Ø£Ùˆ `load_in_4bit` Ø¥Ù„Ù‰ `from_pretrained()` ÙˆÙ‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† `device_map=\"auto\"` Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ù„Ø¯ÙŠÙƒ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø¥Ø¶Ø§ÙØ© Ù…Ø­ÙˆÙ„ Ø¬Ø¯ÙŠØ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© `~peft.PeftModel.add_adapter` Ù„Ø¥Ø¶Ø§ÙØ© Ù…Ø­ÙˆÙ‘Ù„ Ø¬Ø¯ÙŠØ¯ Ø¥Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ­ØªÙˆÙŠ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¹Ù„Ù‰ Ù…Ø­ÙˆÙ‘Ù„ Ø¢Ø®Ø± Ø·Ø§Ù„Ù…Ø§ Ø£Ù† Ø§Ù„Ù…Ø­ÙˆÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯  Ù…Ø·Ø§Ø¨Ù‚Ù‹Ø§ Ù„Ù„Ù†ÙˆØ¹ Ø§Ù„Ø­Ø§Ù„ÙŠ. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ù…Ø­ÙˆÙ„ LoRA Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø±ØªØ¨Ø· Ø¨Ù†Ù…ÙˆØ°Ø¬:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    init_lora_weights=False\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ù„Ø¥Ø¶Ø§ÙØ© Ù…Ø­ÙˆÙ„ Ø¬Ø¯ÙŠØ¯:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù‚Ù… Ø¨ØªØ¹Ù„ÙŠÙ‚ Ù…Ø­ÙˆÙ„ Ø¬Ø¯ÙŠØ¯ Ø¨Ù†ÙØ³ Ø§Ù„ØªÙƒÙˆÙŠÙ†\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… `~peft.PeftModel.set_adapter` Ù„ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ø­ÙˆÙ„ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø§Ø³ØªØ®Ø¯Ù… adapter_1\n",
    "model.set_adapter(\"adapter_1\")\n",
    "output = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n",
    "\n",
    "# Ø§Ø³ØªØ®Ø¯Ù… adapter_2\n",
    "model.set_adapter(\"adapter_2\")\n",
    "output_enabled = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ØªÙ…ÙƒÙŠÙ† ÙˆØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø¨Ù…Ø¬Ø±Ø¯ Ø¥Ø¶Ø§ÙØ© Ù…Ø­ÙˆÙ„ Ø¥Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…ÙƒÙŠÙ† Ø£Ùˆ ØªØ¹Ø·ÙŠÙ„ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø­ÙˆÙ„. Ù„ØªÙ…ÙƒÙŠÙ† ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø­ÙˆÙ„:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "adapter_model_id = \"ybelkada/opt-350m-lora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "text = \"Hello\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "peft_config = PeftConfig.from_pretrained(adapter_model_id)\n",
    "\n",
    "# Ù„Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„Ù‡ Ø¨Ø£ÙˆØ²Ø§Ù† Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\n",
    "peft_config.init_lora_weights = False\n",
    "\n",
    "model.add_adapter(peft_config)\n",
    "model.enable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ù„Ø¥ÙŠÙ‚Ø§Ù ØªØ´ØºÙŠÙ„ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø­ÙˆÙ„:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.disable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ØªØ¯Ø±ÙŠØ¨ Ù…Ø­ÙˆÙ„ PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙŠØ¯Ø¹Ù… Ù…Ø­ÙˆÙ„ PEFT ÙØ¦Ø© `Trainer` Ø¨Ø­ÙŠØ« ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¯Ø±ÙŠØ¨ Ù…Ø­ÙˆÙ„ Ù„Ø­Ø§Ù„ØªÙƒ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©. ÙÙ‡Ùˆ ÙŠØªØ·Ù„Ø¨ ÙÙ‚Ø· Ø¥Ø¶Ø§ÙØ© Ø¨Ø¶Ø¹ Ø³Ø·ÙˆØ± Ø£Ø®Ø±Ù‰ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø­ÙˆÙ„ LoRA:\n",
    "\n",
    "<Tip>\n",
    "\n",
    "Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¯Ù‚ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`ØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ](https://huggingface.co/docs/transformers/main/ar/training) Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØ¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "1. Ø­Ø¯Ø¯ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ø­ÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© (Ø±Ø§Ø¬Ø¹ `~peft.LoraConfig` Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ ÙˆØ¸ÙŠÙØ© Ù‡Ø°Ù‡  Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"ØŒ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ø£Ø¶Ù Ø§Ù„Ù…Ø­ÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ `Trainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, ...)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ù„Ø­ÙØ¸ Ù…Ø­ÙˆÙ„ Ø§Ù„Ù…Ø¯Ø±Ø¨ ÙˆØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø¥Ø¶Ø§ÙØ© Ø·Ø¨Ù‚Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ù„Ù‰ Ù…Ø­ÙˆÙ„ PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø¥Ø¶Ø§ÙØ© Ø·Ø¨Ù‚Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ù„Ù‰ Ù…Ø­ÙˆÙ„ PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ø¬Ø±Ø§Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø¯Ù‚ÙŠÙ‚ Ù„Ù…Ø­ÙˆÙ‘Ù„Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ø¶Ø§ÙÙŠØ© ÙÙˆÙ‚ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ­ØªÙˆÙŠ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¹Ù„Ù‰ Ù…Ø­ÙˆÙ‘Ù„Ø§Øª Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…Ø¹Ù„Ù… `modules_to_save` ÙÙŠ ØªÙƒÙˆÙŠÙ† PEFT Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø£ÙŠØ¶Ù‹Ø§ Ø¶Ø¨Ø· Ø¯Ù‚ÙŠÙ‚ Ù„Ø±Ø£Ø³ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ`lm_head` ÙÙˆÙ‚ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù…Ø­ÙˆÙ‘Ù„ LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    modules_to_save=[\"lm_head\"]ØŒ\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÙˆØ«Ø§Ø¦Ù‚ API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[autodoc]] integrations.PeftAdapterMixin\n",
    "    - load_adapter\n",
    "    - add_adapter\n",
    "    - set_adapter\n",
    "    - disable_adapters\n",
    "    - enable_adapters\n",
    "    - active_adapters\n",
    "    - get_adapter_state_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "TODO: (@younesbelkada @stevhliu)\n",
    "-   Link to PEFT docs for further details\n",
    "-   Trainer\n",
    "-   8-bit / 4-bit examples ?\n",
    "-->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
