{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ (Multiple choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ù…Ù‡Ù…Ø© Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆÙ„ÙƒÙ† Ù…Ø¹ ØªÙˆÙÙŠØ± Ø¹Ø¯Ø© Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø­ØªÙ…Ù„Ø© Ù…Ø¹ Ø³ÙŠØ§Ù‚ØŒ ÙˆÙŠÙØ¯Ø±Ù‘Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©.\n",
    "\n",
    "Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:\n",
    "\n",
    "1. Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ [BERT](https://huggingface.co/google-bert/bert-base-uncased)  Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ `regular` Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [SWAG](https://huggingface.co/datasets/swag) Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ÙØ¶Ù„ Ù…Ù† Ø¨ÙŠÙ† Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚.\n",
    "2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n",
    "\n",
    "Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate\n",
    "```\n",
    "\n",
    "Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø´Ø§Ø±ÙƒØªÙ‡ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©ØŒ Ø£Ø¯Ø®Ù„ Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù…ÙŠØ² Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª SWAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ ØªÙ‡ÙŠØ¦Ø© `regular` Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª SWAG Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "swag = load_dataset(\"swag\", \"regular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø«Ù… Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'fold-ind': '3416',\n",
       " 'gold-source': 'gold',\n",
       " 'label': 0,\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'video-id': 'anetv_jkn6uvmqwh4'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swag[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø§Ù„Ø­Ù‚ÙˆÙ„ ØªØ¨Ø¯Ùˆ ÙƒØ«ÙŠØ±Ø©ØŒ Ø¥Ù„Ø§ Ø£Ù†Ù‡Ø§ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø¨Ø³ÙŠØ·Ø© Ø¬Ø¯Ø§Ù‹:\n",
    "\n",
    "- `sent1` Ùˆ `sent2`: ÙŠØ¹Ø±Ø¶ Ù‡Ø°Ø§Ù† Ø§Ù„Ø­Ù‚Ù„Ø§Ù† Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©ØŒ ÙˆØ¨Ø¯Ù…Ø¬Ù‡Ù…Ø§ Ù…Ø¹Ù‹Ø§ØŒ Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø­Ù‚Ù„ `startphrase`.\n",
    "- `ending`: ÙŠÙ‚ØªØ±Ø­ Ù†Ù‡Ø§ÙŠØ© Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù„Ø¬Ù…Ù„Ø©ØŒ ÙˆØ§Ø­Ø¯Ø© Ù…Ù†Ù‡Ø§ ÙÙ‚Ø· Ù‡ÙŠ Ø§Ù„ØµØ­ÙŠØ­Ø©.\n",
    "- `label`: ÙŠØ­Ø¯Ø¯ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© (Preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…ÙØ¬Ø²Ø¦ BERT Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ø¬Ù…Ù„ ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø¨Ø¹ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ØªØ­ØªØ§Ø¬ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡Ù‡Ø§ Ø¥Ù„Ù‰:\n",
    "\n",
    "1.  Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø±Ø¨Ø¹ Ù†Ø³Ø® Ù…Ù† Ø­Ù‚Ù„ `sent1` ÙˆØ¯Ù…Ø¬ ÙƒÙ„ Ù…Ù†Ù‡Ø§ Ù…Ø¹ `sent2` Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¡ ÙƒÙŠÙÙŠØ© Ø¨Ø¯Ø¡ Ø§Ù„Ø¬Ù…Ù„Ø©.\n",
    "2. Ø¯Ù…Ø¬ `sent2` Ù…Ø¹ ÙƒÙ„ Ù…Ù† Ù†Ù‡Ø§ÙŠØ§Øª Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£Ø±Ø¨Ø¹ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©.\n",
    "3. ØªØªØ¬Ù…ÙŠØ¹ Ù‡Ø§ØªÙŠÙ† Ø§Ù„Ù‚Ø§Ø¦Ù…ØªÙŠÙ† Ù„ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ¬Ø²Ø¦ØªÙ‡Ù…Ø§ØŒ Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø¨Ø­ÙŠØ« ÙŠÙƒÙˆÙ† Ù„ÙƒÙ„ Ù…Ø«Ø§Ù„ Ø­Ù‚ÙˆÙ„ `input_ids` Ùˆ `attention_mask` Ùˆ `labels` Ù…Ù‚Ø§Ø¨Ù„Ø©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n",
    "    question_headers = examples[\"sent2\"]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© `map` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ ğŸ¤— Datasets. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ Ø¯Ø§Ù„Ø© `map` Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `batched=True` Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_swag = swag.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ù„Ø§ ÙŠØ­ØªÙˆÙŠ ğŸ¤— Transformers Ø¹Ù„Ù‰ Ù…Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ØŒ Ù„Ø°Ù„Ùƒ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙƒÙŠÙŠÙ `DataCollatorWithPadding` Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø©. Ù…Ù† Ø§Ù„Ø£ÙƒÙØ£ Ø¥Ø¶Ø§ÙØ© Ø­Ø´Ùˆ (padding) Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù„Ù„Ø¬Ù…Ù„ Ø¥Ù„Ù‰ Ø£Ø·ÙˆÙ„ Ø·ÙˆÙ„ ÙÙŠ Ø¯ÙØ¹Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø­Ø´Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„.\n",
    "\n",
    "ÙŠÙ‚ÙˆÙ… `DataCollatorForMultipleChoice` Ø¨ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆÙŠØ·Ø¨Ù‚ Ø§Ù„Ø­Ø´ÙˆØŒ Ø«Ù… ÙŠØ¹ÙŠØ¯ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø´ÙƒÙ„Ù‡Ø§ Ø§Ù„Ø£ØµÙ„ÙŠ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÙŠÙÙØ¶Ù„ ØºØ§Ù„Ø¨Ù‹Ø§ ØªØ¶Ù…ÙŠÙ† Ù…Ù‚ÙŠØ§Ø³ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚ÙŠÙŠÙ… Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index). Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ù‚ÙŠØ§Ø³ [Ø§Ù„Ø¯Ù‚Ø©](https://huggingface.co/spaces/evaluate-metric/accuracy) (Ø§Ù†Ø¸Ø± Ø¥Ù„Ù‰ [Ø§Ù„Ø¬ÙˆÙ„Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©](https://huggingface.co/docs/evaluate/a_quick_tour) Ù„Ù€ ğŸ¤— Evaluate Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ ÙˆØ­Ø³Ø§Ø¨Ù‡):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø«Ù… Ø£Ù†Ø´Ø¦ Ø¯Ø§Ù„Ø© Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª ÙˆØ§Ù„ØªØ³Ù…ÙŠØ§Øª Ø¥Ù„Ù‰ `compute` Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø¯Ø§Ù„ØªÙƒ `compute_metrics` Ø¬Ø§Ù‡Ø²Ø© Ø§Ù„Ø¢Ù†ØŒ ÙˆØ³ØªØ¹ÙˆØ¯ Ø¥Ù„ÙŠÙ‡Ø§ Ø¹Ù†Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ¯Ø±ÙŠØ¨Ùƒ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± ØªØ¹Ù…Ù‚Ù‹Ø§ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ØŒ Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ [Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\n",
    "Ø£Ùˆ [Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb) Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„  (Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ù‚Ù…Øª Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„!\n",
    "\n",
    "Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ ÙˆØ§Ù‚ØªØ±Ø§Ø­ Ø¥Ø¬Ø§Ø¨ØªÙŠÙ† Ù…Ø­ØªÙ…Ù„ØªÙŠÙ†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"France has a bread law, Le DÃ©cret Pain, with strict rules on what is allowed in a traditional baguette.\"\n",
    "candidate1 = \"The law does not apply to croissants and brioche.\"\n",
    "candidate2 = \"The law applies to baguettes.\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
