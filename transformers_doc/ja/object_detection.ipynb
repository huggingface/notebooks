{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã¯ã€ç”»åƒå†…ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ (äººé–“ã€å»ºç‰©ã€è»Šãªã©) ã‚’æ¤œå‡ºã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ ãƒ“ã‚¸ãƒ§ãƒ³ ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã¯ç”»åƒã‚’å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã¾ã™\n",
    "æ¤œå‡ºã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã¨é–¢é€£ã™ã‚‹ãƒ©ãƒ™ãƒ«ã®åº§æ¨™ã€‚ç”»åƒã«ã¯è¤‡æ•°ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å«ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "ãã‚Œãã‚Œã«ç‹¬è‡ªã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã¨ãƒ©ãƒ™ãƒ«ãŒã‚ã‚Š (ä¾‹: è»Šã¨å»ºç‰©ã‚’æŒã¤ã“ã¨ãŒã§ãã¾ã™)ã€å„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯\n",
    "ç”»åƒã®ã•ã¾ã–ã¾ãªéƒ¨åˆ†ã«å­˜åœ¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ (ãŸã¨ãˆã°ã€ç”»åƒã«ã¯è¤‡æ•°ã®è»ŠãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)ã€‚\n",
    "ã“ã®ã‚¿ã‚¹ã‚¯ã¯ã€æ­©è¡Œè€…ã€é“è·¯æ¨™è­˜ã€ä¿¡å·æ©Ÿãªã©ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã«è‡ªå‹•é‹è»¢ã§ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n",
    "ä»–ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ã€ç”»åƒå†…ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚«ã‚¦ãƒ³ãƒˆã€ç”»åƒæ¤œç´¢ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "\n",
    " 1. Finetune [DETR](https://huggingface.co/docs/transformers/model_doc/detr)ã€ç•³ã¿è¾¼ã¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ¢ãƒ‡ãƒ«\n",
    " [CPPE-5](https://huggingface.co/datasets/cppe-5) ä¸Šã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼/ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’å‚™ãˆãŸãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³\n",
    " ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚\n",
    " 2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/object-detection) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install -q datasets transformers evaluate timm albumentations\n",
    "```\n",
    "\n",
    "ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ Hugging Face Hub ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ğŸ¤— ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚’å¢—å¼·ã™ã‚‹ãŸã‚ã®`albumentations`ã€‚ `timm` ã¯ç¾åœ¨ã€DETR ãƒ¢ãƒ‡ãƒ«ã®ç•³ã¿è¾¼ã¿ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«å¿…è¦ã§ã™ã€‚\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã€ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n",
    "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CPPE-5 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CPPE-5 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://huggingface.co/datasets/cppe-5) ã«ã¯ã€æ¬¡ã®ç”»åƒãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹æ„ŸæŸ“ç—‡ã®ãƒ‘ãƒ³ãƒ‡ãƒŸãƒƒã‚¯ã«ãŠã‘ã‚‹åŒ»ç™‚ç”¨å€‹äººä¿è­·å…· (PPE) ã‚’è­˜åˆ¥ã™ã‚‹æ³¨é‡ˆã€‚\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
       "        num_rows: 29\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cppe5 = load_dataset(\"cppe-5\")\n",
    "cppe5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€1000 æšã®ç”»åƒã‚’å«ã‚€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã¨ 29 æšã®ç”»åƒã‚’å«ã‚€ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆãŒã™ã§ã«ä»˜å±ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã«æ…£ã‚Œã‚‹ãŸã‚ã«ã€ä¾‹ãŒã©ã®ã‚ˆã†ãªã‚‚ã®ã‹ã‚’èª¿ã¹ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 15,\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7F9EC9E77C10>,\n",
       " 'width': 943,\n",
       " 'height': 663,\n",
       " 'objects': {'id': [114, 115, 116, 117],\n",
       "  'area': [3796, 1596, 152768, 81002],\n",
       "  'bbox': [[302.0, 109.0, 73.0, 52.0],\n",
       "   [810.0, 100.0, 57.0, 28.0],\n",
       "   [160.0, 31.0, 248.0, 616.0],\n",
       "   [741.0, 68.0, 202.0, 401.0]],\n",
       "  'category': [4, 4, 0, 0]}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cppe5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ä¾‹ã«ã¯æ¬¡ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "- `image_id`: ã‚µãƒ³ãƒ—ãƒ«ã®ç”»åƒID\n",
    "- `image`: ç”»åƒã‚’å«ã‚€ `PIL.Image.Image` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "- `width`: ç”»åƒã®å¹…\n",
    "- `height`: ç”»åƒã®é«˜ã•\n",
    "- `objects`: ç”»åƒå†…ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸:\n",
    "  - `id`: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ID\n",
    "  - `area`: å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã®é ˜åŸŸ\n",
    "  - `bbox`: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ ([COCO å½¢å¼](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco) )\n",
    "  - `category`: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã€‚å¯èƒ½ãªå€¤ã«ã¯ã€`Coverall (0)`ã€`Face_Shield (1)`ã€`Gloves (2)`ã€`Goggles (3)`ã€ãŠã‚ˆã³ `Mask (4)` ãŒå«ã¾ã‚Œã¾ã™ã€‚\n",
    "\n",
    "`bbox`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒ COCO å½¢å¼ã«å¾“ã£ã¦ã„ã‚‹ã“ã¨ã«æ°—ã¥ãã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã‚Œã¯ DETR ãƒ¢ãƒ‡ãƒ«ãŒäºˆæœŸã™ã‚‹å½¢å¼ã§ã™ã€‚\n",
    "ãŸã ã—ã€ã€Œã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€å†…ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã¯ã€DETR ãŒå¿…è¦ã¨ã™ã‚‹æ³¨é‡ˆå½¢å¼ã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚ã‚ãªãŸã¯ã™ã‚‹ã§ã‚ã‚ã†\n",
    "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹å‰ã«ã€ã„ãã¤ã‹ã®å‰å‡¦ç†å¤‰æ›ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚’ã•ã‚‰ã«æ·±ãç†è§£ã™ã‚‹ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ä¾‹ã‚’è¦–è¦šåŒ–ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "image = cppe5[\"train\"][0][\"image\"]\n",
    "annotations = cppe5[\"train\"][0][\"objects\"]\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "for i in range(len(annotations[\"id\"])):\n",
    "    box = annotations[\"bbox\"][i]\n",
    "    class_idx = annotations[\"category\"][i]\n",
    "    x, y, w, h = tuple(box)\n",
    "    draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n",
    "    draw.text((x, y), id2label[class_idx], fill=\"white\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://i.imgur.com/TdaqPJO.png\" alt=\"CPPE-5 Image Example\"/>\n",
    "</div>\n",
    "\n",
    "é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ã—ã¦å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚’è¦–è¦šåŒ–ã™ã‚‹ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ™ãƒ«ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
    "`category`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã€‚\n",
    "ã¾ãŸã€ãƒ©ãƒ™ãƒ« ID ã‚’ãƒ©ãƒ™ãƒ« ã‚¯ãƒ©ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹è¾æ›¸ (`id2label`) ã‚„ãã®é€† (`label2id`) ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
    "ã“ã‚Œã‚‰ã¯ã€å¾Œã§ãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ãã«ä½¿ç”¨ã§ãã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒãƒƒãƒ—ã‚’å«ã‚ã‚‹ã¨ã€å…±æœ‰ã—ãŸå ´åˆã«ä»–ã®äººãŒãƒ¢ãƒ‡ãƒ«ã‚’å†åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n",
    "ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ãƒãƒ–ã«å–ã‚Šä»˜ã‘ã¾ã™ã€‚\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã«æ…£ã‚Œã‚‹ãŸã‚ã®æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€æ½œåœ¨çš„ãªå•é¡ŒãŒãªã„ã‹ãƒ‡ãƒ¼ã‚¿ã‚’èª¿æŸ»ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é–¢ã™ã‚‹ä¸€èˆ¬çš„ãªå•é¡Œã® 1 ã¤ã¯ã€\n",
    "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã¯ã€ç”»åƒã®ç«¯ã‚’è¶Šãˆã¦ã€Œä¼¸ã³ã‚‹ã€å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã§ã™ã€‚ã“ã®ã‚ˆã†ãªã€Œæš´èµ°ã€å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã¯ã€\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã€ã“ã®æ®µéšã§å¯¾å‡¦ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€ã“ã®å•é¡Œã«é–¢ã™ã‚‹ä¾‹ãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯å†…å®¹ã‚’ã‚ã‹ã‚Šã‚„ã™ãã™ã‚‹ãŸã‚ã«ã€ã“ã‚Œã‚‰ã®ç”»åƒã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_idx = [590, 821, 822, 875, 876, 878, 879]\n",
    "keep = [i for i in range(len(cppe5[\"train\"])) if i not in remove_idx]\n",
    "cppe5[\"train\"] = cppe5[\"train\"].select(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨æ­£ç¢ºã«ä¸€è‡´ã™ã‚‹ã‚ˆã†ã«ã€ä½¿ç”¨ã™ã‚‹äºˆå®šã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "[AutoImageProcessor](https://huggingface.co/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor) ã¯ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦ `pixel_values`ã€`pixel_mask`ã€ãŠã‚ˆã³\n",
    "DETR ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ã€Œãƒ©ãƒ™ãƒ«ã€ã€‚ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã«ã¯ã€å¿ƒé…ã™ã‚‹å¿…è¦ã®ãªã„ã„ãã¤ã‹ã®å±æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "- `image_mean = [0.485, 0.456, 0.406 ]`\n",
    "- `image_std = [0.229, 0.224, 0.225]`\n",
    "\n",
    "ã“ã‚Œã‚‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ç”»åƒã‚’æ­£è¦åŒ–ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å¹³å‡ã¨æ¨™æº–åå·®ã§ã™ã€‚ã“ã‚Œã‚‰ã®ä¾¡å€¤è¦³ã¯éå¸¸ã«é‡è¦ã§ã™\n",
    "äº‹å‰ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸç”»åƒãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã¾ãŸã¯å¾®èª¿æ•´ã™ã‚‹ã¨ãã«è¤‡è£½ã—ã¾ã™ã€‚\n",
    "\n",
    "å¾®èª¿æ•´ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã‚¤ãƒ¡ãƒ¼ã‚¸ ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"facebook/detr-resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”»åƒã‚’`image_processor`ã«æ¸¡ã™å‰ã«ã€2 ã¤ã®å‰å‡¦ç†å¤‰æ›ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ç”¨ã—ã¾ã™ã€‚\n",
    "- ç”»åƒã®æ‹¡å¼µ\n",
    "- DETR ã®æœŸå¾…ã«å¿œãˆã‚‹ãŸã‚ã®æ³¨é‡ˆã®å†ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "\n",
    "ã¾ãšã€ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã—ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã«ã€ä»»æ„ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ç”»åƒæ‹¡å¼µã‚’é©ç”¨ã§ãã¾ã™ã€‚ã“ã“ã§ã¯[Albumentations](https://albumentations.ai/docs/)ã‚’ä½¿ç”¨ã—ã¾ã™...\n",
    "ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€å¤‰æ›ãŒç”»åƒã«å½±éŸ¿ã‚’ä¸ãˆã€ãã‚Œã«å¿œã˜ã¦å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚\n",
    "ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯ã€è©³ç´°ãª [ç‰©ä½“æ¤œå‡ºç”¨ã«ç”»åƒã‚’æ‹¡å¼µã™ã‚‹æ–¹æ³•ã«é–¢ã™ã‚‹ã‚¬ã‚¤ãƒ‰](https://huggingface.co/docs/datasets/object_detection) ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "ä¾‹ã¨ã—ã¦ã¾ã£ãŸãåŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ã“ã“ã§ã‚‚åŒã˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é©ç”¨ã—ã€å„ç”»åƒã®ã‚µã‚¤ã‚ºã‚’ (480, 480) ã«å¤‰æ›´ã—ã¾ã™ã€‚\n",
    "æ°´å¹³ã«åè»¢ã—ã¦æ˜ã‚‹ãã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(480, 480),\n",
    "        albumentations.HorizontalFlip(p=1.0),\n",
    "        albumentations.RandomBrightnessContrast(p=1.0),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`image_processor` ã¯ã€æ³¨é‡ˆãŒæ¬¡ã®å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¾ã™: `{'image_id': int, 'annotations': list[Dict]}`,\n",
    " ã“ã“ã§ã€å„è¾æ›¸ã¯ COCO ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ³¨é‡ˆã§ã™ã€‚ 1 ã¤ã®ä¾‹ã¨ã—ã¦ã€æ³¨é‡ˆã‚’å†ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹é–¢æ•°ã‚’è¿½åŠ ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    " ```py\n",
    ">>> def formatted_anns(image_id, category, area, bbox):\n",
    "...     annotations = []\n",
    "...     for i in range(0, len(category)):\n",
    "...         new_ann = {\n",
    "...             \"image_id\": image_id,\n",
    "...             \"category_id\": category[i],\n",
    "...             \"isCrowd\": 0,\n",
    "...             \"area\": area[i],\n",
    "...             \"bbox\": list(bbox[i]),\n",
    "...         }\n",
    "...         annotations.append(new_ann)\n",
    "\n",
    "...     return annotations\n",
    "```\n",
    "\n",
    "ã“ã‚Œã§ã€ç”»åƒã¨æ³¨é‡ˆã®å¤‰æ›ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming a batch\n",
    "def transform_aug_ann(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Datasets `with_transform` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ã“ã®å‰å‡¦ç†é–¢æ•°ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«é©ç”¨ã—ã¾ã™ã€‚ã“ã®æ–¹æ³•ãŒé©ç”¨ã•ã‚Œã‚‹ã®ã¯ã€\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¦ç´ ã‚’èª­ã¿è¾¼ã‚€ã¨ãã«ã€ãã®å ´ã§å¤‰æ›ã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®æ™‚ç‚¹ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¾‹ãŒå¤‰æ›å¾Œã«ã©ã®ã‚ˆã†ã«ãªã‚‹ã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚ãƒ†ãƒ³ã‚½ãƒ«ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™\n",
    "`pixel_values`ã€ãƒ†ãƒ³ã‚½ãƒ«ã¨ `pixel_mask`ã€ãŠã‚ˆã³ `labels` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],\n",
       "          [ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],\n",
       "          [ 0.9132,  0.9132,  0.9132,  ..., -1.9638, -1.9638, -1.9638],\n",
       "          ...,\n",
       "          [-1.5699, -1.5699, -1.5699,  ..., -1.9980, -1.9980, -1.9980],\n",
       "          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809],\n",
       "          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809]],\n",
       "\n",
       "         [[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],\n",
       "          [ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],\n",
       "          [ 1.3081,  1.3081,  1.3081,  ..., -1.8256, -1.8256, -1.8256],\n",
       "          ...,\n",
       "          [-1.3179, -1.3179, -1.3179,  ..., -1.8606, -1.8606, -1.8606],\n",
       "          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431],\n",
       "          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431]],\n",
       "\n",
       "         [[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\n",
       "          [ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\n",
       "          [ 1.4200,  1.4200,  1.4200,  ..., -1.6302, -1.6302, -1.6302],\n",
       "          ...,\n",
       "          [-1.0201, -1.0201, -1.0201,  ..., -1.5604, -1.5604, -1.5604],\n",
       "          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430],\n",
       "          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430]]]),\n",
       " 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': {'size': tensor([800, 800]), 'image_id': tensor([756]), 'class_labels': tensor([4]), 'boxes': tensor([[0.7340, 0.6986, 0.3414, 0.5944]]), 'area': tensor([519544.4375]), 'iscrowd': tensor([0]), 'orig_size': tensor([480, 480])}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cppe5[\"train\"] = cppe5[\"train\"].with_transform(transform_aug_ann)\n",
    "cppe5[\"train\"][15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å€‹ã€…ã®ç”»åƒã‚’æ­£å¸¸ã«æ‹¡å¼µã—ã€ãã‚Œã‚‰ã®æ³¨é‡ˆã‚’æº–å‚™ã—ã¾ã—ãŸã€‚ãŸã ã—ã€å‰å‡¦ç†ã¯ãã†ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
    "ã¾ã å®Œæˆã—ã¦ã„ã¾ã™ã€‚æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ç”»åƒã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ  `collatâ€‹â€‹e_fn` ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "ç”»åƒ (ç¾åœ¨ã¯ `pixel_values`) ã‚’ãƒãƒƒãƒå†…ã®æœ€å¤§ã®ç”»åƒã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã€å¯¾å¿œã™ã‚‹ `pixel_mask` ã‚’ä½œæˆã—ã¾ã™\n",
    "ã©ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒå®Ÿæ•° (1) ã§ã€ã©ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒãƒ‘ãƒ‡ã‚£ãƒ³ã‚° (0) ã§ã‚ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DETR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§é‡åŠ´åƒã®ã»ã¨ã‚“ã©ã‚’å®Œäº†ã—ãŸã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚\n",
    "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ç”»åƒã¯ã€ã‚µã‚¤ã‚ºã‚’å¤‰æ›´ã—ãŸå¾Œã§ã‚‚ä¾ç„¶ã¨ã—ã¦éå¸¸ã«å¤§ãã„ã§ã™ã€‚ã“ã‚Œã¯ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã¨ã€\n",
    "å°‘ãªãã¨ã‚‚ 1 ã¤ã® GPU ãŒå¿…è¦ã§ã™ã€‚\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯æ¬¡ã®æ‰‹é †ãŒå«ã¾ã‚Œã¾ã™ã€‚\n",
    "1. å‰å‡¦ç†ã¨åŒã˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€[AutoModelForObjectDetection](https://huggingface.co/docs/transformers/main/ja/model_doc/auto#transformers.AutoModelForObjectDetection) ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n",
    "2. [TrainingArguments](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.TrainingArguments) ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
    "3. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¨ã¨ã‚‚ã« [Trainer](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer) ã«æ¸¡ã—ã¾ã™ã€‚\n",
    "4. [train()](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.train) ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n",
    "\n",
    "å‰å‡¦ç†ã«ä½¿ç”¨ã—ãŸã®ã¨åŒã˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ãã¯ã€å¿…ãš`label2id`ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚\n",
    "ãŠã‚ˆã³ `id2label` ãƒãƒƒãƒ—ã¯ã€ä»¥å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸã‚‚ã®ã§ã™ã€‚ã•ã‚‰ã«ã€`ignore_mismatched_sizes=True`ã‚’æŒ‡å®šã—ã¦ã€æ—¢å­˜ã®åˆ†é¡é ­éƒ¨ã‚’æ–°ã—ã„åˆ†é¡é ­éƒ¨ã«ç½®ãæ›ãˆã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TrainingArguments](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.TrainingArguments) ã§ã€`output_dir` ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ§‹æˆã—ã¾ã™ã€‚\n",
    "ç”»åƒåˆ—ãŒå‰Šé™¤ã•ã‚Œã‚‹ãŸã‚ã€æœªä½¿ç”¨ã®åˆ—ã‚’å‰Šé™¤ã—ãªã„ã“ã¨ãŒé‡è¦ã§ã™ã€‚ç”»åƒåˆ—ãŒãªã„ã¨ã€\n",
    "`pixel_values` ã‚’ä½œæˆã§ãã¾ã›ã‚“ã€‚ã“ã®ãŸã‚ã€`remove_unused_columns`ã‚’`False`ã«è¨­å®šã—ã¾ã™ã€‚\n",
    "ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã—ãŸã„å ´åˆã¯ã€`push_to_hub` ã‚’ `True` ã«è¨­å®šã—ã¾ã™ (Hugging ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)\n",
    "é¡”ã«å‘ã‹ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"detr-resnet-50_finetuned_cppe5\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€å¾Œã«ã€ã™ã¹ã¦ã‚’ã¾ã¨ã‚ã¦ã€[train()](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.train) ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=cppe5[\"train\"],\n",
    "    processing_class=image_processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`training_args`ã§`push_to_hub`ã‚’`True`ã«è¨­å®šã—ãŸå ´åˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯\n",
    "ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ãƒãƒ–ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€[push_to_hub()](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.push_to_hub) ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦ã€æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚‚ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã€ä¸€é€£ã® <a href=\"https://cocodataset.org/#detection-eval\">COCO ã‚¹ã‚¿ã‚¤ãƒ«ã®æŒ‡æ¨™</a>ã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡ã•ã‚Œã¾ã™ã€‚\n",
    "æ—¢å­˜ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…ã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨ã§ãã¾ã™ãŒã€ã“ã“ã§ã¯`torchvision`ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…ã‚’ä½¿ç”¨ã—ã¦æœ€çµ‚çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚\n",
    "ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚\n",
    "\n",
    "`torchvision`ã‚¨ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ COCO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ COCO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã® API\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å®šã®å½¢å¼ã§ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€æœ€åˆã«ç”»åƒã¨æ³¨é‡ˆã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¨åŒã˜ã‚ˆã†ã«\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ã¨ãã€`cppe5[\"test\"]` ã‹ã‚‰ã®æ³¨é‡ˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãŸã ã—ã€ç”»åƒ\n",
    "ãã®ã¾ã¾ã§ã„ã‚‹ã¹ãã§ã™ã€‚\n",
    "\n",
    "è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—ã«ã¯å°‘ã—ä½œæ¥­ãŒå¿…è¦ã§ã™ãŒã€å¤§ãã 3 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "ã¾ãšã€`cppe5[\"test\"]` ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¾ã™ã€‚æ³¨é‡ˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# format annotations the same as for training, no need for data augmentation\n",
    "def val_formatted_anns(image_id, objects):\n",
    "    annotations = []\n",
    "    for i in range(0, len(objects[\"id\"])):\n",
    "        new_ann = {\n",
    "            \"id\": objects[\"id\"][i],\n",
    "            \"category_id\": objects[\"category\"][i],\n",
    "            \"iscrowd\": 0,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": objects[\"area\"][i],\n",
    "            \"bbox\": objects[\"bbox\"][i],\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "# Save images and annotations into the files torchvision.datasets.CocoDetection expects\n",
    "def save_cppe5_annotation_file_images(cppe5):\n",
    "    output_json = {}\n",
    "    path_output_cppe5 = f\"{os.getcwd()}/cppe5/\"\n",
    "\n",
    "    if not os.path.exists(path_output_cppe5):\n",
    "        os.makedirs(path_output_cppe5)\n",
    "\n",
    "    path_anno = os.path.join(path_output_cppe5, \"cppe5_ann.json\")\n",
    "    categories_json = [{\"supercategory\": \"none\", \"id\": id, \"name\": id2label[id]} for id in id2label]\n",
    "    output_json[\"images\"] = []\n",
    "    output_json[\"annotations\"] = []\n",
    "    for example in cppe5:\n",
    "        ann = val_formatted_anns(example[\"image_id\"], example[\"objects\"])\n",
    "        output_json[\"images\"].append(\n",
    "            {\n",
    "                \"id\": example[\"image_id\"],\n",
    "                \"width\": example[\"image\"].width,\n",
    "                \"height\": example[\"image\"].height,\n",
    "                \"file_name\": f\"{example['image_id']}.png\",\n",
    "            }\n",
    "        )\n",
    "        output_json[\"annotations\"].extend(ann)\n",
    "    output_json[\"categories\"] = categories_json\n",
    "\n",
    "    with open(path_anno, \"w\") as file:\n",
    "        json.dump(output_json, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    for im, img_id in zip(cppe5[\"image\"], cppe5[\"image_id\"]):\n",
    "        path_img = os.path.join(path_output_cppe5, f\"{img_id}.png\")\n",
    "        im.save(path_img)\n",
    "\n",
    "    return path_output_cppe5, path_anno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€`cocoevaluator`ã§åˆ©ç”¨ã§ãã‚‹`CocoDetection`ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨æ„ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, image_processor, ann_file):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target: converting target to DETR format,\n",
    "        # resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {\"image_id\": image_id, \"annotations\": target}\n",
    "        encoding = self.image_processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()  # remove batch dimension\n",
    "        target = encoding[\"labels\"][0]  # remove batch dimension\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": target}\n",
    "\n",
    "\n",
    "im_processor = AutoImageProcessor.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n",
    "\n",
    "path_output_cppe5, path_anno = save_cppe5_annotation_file_images(cppe5[\"test\"])\n",
    "test_ds_coco_format = CocoDetection(path_output_cppe5, im_processor, path_anno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€å¾Œã«ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulating evaluation results...\n",
       "DONE (t=0.08s).\n",
       "IoU metric: bbox\n",
       " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.352\n",
       " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.681\n",
       " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.292\n",
       " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.168\n",
       " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n",
       " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.274\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.484\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.323\n",
       " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n",
    "module = evaluate.load(\"ybelkada/cocoevaluate\", coco=test_ds_coco_format.coco)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_ds_coco_format, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "\n",
    "        labels = [\n",
    "            {k: v for k, v in t.items()} for t in batch[\"labels\"]\n",
    "        ]  # these are in DETR format, resized + normalized\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "        results = im_processor.post_process(outputs, orig_target_sizes)  # convert outputs of model to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "\n",
    "        module.add(prediction=results, reference=labels)\n",
    "        del batch\n",
    "\n",
    "results = module.compute()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã‚‰ã®çµæœã¯ã€[TrainingArguments](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.TrainingArguments) ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã•ã‚‰ã«æ”¹å–„ã§ãã¾ã™ã€‚è©¦ã—ã¦ã”ã‚‰ã‚“ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETR ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¦è©•ä¾¡ã—ã€Hugging Face Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã¾ã™ã€‚\n",
    "æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [pipeline()](https://huggingface.co/docs/transformers/main/ja/main_classes/pipelines#transformers.pipeline) ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¤œå‡ºã—ã€ãã‚Œã«ç”»åƒã‚’æ¸¡ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import requests\n",
    "\n",
    "url = \"https://i.imgur.com/2lnWoly.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "obj_detector = pipeline(\"object-detection\", model=\"devonho/detr-resnet-50_finetuned_cppe5\")\n",
    "obj_detector(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¿…è¦ã«å¿œã˜ã¦ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®çµæœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Detected Coverall with confidence 0.566 at location [1215.32, 147.38, 4401.81, 3227.08]\n",
       "Detected Mask with confidence 0.584 at location [2449.06, 823.19, 3256.43, 1413.9]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://i.imgur.com/4QZnf9A.png\" alt=\"Object detection result on a new image\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
