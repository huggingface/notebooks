{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load adapters with ğŸ¤— PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft) ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å‡çµã—ã€ãã®ä¸Šã«ã‚ãšã‹ãªè¨“ç·´å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ï¼‰ã‚’è¿½åŠ ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®æƒ…å ±ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«è¨“ç·´ã•ã‚Œã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå°‘ãªãã€å®Œå…¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’ä½ãæŠ‘ãˆã¤ã¤ã€åŒç­‰ã®çµæœã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "PEFTã§è¨“ç·´ã•ã‚ŒãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯é€šå¸¸ã€å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚ˆã‚Šã‚‚1æ¡å°ã•ãã€å…±æœ‰ã€ä¿å­˜ã€èª­ã¿è¾¼ã‚€ã®ãŒä¾¿åˆ©ã§ã™ã€‚\n",
    "\n",
    "<div class=\"flex flex-col justify-center\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png\"/>\n",
    "  <figcaption class=\"text-center\">Hubã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹OPTForCausalLMãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é‡ã¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å…¨ä½“ã‚µã‚¤ã‚ºã®ç´„6MBã§ã€ãƒ¢ãƒ‡ãƒ«é‡ã¿ã®å…¨ã‚µã‚¤ã‚ºã¯ç´„700MBã§ã™ã€‚</figcaption>\n",
    "</div>\n",
    "\n",
    "ğŸ¤— PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](https://huggingface.co/docs/peft/index)ã‚’ã”è¦§ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— PEFTã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦å§‹ã‚ã¾ã—ã‚‡ã†ï¼š\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install peft\n",
    "```\n",
    "\n",
    "æ–°æ©Ÿèƒ½ã‚’è©¦ã—ã¦ã¿ãŸã„å ´åˆã€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã«èˆˆå‘³ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼š\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/huggingface/peft.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported PEFT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformersã¯ã€ã„ãã¤ã‹ã®PEFTï¼ˆParameter Efficient Fine-Tuningï¼‰ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãƒã‚¤ãƒ†ã‚£ãƒ–ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€ãƒ­ãƒ¼ã‚«ãƒ«ã¾ãŸã¯Hubã«æ ¼ç´ã•ã‚ŒãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’ç°¡å˜ã«èª­ã¿è¾¼ã‚“ã§å®Ÿè¡Œã¾ãŸã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)\n",
    "- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)\n",
    "- [AdaLoRA](https://huggingface.co/papers/2303.10512)\n",
    "\n",
    "ä»–ã®PEFTãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå­¦ç¿’ã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´ãªã©ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã€ã¾ãŸã¯ğŸ¤— PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªå…¨èˆ¬ã«ã¤ã„ã¦ã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](https://huggingface.co/docs/peft/index)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a PEFT adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformersã‹ã‚‰PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ä½¿ç”¨ã™ã‚‹ã«ã¯ã€Hubãƒªãƒã‚¸ãƒˆãƒªã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« `adapter_config.json` ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚æ¬¡ã«ã€`AutoModelFor` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ç”¨ã®PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã«ã¯ï¼š\n",
    "\n",
    "1. PEFTãƒ¢ãƒ‡ãƒ«ã®IDã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "2. ãã‚Œã‚’[AutoModelForCausalLM](https://huggingface.co/docs/transformers/main/ja/model_doc/auto#transformers.AutoModelForCausalLM) ã‚¯ãƒ©ã‚¹ã«æ¸¡ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’`AutoModelFor`ã‚¯ãƒ©ã‚¹ã¾ãŸã¯åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ï¼ˆ`OPTForCausalLM`ã¾ãŸã¯`LlamaForCausalLM`ãªã©ï¼‰ã§èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ã¾ãŸã€`load_adapter`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ã€PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ã“ã¨ã‚‚ã§ãã¾ã™ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in 8bit or 4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bitsandbytes` çµ±åˆã¯ã€8ãƒ“ãƒƒãƒˆãŠã‚ˆã³4ãƒ“ãƒƒãƒˆã®ç²¾åº¦ãƒ‡ãƒ¼ã‚¿å‹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€éš›ã«ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ï¼ˆè©³ç´°ã«ã¤ã„ã¦ã¯ `bitsandbytes` çµ±åˆã®[ã‚¬ã‚¤ãƒ‰](https://huggingface.co/docs/transformers/main/ja/./quantization#bitsandbytes-integration)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼‰ã€‚[from_pretrained()](https://huggingface.co/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained) ã« `load_in_8bit` ã¾ãŸã¯ `load_in_4bit` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã€`device_map=\"auto\"` ã‚’è¨­å®šã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹æœçš„ã«ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«åˆ†æ•£é…ç½®ã§ãã¾ã™ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a new adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ—¢å­˜ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã«æ–°ã—ã„ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã« `add_adapter` ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ãŸã ã—ã€æ–°ã—ã„ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ç¾åœ¨ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨åŒã˜ã‚¿ã‚¤ãƒ—ã§ã‚ã‚‹é™ã‚Šã€ã“ã‚Œã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€ãƒ¢ãƒ‡ãƒ«ã«æ—¢å­˜ã® LoRA ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒã‚¢ã‚¿ãƒƒãƒã•ã‚Œã¦ã„ã‚‹å ´åˆï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    init_lora_weights=False\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ–°ã—ã„ã‚¢ãƒ€ãƒ—ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã«ã¯:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach new adapter with same config\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_adapter` ã‚’ä½¿ç”¨ã—ã¦ã€ã©ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã‚’è¨­å®šã§ãã¾ã™ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use adapter_1\n",
    "model.set_adapter(\"adapter_1\")\n",
    "output = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n",
    "\n",
    "# use adapter_2\n",
    "model.set_adapter(\"adapter_2\")\n",
    "output_enabled = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable and disable adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ ã—ãŸã‚‰ã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æœ‰åŠ¹ã¾ãŸã¯ç„¡åŠ¹ã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€æ¬¡ã®æ‰‹é †ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "adapter_model_id = \"ybelkada/opt-350m-lora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "text = \"Hello\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "peft_config = PeftConfig.from_pretrained(adapter_model_id)\n",
    "\n",
    "# to initiate with random weights\n",
    "peft_config.init_lora_weights = False\n",
    "\n",
    "model.add_adapter(peft_config)\n",
    "model.enable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç„¡åŠ¹ã«ã™ã‚‹ã«ã¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.disable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a PEFT adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯[Trainer](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer)ã‚¯ãƒ©ã‚¹ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãŠã‚Šã€ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«å¯¾ã—ã¦ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã ã‘ã§æ¸ˆã¿ã¾ã™ã€‚ãŸã¨ãˆã°ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆ:\n",
    "\n",
    "<Tip>\n",
    "\n",
    "[Trainer](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer)ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´](https://huggingface.co/docs/transformers/main/ja/training)ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "1. ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®æ§‹æˆã‚’å®šç¾©ã—ã¾ã™ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°ã«ã¤ã„ã¦ã¯`LoraConfig`ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ã“ã‚Œã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’ [Trainer](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer) ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, ...)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¿å­˜ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã‚¢ãƒ€ãƒ—ã‚¿ã¨ãã‚Œã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®æ‰‹é †ï¼š"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
