{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ä»˜ã‘ã¯ã€ç‰¹å®šã®ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ä¸€èˆ¬çš„ãªç¾å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯æ¬¡ã®ã‚‚ã®ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "è¦–è¦šéšœå®³è€…ãŒã•ã¾ã–ã¾ãªçŠ¶æ³ã‚’ä¹—ã‚Šè¶Šãˆã‚‰ã‚Œã‚‹ã‚ˆã†æ”¯æ´ã—ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³\n",
    "ç”»åƒã‚’èª¬æ˜ã™ã‚‹ã“ã¨ã§äººã€…ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã®ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚’å‘ä¸Šã•ã›ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "* ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n",
    "* å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate -q\n",
    "pip install jiwer -q\n",
    "```\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the PokÃ©mon BLIP captions dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€{image-caption} ãƒšã‚¢ã§æ§‹æˆã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚ç‹¬è‡ªã®ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã«ã¯\n",
    "PyTorch ã§ã¯ã€[ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb) ã‚’å‚ç…§ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"lambdalabs/pokemon-blip-captions\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image', 'text'],\n",
    "        num_rows: 833\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ `image`ã¨`text`ã® 2 ã¤ã®æ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "<Tip>\n",
    "\n",
    "å¤šãã®ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€ç”»åƒã”ã¨ã«è¤‡æ•°ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ã‚ˆã†ãªå ´åˆã€ä¸€èˆ¬çš„ãªæˆ¦ç•¥ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ä¸­ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "`train_test_split` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ã‚¤ãƒ³ ã‚¹ãƒ—ãƒªãƒƒãƒˆã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã‹ã‚‰ã®ã„ãã¤ã‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¦–è¦šåŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_images(images, captions):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, 12))\n",
    "        plt.title(caption)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "sample_images_to_visualize = [np.array(train_ds[i][\"image\"]) for i in range(5)]\n",
    "sample_captions = [train_ds[i][\"text\"] for i in range(5)]\n",
    "plot_images(sample_images_to_visualize, sample_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_training_images_image_cap.png\" alt=\"Sample training images\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ 2 ã¤ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ (ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆ) ãŒã‚ã‚‹ãŸã‚ã€å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å‰å‡¦ç†ã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã‚Œã‚’è¡Œã†ã«ã¯ã€å¾®èª¿æ•´ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸãƒ—ãƒ­ã‚»ãƒƒã‚µ ã‚¯ãƒ©ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "checkpoint = \"microsoft/git-base\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ—ãƒ­ã‚»ãƒƒã‚µã¯å†…éƒ¨ã§ç”»åƒã‚’å‰å‡¦ç†ã— (ã‚µã‚¤ã‚ºå¤‰æ›´ã‚„ãƒ”ã‚¯ã‚»ãƒ« ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å«ã‚€)ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    captions = [x for x in example_batch[\"text\"]]\n",
    "    inputs = processor(images=images, text=captions, padding=\"max_length\")\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs\n",
    "\n",
    "\n",
    "train_ds.set_transform(transforms)\n",
    "test_ds.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ãŒã§ããŸã‚‰ã€å¾®èª¿æ•´ç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"microsoft/git-base\"](https://huggingface.co/microsoft/git-base) ã‚’ [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã€[Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge) ã¾ãŸã¯ [Word Error Rate](https://huggingface.co/spaces/evaluate-metric/) ã§è©•ä¾¡ã•ã‚Œã¾ã™ã€‚ãã†ã ã£ãŸï¼‰ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Word Error Rate (WER) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã‚Œã‚’è¡Œã†ã«ã¯ ğŸ¤— Evaluate ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ WER ã®æ½œåœ¨çš„ãªåˆ¶é™ã‚„ãã®ä»–ã®å•é¡Œç‚¹ã«ã¤ã„ã¦ã¯ã€[ã“ã®ã‚¬ã‚¤ãƒ‰](https://huggingface.co/spaces/evaluate-metric/wer) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "wer = load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predicted = logits.argmax(-1)\n",
    "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)\n",
    "    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "    return {\"wer_score\": wer_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§ã€ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ã“ã‚Œã«ã¯ ğŸ¤— [Trainer](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "ã¾ãšã€[TrainingArguments](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.TrainingArguments) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-pokemon\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=50,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer æ¬¡ã«ã€æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã¨ä¸€ç·’ã« ğŸ¤— ã«æ¸¡ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€[Trainer](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer) ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã® [train()](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.train) ã‚’å‘¼ã³å‡ºã™ã ã‘ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé€²ã‚€ã«ã¤ã‚Œã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æå¤±ãŒã‚¹ãƒ ãƒ¼ã‚ºã«æ¸›å°‘ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [push_to_hub()](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.push_to_hub) ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_ds` ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’å–å¾—ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/test_image_image_cap.png\" alt=\"Test image\"/>\n",
    "</div>\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ç”¨ã®ç”»åƒã‚’æº–å‚™ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate` ã‚’å‘¼ã³å‡ºã—ã¦äºˆæ¸¬ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "a drawing of a pink and blue pokemon\n",
    "```\n",
    "\n",
    "å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šã€éå¸¸ã«å„ªã‚ŒãŸã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒç”Ÿæˆã•ã‚ŒãŸã‚ˆã†ã§ã™ã€‚"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
