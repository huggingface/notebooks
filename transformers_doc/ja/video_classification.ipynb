{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ“ãƒ‡ã‚ªåˆ†é¡ã¯ã€ãƒ“ãƒ‡ã‚ªå…¨ä½“ã«ãƒ©ãƒ™ãƒ«ã¾ãŸã¯ã‚¯ãƒ©ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ãƒ“ãƒ‡ã‚ªã«ã¯ã€å„ãƒ“ãƒ‡ã‚ªã« 1 ã¤ã®ã‚¯ãƒ©ã‚¹ã®ã¿ãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ãƒ“ãƒ‡ã‚ªåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã¯ãƒ“ãƒ‡ã‚ªã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒ“ãƒ‡ã‚ªãŒã©ã®ã‚¯ãƒ©ã‚¹ã«å±ã™ã‚‹ã‹ã«ã¤ã„ã¦ã®äºˆæ¸¬ã‚’è¿”ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ“ãƒ‡ã‚ªã®å†…å®¹ã‚’åˆ†é¡ã§ãã¾ã™ã€‚ãƒ“ãƒ‡ã‚ªåˆ†é¡ã®å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³/ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£èªè­˜ã§ã‚ã‚Šã€ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã¾ãŸã€è¦–è¦šéšœå®³ã®ã‚ã‚‹äººã«ã¨ã£ã¦ã€ç‰¹ã«é€šå‹¤æ™‚ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "1. [UCF101](https://www.crcv.ucf.edu/) ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§ [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚ data/UCF101.php) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚\n",
    "2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/video-classification) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "```bash\n",
    "pip install -q pytorchvideo transformers evaluate\n",
    "```\n",
    "\n",
    "[PyTorchVideo](https://pytorchvideo.org/) (`pytorchvideo` ã¨å‘¼ã°ã‚Œã¾ã™) ã‚’ä½¿ç”¨ã—ã¦ãƒ“ãƒ‡ã‚ªã‚’å‡¦ç†ã—ã€æº–å‚™ã—ã¾ã™ã€‚\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load UCF101 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¾ãšã€[UCF-101 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://www.crcv.ucf.edu/data/UCF101.php) ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
    "filename = \"UCF101_subset.tar.gz\"\n",
    "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸå¾Œã€åœ§ç¸®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’æŠ½å‡ºã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(file_path) as t:\n",
    "     t.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤§ã¾ã‹ã«è¨€ã†ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ¬¡ã®ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "```bash\n",
    "UCF101_subset/\n",
    "    train/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "    val/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "    test/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "```\n",
    "\n",
    "(`sorted`)ã•ã‚ŒãŸ ãƒ“ãƒ‡ã‚ª ãƒ‘ã‚¹ã¯æ¬¡ã®ã‚ˆã†ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "\n",
    "```bash\n",
    "...\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\n",
    "...\n",
    "```\n",
    "\n",
    "åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—/ã‚·ãƒ¼ãƒ³ã«å±ã™ã‚‹ãƒ“ãƒ‡ã‚ª ã‚¯ãƒªãƒƒãƒ—ãŒã‚ã‚Šã€ãƒ“ãƒ‡ã‚ª ãƒ•ã‚¡ã‚¤ãƒ« ãƒ‘ã‚¹ã§ã¯ã‚°ãƒ«ãƒ¼ãƒ—ãŒ`g`ã§ç¤ºã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ãŸã¨ãˆã°ã€`v_ApplyEyeMakeup_g07_c04.avi`ã‚„`v_ApplyEyeMakeup_g07_c06.avi`ãªã©ã§ã™ã€‚\n",
    "\n",
    "æ¤œè¨¼ã¨è©•ä¾¡ã®åˆ†å‰²ã§ã¯ã€[ãƒ‡ãƒ¼ã‚¿æ¼æ´©](https://www.kaggle.com/code/alexisbcook/data-leakage) ã‚’é˜²ããŸã‚ã«ã€åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—/ã‚·ãƒ¼ãƒ³ã‹ã‚‰ã®ãƒ“ãƒ‡ã‚ª ã‚¯ãƒªãƒƒãƒ—ã‚’ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã¯ã€ã“ã®æƒ…å ±ãŒè€ƒæ…®ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã«å­˜åœ¨ã™ã‚‹ãƒ©ãƒ™ãƒ«ã®ã‚»ãƒƒãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹ã¨ãã«å½¹ç«‹ã¤ 2 ã¤ã®è¾æ›¸ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "\n",
    "* `label2id`: ã‚¯ãƒ©ã‚¹åã‚’æ•´æ•°ã«ãƒãƒƒãƒ—ã—ã¾ã™ã€‚\n",
    "* `id2label`: æ•´æ•°ã‚’ã‚¯ãƒ©ã‚¹åã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress']."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å€‹æ€§çš„ãªã‚¯ãƒ©ã‚¹ãŒ10ç¨®é¡ã‚ã‚Šã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã«ã¯ã€ã‚¯ãƒ©ã‚¹ã”ã¨ã« 30 å€‹ã®ãƒ“ãƒ‡ã‚ªãŒã‚ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a model to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ãã‚Œã«é–¢é€£ã™ã‚‹ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‹ã‚‰ãƒ“ãƒ‡ã‚ªåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã«ã¯äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãŒä»˜å±ã—ã¦ãŠã‚Šã€åˆ†é¡ãƒ˜ãƒƒãƒ‰ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹ã¨ãã«å½¹ç«‹ã¡ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "model_ckpt = \"MCG-NJU/videomae-base\"\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã€æ¬¡ã®è­¦å‘ŠãŒè¡¨ç¤ºã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "```bash\n",
    "Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']\n",
    "- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "```\n",
    "\n",
    "ã“ã®è­¦å‘Šã¯ã€ä¸€éƒ¨ã®é‡ã¿ (ãŸã¨ãˆã°ã€`classifier`å±¤ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹) ã‚’ç ´æ£„ã—ã€ä»–ã®ã„ãã¤ã‹ã®é‡ã¿ (æ–°ã—ã„`classifier`å±¤ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹) ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã®å ´åˆã€ã“ã‚Œã¯äºˆæƒ³ã•ã‚Œã‚‹ã“ã¨ã§ã™ã€‚äº‹å‰ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸé‡ã¿ã‚’æŒãŸãªã„æ–°ã—ã„é ­éƒ¨ã‚’è¿½åŠ ã—ã¦ã„ã‚‹ãŸã‚ã€æ¨è«–ã«ä½¿ç”¨ã™ã‚‹å‰ã«ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè­¦å‘Šã—ã¾ã™ã€‚ã“ã‚Œã¯ã¾ã•ã«ç§ãŸã¡ãŒè¡ŒãŠã†ã¨ã—ã¦ã„ã‚‹ã‚‚ã®ã§ã™ã€‚ã™ã‚‹ã€‚\n",
    "\n",
    "**æ³¨æ„** [ã“ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) ã¯ã€åŒæ§˜ã®ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§å¾®èª¿æ•´ã•ã‚Œã¦ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå–å¾—ã•ã‚ŒãŸãŸã‚ã€ã“ã®ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã‹ãªã‚Šã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã®é‡è¤‡ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯ã€‚ `MCG-NJU/videomae-base-finetuned-kinetics` ã‚’å¾®èª¿æ•´ã—ã¦å–å¾—ã—ãŸ [ã“ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) ã‚’ç¢ºèªã§ãã¾ã™ã€‚ -ã‚­ãƒãƒ†ã‚£ã‚¯ã‚¹`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ“ãƒ‡ã‚ªã®å‰å‡¦ç†ã«ã¯ã€[PyTorchVideo ãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://pytorchvideo.org/) ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚ã¾ãšã€å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤‰æ›ã«ã¯ã€å‡ä¸€ãªæ™‚é–“ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ãƒ”ã‚¯ã‚»ãƒ«æ­£è¦åŒ–ã€ãƒ©ãƒ³ãƒ€ãƒ  ã‚¯ãƒ­ãƒƒãƒ”ãƒ³ã‚°ã€ãŠã‚ˆã³ãƒ©ãƒ³ãƒ€ãƒ ãªæ°´å¹³åè»¢ã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨â€‹â€‹ã—ã¾ã™ã€‚æ¤œè¨¼ãŠã‚ˆã³è©•ä¾¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤‰æ›ã§ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãªãƒˆãƒªãƒŸãƒ³ã‚°ã¨æ°´å¹³åè»¢ã‚’é™¤ãã€åŒã˜å¤‰æ›ãƒã‚§ãƒ¼ãƒ³ã‚’ç¶­æŒã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®å¤‰æ›ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[PyTorchVideo ã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pytorchvideo.org) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸ`image_processor`ã‚’ä½¿ç”¨ã—ã¦ã€æ¬¡ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
    "\n",
    "* ãƒ“ãƒ‡ã‚ª ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒæ­£è¦åŒ–ã•ã‚Œã‚‹ç”»åƒã®å¹³å‡å€¤ã¨æ¨™æº–åå·®ã€‚\n",
    "* ãƒ“ãƒ‡ã‚ª ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚µã‚¤ã‚ºãŒå¤‰æ›´ã•ã‚Œã‚‹ç©ºé–“è§£åƒåº¦ã€‚\n",
    "\n",
    "ã¾ãšã€ã„ãã¤ã‹ã®å®šæ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®å¤‰æ›ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãã‚Œãã‚Œå®šç¾©ã—ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã‹ã‚‰å§‹ã‚ã¾ã™:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    RandomCrop(resize_to),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒã˜ä¸€é€£ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¤œè¨¼ã‚»ãƒƒãƒˆã¨è©•ä¾¡ã‚»ãƒƒãƒˆã«é©ç”¨ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ³¨æ„**: ä¸Šè¨˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€[å…¬å¼ PyTorchVideo ã‚µãƒ³ãƒ—ãƒ«](https://pytorchvideo.org/docs/tutorial_classification#dataset) ã‹ã‚‰å–å¾—ã—ãŸã‚‚ã®ã§ã™ã€‚ [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ UCF-101 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚å†…éƒ¨ã§ã¯ã€[`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã—ã¾ã™ã€‚ `LabeledVideoDataset` ã‚¯ãƒ©ã‚¹ã¯ã€PyTorchVideo ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ã™ã¹ã¦ã®ãƒ“ãƒ‡ã‚ªã®åŸºæœ¬ã‚¯ãƒ©ã‚¹ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€PyTorchVideo ã§æ—¢è£½ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚«ã‚¹ã‚¿ãƒ  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ `LabeledVideoDataset` ã‚¯ãƒ©ã‚¹ã‚’æ‹¡å¼µã§ãã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€`data`API [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒåŒæ§˜ã®æ§‹é€  (ä¸Šã«ç¤ºã—ãŸã‚‚ã®) ã«å¾“ã£ã¦ã„ã‚‹å ´åˆã¯ã€`pytorchvideo.data.Ucf101()` ã‚’ä½¿ç”¨ã™ã‚‹ã¨å•é¡Œãªãå‹•ä½œã™ã‚‹ã¯ãšã§ã™ã€‚\n",
    "\n",
    "`num_videos` å¼•æ•°ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ãƒ“ãƒ‡ã‚ªã®æ•°ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# (300, 30, 75)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the preprocessed video for better debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...\n",
       "    The video tensor is expected to have the following shape:\n",
       "    (num_frames, num_channels, height, width).\n",
       "    \"\"\"\n",
       "    frames = []\n",
       "    for video_frame in video_tensor:\n",
       "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
       "        frames.append(frame_unnormalized)\n",
       "    kargs = {\"duration\": 0.25}\n",
       "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
       "    return filename"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)\n",
    "\n",
    "sample_video = next(iter(train_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif\" alt=\"Person playing basketball\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformers ã® [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) ã‚’ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«åˆ©ç”¨ã—ã¾ã™ã€‚ `Trainer`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ§‹æˆã¨è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æœ€ã‚‚é‡è¦ãªã®ã¯ [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) ã§ã€ã“ã‚Œã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ§‹æˆã™ã‚‹ãŸã‚ã®ã™ã¹ã¦ã®å±æ€§ã‚’å«ã‚€ã‚¯ãƒ©ã‚¹ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼åãŒå¿…è¦ã§ã™ã€‚ã¾ãŸã€ğŸ¤— Hub ä¸Šã®ãƒ¢ãƒ‡ãƒ« ãƒªãƒã‚¸ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®æƒ…å ±ã‚’åŒæœŸã™ã‚‹ã®ã«ã‚‚å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã®ã»ã¨ã‚“ã©ã¯ä¸€ç›®ç­ç„¶ã§ã™ãŒã€ã“ã“ã§éå¸¸ã«é‡è¦ãªã®ã¯`remove_unused_columns=False`ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å‘¼ã³å‡ºã—é–¢æ•°ã§ä½¿ç”¨ã•ã‚Œãªã„æ©Ÿèƒ½ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯`True`ã§ã™ã€‚ã“ã‚Œã¯ã€é€šå¸¸ã€æœªä½¿ç”¨ã®ç‰¹å¾´åˆ—ã‚’å‰Šé™¤ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å‘¼ã³å‡ºã—é–¢æ•°ã¸ã®å…¥åŠ›ã‚’è§£å‡ã—ã‚„ã™ãã™ã‚‹ã“ã¨ãŒç†æƒ³çš„ã§ã‚ã‚‹ãŸã‚ã§ã™ã€‚ãŸã ã—ã€ã“ã®å ´åˆã€`pixel_values` (ãƒ¢ãƒ‡ãƒ«ãŒå…¥åŠ›ã§æœŸå¾…ã™ã‚‹å¿…é ˆã‚­ãƒ¼ã§ã™) ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€æœªä½¿ç”¨ã®æ©Ÿèƒ½ (ç‰¹ã«`video`) ãŒå¿…è¦ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\n",
    "num_epochs = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytorchvideo.data.Ucf101()` ã«ã‚ˆã£ã¦è¿”ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ `__len__` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã›ã‚“ã€‚ãã®ãŸã‚ã€`TrainingArguments`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ã¨ãã«`max_steps`ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã«ã€äºˆæ¸¬ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€ã“ã‚Œã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã™ã‚‹`metric`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚å¿…è¦ãªå‰å‡¦ç†ã¯ã€äºˆæ¸¬ã•ã‚ŒãŸãƒ­ã‚¸ãƒƒãƒˆã® argmax ã‚’å–å¾—ã™ã‚‹ã“ã¨ã ã‘ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è©•ä¾¡ã«é–¢ã™ã‚‹æ³¨æ„äº‹é …**:\n",
    "\n",
    "[VideoMAE è«–æ–‡](https://huggingface.co/papers/2203.12602) ã§ã¯ã€è‘—è€…ã¯æ¬¡ã®è©•ä¾¡æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚å½¼ã‚‰ã¯ãƒ†ã‚¹ãƒˆ ãƒ“ãƒ‡ã‚ªã‹ã‚‰ã®ã„ãã¤ã‹ã®ã‚¯ãƒªãƒƒãƒ—ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€ãã‚Œã‚‰ã®ã‚¯ãƒªãƒƒãƒ—ã«ã•ã¾ã–ã¾ãªã‚¯ãƒ­ãƒƒãƒ—ã‚’é©ç”¨ã—ã¦ã€åˆè¨ˆã‚¹ã‚³ã‚¢ã‚’å ±å‘Šã—ã¾ã™ã€‚ãŸã ã—ã€å˜ç´”ã•ã¨ç°¡æ½”ã•ã‚’ä¿ã¤ãŸã‚ã«ã€ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ãã‚Œã‚’è€ƒæ…®ã—ã¾ã›ã‚“ã€‚\n",
    "\n",
    "ã¾ãŸã€ã‚µãƒ³ãƒ—ãƒ«ã‚’ã¾ã¨ã‚ã¦ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ `collatâ€‹â€‹e_fn` ã‚’å®šç¾©ã—ã¾ã™ã€‚å„ãƒãƒƒãƒã¯ã€`pixel_values` ã¨ `labels` ã¨ã„ã† 2 ã¤ã®ã‚­ãƒ¼ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ã“ã‚Œã‚‰ã™ã¹ã¦ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã¨ã‚‚ã«`Trainer`ã«æ¸¡ã™ã ã‘ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã™ã§ã«ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã—ã¦ã„ã‚‹ã®ã«ã€ãªãœãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ã—ã¦`image_processor`ã‚’æ¸¡ã—ãŸã®ã‹ä¸æ€è­°ã«æ€ã†ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã‚Œã¯ã€ã‚¤ãƒ¡ãƒ¼ã‚¸ ãƒ—ãƒ­ã‚»ãƒƒã‚µæ§‹æˆãƒ•ã‚¡ã‚¤ãƒ« (JSON ã¨ã—ã¦ä¿å­˜) ã‚‚ãƒãƒ–ä¸Šã®ãƒªãƒã‚¸ãƒˆãƒªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã ã‘ã§ã™ã€‚\n",
    "\n",
    "æ¬¡ã«ã€`train` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [push_to_hub()](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.push_to_hub) ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "\n",
    "æ¨è«–ã®ãŸã‚ã«ãƒ“ãƒ‡ã‚ªã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_video = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif\" alt=\"Teams playing basketball\"/>\n",
    "</div>\n",
    "\n",
    "æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline). ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ“ãƒ‡ã‚ªåˆ†é¡ç”¨ã®` pipeline`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã«ãƒ“ãƒ‡ã‚ªã‚’æ¸¡ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9272987842559814, 'label': 'BasketballDunk'},\n",
       " {'score': 0.017777055501937866, 'label': 'BabyCrawling'},\n",
       " {'score': 0.01663011871278286, 'label': 'BalanceBeam'},\n",
       " {'score': 0.009560945443809032, 'label': 'BandMarching'},\n",
       " {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "video_cls = pipeline(model=\"my_awesome_video_cls_model\")\n",
    "video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¿…è¦ã«å¿œã˜ã¦ã€`pipeline`ã®çµæœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, video):\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
    "    inputs = {\n",
    "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
    "        \"labels\": torch.tensor(\n",
    "            [sample_test_video[\"label\"]]\n",
    "        ),  # this can be skipped if you don't have labels available.\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits `ã‚’è¿”ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = run_inference(trained_model, sample_test_video[\"video\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logits` ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Predicted class: BasketballDunk"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
