{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tjAIM7BOYhw?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tjAIM7BOYhw?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”»åƒåˆ†é¡ã§ã¯ã€ç”»åƒã«ãƒ©ãƒ™ãƒ«ã¾ãŸã¯ã‚¯ãƒ©ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚„éŸ³å£°ã®åˆ†é¡ã¨ã¯ç•°ãªã‚Šã€å…¥åŠ›ã¯\n",
    "ç”»åƒã‚’æ§‹æˆã™ã‚‹ãƒ”ã‚¯ã‚»ãƒ«å€¤ã€‚æå‚·ã®æ¤œå‡ºãªã©ã€ç”»åƒåˆ†é¡ã«ã¯å¤šãã®ç”¨é€”ãŒã‚ã‚Šã¾ã™\n",
    "è‡ªç„¶ç½å®³ã®å¾Œã€ä½œç‰©ã®å¥åº·çŠ¶æ…‹ã‚’ç›£è¦–ã—ãŸã‚Šã€ç—…æ°—ã®å…†å€™ãŒãªã„ã‹åŒ»ç™‚ç”»åƒã‚’ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚Šã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "1. [Food-101](https://huggingface.co/datasets/food101) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® [ViT](https://huggingface.co/docs/transformers/main/ja/tasks/model_doc/vit) ã‚’å¾®èª¿æ•´ã—ã¦ã€ç”»åƒå†…ã®é£Ÿå“ã‚’åˆ†é¡ã—ã¾ã™ã€‚\n",
    "2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/image-classification) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate\n",
    "```\n",
    "\n",
    "Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Food-101 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasetsã€ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ Food-101 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ¬¡ã®æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™\n",
    "å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_test_split` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `train` åˆ†å‰²ã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F52AFC8AC50>,\n",
       " 'label': 79}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®å„ä¾‹ã«ã¯ 2 ã¤ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "- `image`: é£Ÿå“ã® PIL ç”»åƒ\n",
    "- `label`: é£Ÿå“ã®ãƒ©ãƒ™ãƒ«ã‚¯ãƒ©ã‚¹\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ãŒãƒ©ãƒ™ãƒ« ID ã‹ã‚‰ãƒ©ãƒ™ãƒ«åã‚’å–å¾—ã—ã‚„ã™ãã™ã‚‹ãŸã‚ã«ã€ãƒ©ãƒ™ãƒ«åã‚’ãƒãƒƒãƒ—ã™ã‚‹è¾æ›¸ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "æ•´æ•°ã¸ã®å¤‰æ›ã€ã¾ãŸã¯ãã®é€†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§ã€ãƒ©ãƒ™ãƒ« ID ã‚’ãƒ©ãƒ™ãƒ«åã«å¤‰æ›ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prime_rib'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(79)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ViT ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ç”»åƒã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å‡¦ç†ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éå‰°é©åˆã‚’å›é¿ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹ãŸã‚ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°éƒ¨åˆ†ã«ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è¿½åŠ ã—ã¾ã™ã€‚\n",
    "ã“ã“ã§ã¯ã€Keras å‰å‡¦ç†ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ› (ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’å«ã‚€) ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
    "æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ› (ä¸­å¤®ã®ãƒˆãƒªãƒŸãƒ³ã‚°ã€ã‚µã‚¤ã‚ºå¤‰æ›´ã€æ­£è¦åŒ–ã®ã¿)ã€‚ `tf.image` ã¾ãŸã¯\n",
    "ä»–ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    "train_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"train_data_augmentation\",\n",
    ")\n",
    "\n",
    "val_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.CenterCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ],\n",
    "    name=\"val_data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ä¸€åº¦ã« 1 ã¤ã®ç”»åƒã§ã¯ãªãã€ç”»åƒã®ãƒãƒƒãƒã«é©åˆ‡ãªå¤‰æ›ã‚’é©ç”¨ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def convert_to_tf_tensor(image: Image):\n",
    "    np_image = np.array(image)\n",
    "    tf_image = tf.convert_to_tensor(np_image)\n",
    "    # `expand_dims()` is used to add a batch dimension since\n",
    "    # the TF augmentation layers operates on batched inputs.\n",
    "    return tf.expand_dims(tf_image, 0)\n",
    "\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ `set_transform` ã‚’ä½¿ç”¨ã—ã¦ã€ãã®å ´ã§å¤‰æ›ã‚’é©ç”¨ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food[\"train\"].set_transform(preprocess_train)\n",
    "food[\"test\"].set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€å¾Œã®å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€`DefaultDataCollatâ€‹â€‹or`ã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã‚’ä½œæˆã—ã¾ã™ã€‚ ğŸ¤— Transformers ã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ç…§åˆæ©Ÿèƒ½ã¨ã¯ç•°ãªã‚Šã€\n",
    "`DefaultDataCollatâ€‹â€‹or` ã¯ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãªã©ã®è¿½åŠ ã®å‰å‡¦ç†ã‚’é©ç”¨ã—ã¾ã›ã‚“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚ã‚‹ã¨ã€å¤šãã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã™ãã«ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™\n",
    "ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸè©•ä¾¡æ–¹æ³•ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™\n",
    "[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) æŒ‡æ¨™ (è©³ç´°ã«ã¤ã„ã¦ã¯ã€ğŸ¤— è©•ä¾¡ [ã‚¯ã‚¤ãƒƒã‚¯ ãƒ„ã‚¢ãƒ¼](https://huggingface.co/docs/evaluate/a_quick_tour) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è¨ˆç®—ã™ã‚‹æ–¹æ³•):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€äºˆæ¸¬ã¨ãƒ©ãƒ™ãƒ«ã‚’ `compute` ã«æ¸¡ã—ã¦ç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§ `compute_metrics`é–¢æ•°ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¨­å®šã™ã‚‹ã¨ãã«ã“ã®é–¢æ•°ã«æˆ»ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "\n",
    "Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ã¾ãš [åŸºæœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://huggingface.co/docs/transformers/main/ja/tasks/./training#train-a-tensorflow-model-with-keras) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "\n",
    "TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€æ¬¡ã®æ‰‹é †ã«å¾“ã„ã¾ã™ã€‚\n",
    "1. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
    "2. äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚\n",
    "3. ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` ã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "4. ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚\n",
    "5. ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ ã—ã€`fit()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "6. ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã—ã¾ã™ã€‚\n",
    "\n",
    "ã¾ãšã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_train_steps = len(food[\"train\"]) * num_epochs\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ãƒ©ãƒ™ãƒ« ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã¨ã‚‚ã« [TFAutoModelForImageClassification](https://huggingface.co/docs/transformers/main/ja/model_doc/auto#transformers.TFAutoModelForImageClassification) ã‚’ä½¿ç”¨ã—ã¦ ViT ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert your datasets to the `tf.data.Dataset` format using the `to_tf_dataset` and your `data_collator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = food[\"train\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# converting our test dataset to tf.data.Dataset\n",
    "tf_eval_dataset = food[\"test\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compile()` ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "äºˆæ¸¬ã‹ã‚‰ç²¾åº¦ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã«ã¯ã€[Keras callbacks](https://huggingface.co/docs/transformers/main/ja/tasks/../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "`compute_metrics` é–¢æ•°ã‚’ [KerasMetricCallback](https://huggingface.co/docs/transformers/main/ja/tasks/../main_classes/keras_callbacks#transformers.KerasMetricCallback) ã«æ¸¡ã—ã¾ã™ã€‚\n",
    "[PushToHubCallback](https://huggingface.co/docs/transformers/main/ja/tasks/../main_classes/keras_callbacks#transformers.PushToHubCallback) ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"food_classifier\",\n",
    "    tokenizer=image_processor,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1/5\n",
       "250/250 [==============================] - 313s 1s/step - loss: 2.5623 - val_loss: 1.4161 - accuracy: 0.9290\n",
       "Epoch 2/5\n",
       "250/250 [==============================] - 265s 1s/step - loss: 0.9181 - val_loss: 0.6808 - accuracy: 0.9690\n",
       "Epoch 3/5\n",
       "250/250 [==============================] - 252s 1s/step - loss: 0.3910 - val_loss: 0.4303 - accuracy: 0.9820\n",
       "Epoch 4/5\n",
       "250/250 [==============================] - 251s 1s/step - loss: 0.2028 - val_loss: 0.3191 - accuracy: 0.9900\n",
       "Epoch 5/5\n",
       "250/250 [==============================] - 238s 949ms/step - loss: 0.1232 - val_loss: 0.3259 - accuracy: 0.9890"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãŠã‚ã§ã¨ã†ï¼ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã€ğŸ¤— Hub ã§å…±æœ‰ã—ã¾ã—ãŸã€‚ã“ã‚Œã§æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ç”»åƒåˆ†é¡ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã®è©³ç´°ãªä¾‹ã«ã¤ã„ã¦ã¯ã€å¯¾å¿œã™ã‚‹ [PyTorch ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "\n",
    "æ¨è«–ã‚’å®Ÿè¡Œã—ãŸã„ç”»åƒã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"food101\", split=\"validation[:10]\")\n",
    "image = ds[\"image\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\" alt=\"image of beignets\"/>\n",
    "</div>\n",
    "\n",
    "æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [pipeline()](https://huggingface.co/docs/transformers/main/ja/main_classes/pipelines#transformers.pipeline) ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ç”»åƒåˆ†é¡ç”¨ã®`pipeline`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã«ç”»åƒã‚’æ¸¡ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.31856709718704224, 'label': 'beignets'},\n",
       " {'score': 0.015232225880026817, 'label': 'bruschetta'},\n",
       " {'score': 0.01519392803311348, 'label': 'chicken_wings'},\n",
       " {'score': 0.013022331520915031, 'label': 'pork_chop'},\n",
       " {'score': 0.012728818692266941, 'label': 'prime_rib'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model\")\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¿…è¦ã«å¿œã˜ã¦ã€`pipeline`ã®çµæœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
    "\n",
    "\n",
    "\n",
    "ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ç”»åƒã‚’å‰å‡¦ç†ã—ã€`input`ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MariaK/food_classifier\")\n",
    "inputs = image_processor(image, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’è¿”ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\"MariaK/food_classifier\")\n",
    "logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€ã‚‚é«˜ã„ç¢ºç‡ã§äºˆæ¸¬ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’å–å¾—ã—ã€ãƒ¢ãƒ‡ãƒ«ã® `id2label` ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ™ãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beignets'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
