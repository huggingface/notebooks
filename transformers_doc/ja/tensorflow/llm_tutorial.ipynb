{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMã€ã¾ãŸã¯Large Language Modelsï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®éµã¨ãªã‚‹è¦ç´ ã§ã™ã€‚è¦ã™ã‚‹ã«ã€ã“ã‚Œã‚‰ã¯å¤§è¦æ¨¡ãªäº‹å‰è¨“ç·´æ¸ˆã¿ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã€ä¸ãˆã‚‰ã‚ŒãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦æ¬¡ã®å˜èªï¼ˆã¾ãŸã¯ã€ã‚ˆã‚Šæ­£ç¢ºã«ã¯ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’äºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã‚’1ã¤ãšã¤äºˆæ¸¬ã™ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™ã ã‘ã§ã¯æ–°ã—ã„æ–‡ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½•ã‹ã‚ˆã‚Šç²¾å·§ãªã“ã¨ã‚’ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚è‡ªå·±å›å¸°ç”Ÿæˆã‚’è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "è‡ªå·±å›å¸°ç”Ÿæˆã¯ã€æ¨è«–æ™‚ã®æ‰‹ç¶šãã§ã€ã„ãã¤ã‹ã®åˆæœŸå…¥åŠ›ã‚’ä¸ãˆãŸçŠ¶æ…‹ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’åå¾©çš„ã«å‘¼ã³å‡ºã™æ‰‹æ³•ã§ã™ã€‚ğŸ¤— Transformersã§ã¯ã€ã“ã‚Œã¯[generate()](https://huggingface.co/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate)ãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã£ã¦å‡¦ç†ã•ã‚Œã€ã“ã‚Œã¯ç”Ÿæˆèƒ½åŠ›ã‚’æŒã¤ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ä»¥ä¸‹ã®ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼š\n",
    "\n",
    "* LLMã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹æ–¹æ³•\n",
    "* ä¸€èˆ¬çš„ãªè½ã¨ã—ç©´ã‚’å›é¿ã™ã‚‹æ–¹æ³•\n",
    "* LLMã‚’æœ€å¤§é™ã«æ´»ç”¨ã™ã‚‹ãŸã‚ã®æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install transformers bitsandbytes>=0.39.0 -q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°](https://huggingface.co/docs/transformers/main/ja/tasks/language_modeling)ã®ãŸã‚ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡åˆ†å¸ƒã‚’è¿”ã—ã¾ã™ã€‚\n",
    "\n",
    "<!-- [GIF 1 -- FWD PASS] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"Forward pass of an LLM\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "LLMï¼ˆLanguage Modelï¼‰ã«ã‚ˆã‚‹è‡ªå·±å›å¸°ç”Ÿæˆã®é‡è¦ãªå´é¢ã®1ã¤ã¯ã€ã“ã®ç¢ºç‡åˆ†å¸ƒã‹ã‚‰æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠã™ã‚‹æ–¹æ³•ã§ã™ã€‚ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€æ¬¡ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¾—ã‚‰ã‚Œã‚‹é™ã‚Šã€ä½•ã§ã‚‚å¯èƒ½ã§ã™ã€‚ã“ã‚Œã¯ã€ç¢ºç‡åˆ†å¸ƒã‹ã‚‰æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠã™ã‚‹ã ã‘ã®ã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹æ³•ã‹ã‚‰ã€çµæœã®åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å‰ã«æ•°ã€…ã®å¤‰æ›ã‚’é©ç”¨ã™ã‚‹ã»ã©è¤‡é›‘ãªæ–¹æ³•ã¾ã§ã€ã‚ã‚‰ã‚†ã‚‹æ–¹æ³•ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
    "\n",
    "\n",
    "<!-- [GIF 2 -- TEXT GENERATION] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"Autoregressive generation iteratively selects the next token from a probability distribution to generate text\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "ä¸Šè¨˜ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ã‚ã‚‹åœæ­¢æ¡ä»¶ãŒæº€ãŸã•ã‚Œã‚‹ã¾ã§åå¾©çš„ã«ç¹°ã‚Šè¿”ã•ã‚Œã¾ã™ã€‚ç†æƒ³çš„ã«ã¯ã€åœæ­¢æ¡ä»¶ã¯ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦æŒ‡ç¤ºã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã¯çµ‚äº†ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆ`EOS`ï¼‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡ºåŠ›ã™ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’å­¦ç¿’ã™ã¹ãã§ã™ã€‚ã“ã‚ŒãŒãã†ã§ãªã„å ´åˆã€ç”Ÿæˆã¯ã‚ã‚‰ã‹ã˜ã‚å®šç¾©ã•ã‚ŒãŸæœ€å¤§é•·ã«é”ã—ãŸã¨ãã«åœæ­¢ã—ã¾ã™ã€‚\n",
    "\n",
    "ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠã‚¹ãƒ†ãƒƒãƒ—ã¨åœæ­¢æ¡ä»¶ã‚’é©åˆ‡ã«è¨­å®šã™ã‚‹ã“ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚¿ã‚¹ã‚¯ã§æœŸå¾…ã©ãŠã‚Šã«æŒ¯ã‚‹èˆã†ãŸã‚ã«é‡è¦ã§ã™ã€‚ãã‚ŒãŒã€å„ãƒ¢ãƒ‡ãƒ«ã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸ [GenerationConfig](https://huggingface.co/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationConfig) ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ç†ç”±ã§ã‚ã‚Šã€ã“ã‚Œã«ã¯å„ªã‚ŒãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãŒå«ã¾ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã¨ä¸€ç·’ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚\n",
    "\n",
    "ã‚³ãƒ¼ãƒ‰ã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ï¼\n",
    "\n",
    "<Tip>\n",
    "\n",
    "åŸºæœ¬çš„ãªLLMã®ä½¿ç”¨ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã€é«˜ãƒ¬ãƒ™ãƒ«ã® [`Pipeline`](https://huggingface.co/docs/transformers/main/ja/pipeline_tutorial) ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒè‰¯ã„å‡ºç™ºç‚¹ã§ã™ã€‚ãŸã ã—ã€LLMã¯ã—ã°ã—ã°é‡å­åŒ–ã‚„ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠã‚¹ãƒ†ãƒƒãƒ—ã®ç´°ã‹ã„åˆ¶å¾¡ãªã©ã®é«˜åº¦ãªæ©Ÿèƒ½ãŒå¿…è¦ã§ã‚ã‚Šã€ã“ã‚Œã¯ [generate()](https://huggingface.co/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate) ã‚’ä»‹ã—ã¦æœ€è‰¯ã«è¡Œã‚ã‚Œã¾ã™ã€‚LLMã¨ã®è‡ªå·±å›å¸°ç”Ÿæˆã¯ãƒªã‚½ãƒ¼ã‚¹ãŒå¤šãå¿…è¦ã§ã‚ã‚Šã€é©åˆ‡ãªã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ãŸã‚ã«GPUã§å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "<!-- TODO: llama 2ï¼ˆã¾ãŸã¯ã‚ˆã‚Šæ–°ã—ã„ä¸€èˆ¬çš„ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰ãŒåˆ©ç”¨å¯èƒ½ã«ãªã£ãŸã‚‰ã€ä¾‹ã‚’æ›´æ–°ã™ã‚‹ -->\n",
    "ã¾ãšã€ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"openlm-research/open_llama_7b\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_pretrained` å‘¼ã³å‡ºã—ã§2ã¤ã®ãƒ•ãƒ©ã‚°ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "- `device_map` ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ãªãŸã®GPUã«ç§»å‹•ã•ã›ã¾ã™\n",
    "- `load_in_4bit` ã¯[4ãƒ“ãƒƒãƒˆã®å‹•çš„é‡å­åŒ–](https://huggingface.co/docs/transformers/main/ja/main_classes/quantization)ã‚’é©ç”¨ã—ã¦ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹ä»–ã®æ–¹æ³•ã‚‚ã‚ã‚Šã¾ã™ãŒã€ã“ã‚Œã¯LLMã‚’å§‹ã‚ã‚‹ãŸã‚ã®è‰¯ã„åŸºæº–ã§ã™ã€‚\n",
    "\n",
    "æ¬¡ã«ã€[ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶](https://huggingface.co/docs/transformers/main/ja/tokenizer_summary)ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚’å‰å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\")\n",
    "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_inputs` å¤‰æ•°ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’ä¿æŒã—ã¦ã„ã¾ã™ã€‚ [generate()](https://huggingface.co/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate) ã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ãŒæ¸¡ã•ã‚Œã¦ã„ãªã„å ´åˆã§ã‚‚ã€æœ€å–„ã®åŠªåŠ›ã‚’ã—ã¦ãã‚Œã‚’æ¨æ¸¬ã—ã‚ˆã†ã¨ã—ã¾ã™ãŒã€ã§ãã‚‹é™ã‚Šæ¸¡ã™ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚æœ€é©ãªçµæœã‚’å¾—ã‚‹ãŸã‚ã§ã™ã€‚\n",
    "\n",
    "æœ€å¾Œã«ã€[generate()](https://huggingface.co/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate) ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã€ãã‚Œã‚’è¡¨ç¤ºã™ã‚‹å‰ã«ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A list of colors: red, blue, green, yellow, black, white, and brown'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§å®Œäº†ã§ã™ï¼ã‚ãšã‹ãªã‚³ãƒ¼ãƒ‰è¡Œæ•°ã§ã€LLMï¼ˆLarge Language Modelï¼‰ã®ãƒ‘ãƒ¯ãƒ¼ã‚’æ´»ç”¨ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ç”Ÿæˆæˆ¦ç•¥](https://huggingface.co/docs/transformers/main/ja/generation_strategies)ã¯ãŸãã•ã‚“ã‚ã‚Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å€¤ãŒã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«é©ã—ã¦ã„ãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚å‡ºåŠ›ãŒæœŸå¾…é€šã‚Šã§ãªã„å ´åˆã€æœ€ã‚‚ä¸€èˆ¬çš„ãªè½ã¨ã—ç©´ã¨ãã®å›é¿æ–¹æ³•ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"openlm-research/open_llama_7b\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated output is too short/long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GenerationConfig](https://huggingface.co/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationConfig) ãƒ•ã‚¡ã‚¤ãƒ«ã§æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€`generate` ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ€å¤§ã§ 20 ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§è¿”ã—ã¾ã™ã€‚æˆ‘ã€…ã¯ `generate` ã‚³ãƒ¼ãƒ«ã§ `max_new_tokens` ã‚’æ‰‹å‹•ã§è¨­å®šã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¿”ã•ã‚Œã‚‹æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€å¤§æ•°ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚LLMï¼ˆæ­£ç¢ºã«ã¯ã€[ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å°‚ç”¨ãƒ¢ãƒ‡ãƒ«](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)ï¼‰ã‚‚å‡ºåŠ›ã®ä¸€éƒ¨ã¨ã—ã¦å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿”ã™ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# By default, the output will contain up to 20 tokens\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting `max_new_tokens` allows you to control the maximum length\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect generation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ [GenerationConfig](https://huggingface.co/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationConfig) ãƒ•ã‚¡ã‚¤ãƒ«ã§æŒ‡å®šã•ã‚Œã¦ã„ãªã„é™ã‚Šã€`generate` ã¯å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠã—ã¾ã™ï¼ˆè²ªæ¬²ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰ã€‚ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦ã€ã“ã‚Œã¯æœ›ã¾ã—ããªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚„ã‚¨ãƒƒã‚»ã‚¤ã®ã‚ˆã†ãªå‰µé€ çš„ãªã‚¿ã‚¹ã‚¯ã§ã¯ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒæœ‰ç›Šã§ã™ã€‚ä¸€æ–¹ã€éŸ³å£°ã®è»¢å†™ã‚„ç¿»è¨³ã®ã‚ˆã†ãªå…¥åŠ›ã«åŸºã¥ãã‚¿ã‚¹ã‚¯ã§ã¯ã€è²ªæ¬²ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒæœ‰ç›Šã§ã™ã€‚`do_sample=True` ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã§ãã¾ã™ã€‚ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã®è©³ç´°ã¯ã€ã“ã®[ãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆ](https://huggingface.co/blog/how-to-generate)ã§å­¦ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat. I am a cat. I am a cat. I am a cat'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed or reproducibility -- you don't need this unless you want full reproducibility\n",
    "from transformers import set_seed\n",
    "set_seed(0)\n",
    "\n",
    "model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# LLM + greedy decoding = repetitive, boring output\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat.\\nI just need to be. I am always.\\nEvery time'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With sampling, the output becomes more creative!\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong padding side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMï¼ˆLarge Language Modelsï¼‰ã¯[ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å°‚ç”¨](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚Šã€å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚å…¥åŠ›ãŒåŒã˜é•·ã•ã§ãªã„å ´åˆã€ãã‚Œã‚‰ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚LLMã¯ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ã®ç¶šãã‚’å­¦ç¿’ã—ã¦ã„ãªã„ãŸã‚ã€å…¥åŠ›ã¯å·¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€ç”Ÿæˆã«å¯¾ã—ã¦æ³¨ç›®ãƒã‚¹ã‚¯ã‚’æ¸¡ã—å¿˜ã‚Œãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer initialized above has right-padding active by default: the 1st sequence,\n",
    "# which is shorter, has padding on the right side. Generation fails.\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids[0], skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 3, 4, 5, 6,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With left-padding, it works as expected!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚ªãƒ¼ãƒˆãƒªã‚°ãƒ¬ãƒƒã‚·ãƒ–ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã¯æ¯”è¼ƒçš„ç°¡å˜ã§ã™ãŒã€LLMã‚’æœ€å¤§é™ã«æ´»ç”¨ã™ã‚‹ã“ã¨ã¯å¤šãã®è¦ç´ ãŒçµ¡ã‚€ãŸã‚ã€æŒ‘æˆ¦çš„ãªè©¦ã¿ã¨ãªã‚Šã¾ã™ã€‚LLMã®ä½¿ç”¨ã¨ç†è§£ã‚’ã•ã‚‰ã«æ·±ã‚ã‚‹ãŸã‚ã®æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«ã¤ã„ã¦ã¯ä»¥ä¸‹ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
    "\n",
    "<!-- TODO: æ–°ã—ã„ã‚¬ã‚¤ãƒ‰ã§å®Œäº† -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced generate usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ã‚¬ã‚¤ãƒ‰](https://huggingface.co/docs/transformers/main/ja/generation_strategies)ï¼šç•°ãªã‚‹ç”Ÿæˆæ–¹æ³•ã‚’åˆ¶å¾¡ã™ã‚‹æ–¹æ³•ã€ç”Ÿæˆæ§‹æˆãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®šæ–¹æ³•ã€å‡ºåŠ›ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ–¹æ³•ã«ã¤ã„ã¦ã®ã‚¬ã‚¤ãƒ‰;\n",
    "2. [GenerationConfig](https://huggingface.co/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationConfig)ã€[generate()](https://huggingface.co/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate)ã€ãŠã‚ˆã³[ç”Ÿæˆé–¢é€£ã‚¯ãƒ©ã‚¹](https://huggingface.co/docs/transformers/main/ja/internal/generation_utils)ã«é–¢ã™ã‚‹APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM leaderboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Open LLM ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ï¼šã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã«ç„¦ç‚¹ã‚’å½“ã¦ãŸãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰;\n",
    "2. [Open LLM-Perf ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)ï¼šLLMã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã«ç„¦ç‚¹ã‚’å½“ã¦ãŸãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency and throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ã‚¬ã‚¤ãƒ‰](https://huggingface.co/docs/transformers/main/ja/main_classes/quantization)ï¼šãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ã‚¯ã‚ªãƒ³ã‚¿ã‚¤ã‚ºã«é–¢ã™ã‚‹ã‚¬ã‚¤ãƒ‰ã€‚ã“ã‚Œã«ã‚ˆã‚Šãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’åŠ‡çš„ã«å‰Šæ¸›ã™ã‚‹æ–¹æ³•ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`text-generation-inference`](https://github.com/huggingface/text-generation-inference)ï¼šLLMç”¨ã®æœ¬ç•ªå‘ã‘ã‚µãƒ¼ãƒãƒ¼;\n",
    "2. [`optimum`](https://github.com/huggingface/optimum)ï¼šç‰¹å®šã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ‡ãƒã‚¤ã‚¹å‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸğŸ¤— Transformersã®æ‹¡å¼µã€‚"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
