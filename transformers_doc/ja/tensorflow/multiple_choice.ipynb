{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤šè‚¢é¸æŠã‚¿ã‚¹ã‚¯ã¯è³ªå•å¿œç­”ã«ä¼¼ã¦ã„ã¾ã™ãŒã€ã„ãã¤ã‹ã®å€™è£œã®å›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã¨ã‚‚ã«æä¾›ã•ã‚Œã€æ­£ã—ã„å›ç­”ã‚’é¸æŠã™ã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã‚‹ç‚¹ãŒç•°ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "1. [SWAG](https://huggingface.co/datasets/swag) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã€Œé€šå¸¸ã€æ§‹æˆã§ [BERT](https://huggingface.co/google-bert/bert-base-uncased) ã‚’å¾®èª¿æ•´ã—ã¦ã€æœ€é©ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é¸æŠã—ã¾ã™è¤‡æ•°ã®é¸æŠè‚¢ã¨ä½•ã‚‰ã‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ã¦å›ç­”ã—ã¾ã™ã€‚\n",
    "2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate\n",
    "```\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SWAG dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¾ãšã€ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ SWAG ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã€Œé€šå¸¸ã€æ§‹æˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "swag = load_dataset(\"swag\", \"regular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'fold-ind': '3416',\n",
       " 'gold-source': 'gold',\n",
       " 'label': 0,\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'video-id': 'anetv_jkn6uvmqwh4'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swag[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã“ã«ã¯ãŸãã•ã‚“ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ãŒã€å®Ÿéš›ã¯éå¸¸ã«ç°¡å˜ã§ã™ã€‚\n",
    "\n",
    "- `sent1` ã¨ `sent2`: ã“ã‚Œã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯æ–‡ã®å§‹ã¾ã‚Šã‚’ç¤ºã—ã€ã“ã® 2 ã¤ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨ `startphrase` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚\n",
    "- `ending`: æ–‡ã®çµ‚ã‚ã‚Šæ–¹ã¨ã—ã¦è€ƒãˆã‚‰ã‚Œã‚‹çµ‚ã‚ã‚Šæ–¹ã‚’ç¤ºå”†ã—ã¾ã™ãŒã€æ­£ã—ã„ã®ã¯ 1 ã¤ã ã‘ã§ã™ã€‚\n",
    "- `label`: æ­£ã—ã„æ–‡ã®çµ‚ã‚ã‚Šã‚’è­˜åˆ¥ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€BERT ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€æ–‡ã®å§‹ã¾ã‚Šã¨ 4 ã¤ã®å¯èƒ½ãªçµ‚ã‚ã‚Šã‚’å‡¦ç†ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½œæˆã™ã‚‹å‰å‡¦ç†é–¢æ•°ã¯æ¬¡ã®ã“ã¨ã‚’è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "1. `sent1` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã‚³ãƒ”ãƒ¼ã‚’ 4 ã¤ä½œæˆã—ã€ãã‚Œãã‚Œã‚’ `sent2` ã¨çµ„ã¿åˆã‚ã›ã¦æ–‡ã®å§‹ã¾ã‚Šã‚’å†ç¾ã—ã¾ã™ã€‚\n",
    "2. `sent2` ã‚’ 4 ã¤ã®å¯èƒ½ãªæ–‡æœ«å°¾ã®ãã‚Œãã‚Œã¨çµ„ã¿åˆã‚ã›ã¾ã™ã€‚\n",
    "3. ã“ã‚Œã‚‰ 2 ã¤ã®ãƒªã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã§ãã‚‹ã‚ˆã†ã«ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã€ãã®å¾Œã€å„ä¾‹ã«å¯¾å¿œã™ã‚‹ `input_ids`ã€`attention_mask`ã€ãŠã‚ˆã³ `labels` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«éãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n",
    "    question_headers = examples[\"sent2\"]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å‰å‡¦ç†é–¢æ•°ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets `map` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ `batched=True` ã‚’è¨­å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¤‡æ•°ã®è¦ç´ ã‚’ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€`map` é–¢æ•°ã‚’é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_swag = swag.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataCollatorForMultipleChoice` ã¯ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã‚’å¹³å¦åŒ–ã—ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨ã—ã¦ã€çµæœã‚’éå¹³å¦åŒ–ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForMultipleChoice\n",
    "collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚ã‚‹ã¨ã€å¤šãã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€è©•ä¾¡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã™ã°ã‚„ããƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™ (ğŸ¤— Evaluate [ã‚¯ã‚¤ãƒƒã‚¯ ãƒ„ã‚¢ãƒ¼](https://huggingface.co/docs/evaluate/a_quick_tour) ã‚’å‚ç…§ã—ã¦ãã ã•ã„) ) ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã¨è¨ˆç®—æ–¹æ³•ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€æ¬¡ã‚’å‚ç…§ã—ã¦ãã ã•ã„)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€äºˆæ¸¬ã¨ãƒ©ãƒ™ãƒ«ã‚’ `compute` ã«æ¸¡ã—ã¦ç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§`compute_metrics`é–¢æ•°ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ãã«ã“ã®é–¢æ•°ã«æˆ»ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](https://huggingface.co/docs/transformers/main/ja/tasks/../training#train-a-tensorflow-model-with-keras) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
    "\n",
    "</Tip>\n",
    "TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_train_epochs = 2\n",
    "total_train_steps = (len(tokenized_swag[\"train\"]) // batch_size) * num_train_epochs\n",
    "optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€[TFAutoModelForMultipleChoice](https://huggingface.co/docs/transformers/main/ja/model_doc/auto#transformers.TFAutoModelForMultipleChoice) ã‚’ä½¿ç”¨ã—ã¦ BERT ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForMultipleChoice\n",
    "\n",
    "model = TFAutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[prepare_tf_dataset()](https://huggingface.co/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset) ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_swag[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_swag[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)  # No loss argument!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹å‰ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹æœ€å¾Œã® 2 ã¤ã®ã“ã¨ã¯ã€äºˆæ¸¬ã‹ã‚‰ç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã¨ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹æ–¹æ³•ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚ã©ã¡ã‚‰ã‚‚ [Keras ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯](https://huggingface.co/docs/transformers/main/ja/tasks/../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¦è¡Œã‚ã‚Œã¾ã™ã€‚\n",
    "\n",
    "`compute_metrics` é–¢æ•°ã‚’ [KerasMetricCallback](https://huggingface.co/docs/transformers/main/ja/main_classes/keras_callbacks#transformers.KerasMetricCallback) ã«æ¸¡ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PushToHubCallback](https://huggingface.co/docs/transformers/main/ja/main_classes/keras_callbacks#transformers.PushToHubCallback) ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´æ‰€ã‚’æŒ‡å®šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã¾ã¨ã‚ã¦ãƒãƒ³ãƒ‰ãƒ«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "\n",
    "\n",
    "<Tip>\n",
    "\n",
    "è¤‡æ•°é¸æŠç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã®è©³ç´°ãªä¾‹ã«ã¤ã„ã¦ã¯ã€å¯¾å¿œã™ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "[PyTorch ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\n",
    "ã¾ãŸã¯ [TensorFlow ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)ã€‚\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "\n",
    "ã„ãã¤ã‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ 2 ã¤ã®å›ç­”å€™è£œã‚’è€ƒãˆã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"France has a bread law, Le DÃ©cret Pain, with strict rules on what is allowed in a traditional baguette.\"\n",
    "candidate1 = \"The law does not apply to croissants and brioche.\"\n",
    "candidate2 = \"The law applies to baguettes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å›ç­”å€™è£œã®ãƒšã‚¢ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_swag_model\")\n",
    "inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"tf\", padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForMultipleChoice\n",
    "\n",
    "model = TFAutoModelForMultipleChoice.from_pretrained(\"my_awesome_swag_model\")\n",
    "inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}\n",
    "outputs = model(inputs)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€ã‚‚é«˜ã„ç¢ºç‡ã§ã‚¯ãƒ©ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "predicted_class"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
