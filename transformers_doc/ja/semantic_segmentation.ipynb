{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dKE8SIt9C-w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dKE8SIt9C-w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ç”»åƒã®å€‹ã€…ã®ãƒ”ã‚¯ã‚»ãƒ«ã«ãƒ©ãƒ™ãƒ«ã¾ãŸã¯ã‚¯ãƒ©ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ã„ãã¤ã‹ã®ã‚¿ã‚¤ãƒ—ãŒã‚ã‚Šã¾ã™ãŒã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®å ´åˆã€åŒã˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€æ„ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹é–“ã®åŒºåˆ¥ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚ä¸¡æ–¹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«åŒã˜ãƒ©ãƒ™ãƒ«ãŒä»˜ã‘ã‚‰ã‚Œã¾ã™ (ãŸã¨ãˆã°ã€`car-1`ã¨`car-2`ã®ä»£ã‚ã‚Šã«`car`)ã€‚ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸€èˆ¬çš„ãªç¾å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ã€æ­©è¡Œè€…ã‚„é‡è¦ãªäº¤é€šæƒ…å ±ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã®è‡ªå‹•é‹è»¢è»Šã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€åŒ»ç™‚ç”»åƒå†…ã®ç´°èƒã¨ç•°å¸¸ã®è­˜åˆ¥ã€è¡›æ˜Ÿç”»åƒã‹ã‚‰ã®ç’°å¢ƒå¤‰åŒ–ã®ç›£è¦–ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "1. [SceneParse150](https://huggingface.co/datasets/scene_parse_150) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n",
    "2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/image-segmentation) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "```bash\n",
    "pip install -q datasets transformers evaluate\n",
    "```\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SceneParse150 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¾ãšã€SceneParse150 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_test_split` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `train` åˆ†å‰²ã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,\n",
       " 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,\n",
       " 'scene_category': 368}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `image`: ã‚·ãƒ¼ãƒ³ã® PIL ã‚¤ãƒ¡ãƒ¼ã‚¸ã€‚\n",
    "- `annotation`: ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒãƒƒãƒ—ã® PIL ã‚¤ãƒ¡ãƒ¼ã‚¸ã€‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ã‚‚ã‚ã‚Šã¾ã™ã€‚\n",
    "- `scene_category`: \"kitchen\"ã‚„\"office\"ãªã©ã®ç”»åƒã‚·ãƒ¼ãƒ³ã‚’èª¬æ˜ã™ã‚‹ã‚«ãƒ†ã‚´ãƒª IDã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€`image`ã¨`annotation`ã®ã¿ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚ã©ã¡ã‚‰ã‚‚ PIL ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚\n",
    "\n",
    "ã¾ãŸã€ãƒ©ãƒ™ãƒ« ID ã‚’ãƒ©ãƒ™ãƒ« ã‚¯ãƒ©ã‚¹ã«ãƒãƒƒãƒ—ã™ã‚‹è¾æ›¸ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€å¾Œã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã™ã‚‹ã¨ãã«å½¹ç«‹ã¡ã¾ã™ã€‚ãƒãƒ–ã‹ã‚‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€`id2label` ãŠã‚ˆã³ `label2id` ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€SegFormer ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã¨æ³¨é‡ˆã‚’æº–å‚™ã—ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚ˆã†ãªä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ ã‚¯ãƒ©ã‚¹ã¨ã—ã¦ã‚¼ãƒ­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãŸã ã—ã€å®Ÿéš›ã«ã¯èƒŒæ™¯ã‚¯ãƒ©ã‚¹ã¯ 150 å€‹ã®ã‚¯ãƒ©ã‚¹ã«å«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€`do_reduce_labels=True`ã‚’è¨­å®šã—ã¦ã™ã¹ã¦ã®ãƒ©ãƒ™ãƒ«ã‹ã‚‰ 1 ã¤ã‚’å¼•ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚¼ãƒ­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ `255` ã«ç½®ãæ›ãˆã‚‰ã‚Œã‚‹ãŸã‚ã€SegFormer ã®æå¤±é–¢æ•°ã«ã‚ˆã£ã¦ç„¡è¦–ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"nvidia/mit-b0\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã‚’éå­¦ç¿’ã«å¯¾ã—ã¦ã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹ãŸã‚ã«ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é©ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€[torchvision](https://pytorch.org/vision/stable/index.html) ã® [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ ) ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã®è‰²ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å¤‰æ›´ã—ã¾ã™ãŒã€ä»»æ„ã®ç”»åƒãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ColorJitter\n",
    "\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã¨æ³¨é‡ˆã‚’æº–å‚™ã™ã‚‹ãŸã‚ã® 2 ã¤ã®å‰å‡¦ç†é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®é–¢æ•°ã¯ã€ç”»åƒã‚’`pixel_values`ã«å¤‰æ›ã—ã€æ³¨é‡ˆã‚’`labels`ã«å¤‰æ›ã—ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã®å ´åˆã€ç”»åƒã‚’ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã«æä¾›ã™ã‚‹å‰ã« `jitter` ãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã®å ´åˆã€ãƒ†ã‚¹ãƒˆä¸­ã«ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãŒé©ç”¨ã•ã‚Œãªã„ãŸã‚ã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã¯`images`ã‚’åˆ‡ã‚Šå–ã£ã¦æ­£è¦åŒ–ã—ã€`ãƒ©ãƒ™ãƒ«`ã®ã¿ã‚’åˆ‡ã‚Šå–ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«`jitter`ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets `set_transform` é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚å¤‰æ›ã¯ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§é©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€é«˜é€Ÿã§æ¶ˆè²»ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒå°‘ãªããªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚ã‚‹ã¨ã€å¤šãã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€è©•ä¾¡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã™ã°ã‚„ããƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€[Mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ (ğŸ¤— Evaluate [ã‚¯ã‚¤ãƒƒã‚¯ ãƒ„ã‚¢ãƒ¼](https://huggingface.co/docs/evaluate/a_quick_tour) ã‚’å‚ç…§ã—ã¦ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è¨ˆç®—ã™ã‚‹æ–¹æ³•ã®è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ `compute` ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚äºˆæ¸¬ã‚’æ¬¡ã®ã‚ˆã†ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n",
    "æœ€åˆã«ãƒ­ã‚¸ãƒƒãƒˆã‚’ä½œæˆã—ã€æ¬¡ã« `compute` ã‚’å‘¼ã³å‡ºã™å‰ã«ãƒ©ãƒ™ãƒ«ã®ã‚µã‚¤ã‚ºã«ä¸€è‡´ã™ã‚‹ã‚ˆã†ã«å†å½¢æˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§`compute_metrics`é–¢æ•°ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ãã«ã“ã®é–¢æ•°ã«æˆ»ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "[Trainer](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer) ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã“](https://huggingface.co/docs/transformers/main/ja/tasks/../training#finetune-with-trainer) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ [AutoModelForSemanticSegmentation](https://huggingface.co/docs/transformers/main/ja/model_doc/auto#transformers.AutoModelForSemanticSegmentation) ã‚’ä½¿ç”¨ã—ã¦ SegFormer ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ©ãƒ™ãƒ« ID ã¨ãƒ©ãƒ™ãƒ« ã‚¯ãƒ©ã‚¹é–“ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã®æ™‚ç‚¹ã§æ®‹ã£ã¦ã„ã‚‹æ‰‹é †ã¯æ¬¡ã® 3 ã¤ã ã‘ã§ã™ã€‚\n",
    "\n",
    "1. [TrainingArguments](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.TrainingArguments) ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚ `image` åˆ—ãŒå‰Šé™¤ã•ã‚Œã‚‹ãŸã‚ã€æœªä½¿ç”¨ã®åˆ—ã‚’å‰Šé™¤ã—ãªã„ã“ã¨ãŒé‡è¦ã§ã™ã€‚ `image` åˆ—ãŒãªã„ã¨ã€`pixel_values` ã‚’ä½œæˆã§ãã¾ã›ã‚“ã€‚ã“ã®å‹•ä½œã‚’é˜²ãã«ã¯ã€`remove_unused_columns=False`ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚ä»–ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ `output_dir` ã ã‘ã§ã™ã€‚ `push_to_hub=True`ã‚’è¨­å®šã—ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ (ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€Hugging Face ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚äº†æ™‚ã«ã€[Trainer](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer) ã¯ IoU ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’è©•ä¾¡ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚\n",
    "2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã€ãŠã‚ˆã³ `compute_metrics` é–¢æ•°ã¨ã¨ã‚‚ã« [Trainer](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer) ã«æ¸¡ã—ã¾ã™ã€‚\n",
    "3. [train()](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.train) ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segformer-b0-scene-parse-150\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [push_to_hub()](https://huggingface.co/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.push_to_hub) ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "\n",
    "æ¨è«–ã®ãŸã‚ã«ç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ds[0][\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png\" alt=\"Image of bedroom\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [pipeline()](https://huggingface.co/docs/transformers/main/ja/main_classes/pipelines#transformers.pipeline) ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã® `pipeline`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã«ç”»åƒã‚’æ¸¡ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': None,\n",
       "  'label': 'wall',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},\n",
       " {'score': None,\n",
       "  'label': 'sky',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},\n",
       " {'score': None,\n",
       "  'label': 'floor',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},\n",
       " {'score': None,\n",
       "  'label': 'ceiling',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},\n",
       " {'score': None,\n",
       "  'label': 'bed ',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},\n",
       " {'score': None,\n",
       "  'label': 'windowpane',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},\n",
       " {'score': None,\n",
       "  'label': 'cabinet',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},\n",
       " {'score': None,\n",
       "  'label': 'chair',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},\n",
       " {'score': None,\n",
       "  'label': 'armchair',\n",
       "  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "segmenter = pipeline(\"image-segmentation\", model=\"my_awesome_seg_model\")\n",
    "segmenter(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¿…è¦ã«å¿œã˜ã¦ã€`pipeline`ã®çµæœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ç”»åƒã‚’ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã§å‡¦ç†ã—ã€`pixel_values` ã‚’ GPU ã«é…ç½®ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # use GPU if available, otherwise use a CPU\n",
    "encoding = image_processor(image, return_tensors=\"pt\")\n",
    "pixel_values = encoding.pixel_values.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(pixel_values=pixel_values)\n",
    "logits = outputs.logits.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«å†ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_logits = nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "pred_seg = upsampled_logits.argmax(dim=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çµæœã‚’è¦–è¦šåŒ–ã™ã‚‹ã«ã¯ã€[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚«ãƒ©ãƒ¼ ãƒ‘ãƒ¬ãƒƒãƒˆ](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) ã‚’ã€ãã‚Œãã‚Œã‚’ãƒãƒƒãƒ—ã™ã‚‹ `ade_palette()` ã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚’ RGB å€¤ã«å¤‰æ›ã—ã¾ã™ã€‚æ¬¡ã«ã€ç”»åƒã¨äºˆæ¸¬ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒãƒƒãƒ—ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ­ãƒƒãƒˆã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[pred_seg == label, :] = color\n",
    "color_seg = color_seg[..., ::-1]  # convert to BGR\n",
    "\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png\" alt=\"Image of bedroom overlaid with segmentation map\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
