{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-to-Image Task Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-to-Image ã‚¿ã‚¹ã‚¯ã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒç”»åƒã‚’å—ä¿¡ã—ã€åˆ¥ã®ç”»åƒã‚’å‡ºåŠ›ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ã“ã‚Œã«ã¯ã€ç”»åƒå¼·åŒ– (è¶…è§£åƒåº¦ã€ä½å…‰é‡å¼·åŒ–ã€ãƒ‡ã‚£ãƒ¬ã‚¤ãƒ³ãªã©)ã€ç”»åƒä¿®å¾©ãªã©ã‚’å«ã‚€ã•ã¾ã–ã¾ãªã‚µãƒ–ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
    "- è¶…è§£åƒåº¦ã‚¿ã‚¹ã‚¯ã«ç”»åƒé–“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã›ãšã«ã€åŒã˜ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã‚¤ãƒ¡ãƒ¼ã‚¸é–“ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸæ™‚ç‚¹ã§ã¯ã€`image-to-image`ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯è¶…è§£åƒåº¦ã‚¿ã‚¹ã‚¯ã®ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    "```bash\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "[Swin2SR ãƒ¢ãƒ‡ãƒ«](https://huggingface.co/caidas/swin2SR-lightweight-x2-64) ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚æ¬¡ã«ã€ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ¨è«–ã§ãã¾ã™ã€‚ç¾æ™‚ç‚¹ã§ã¯ã€[Swin2SR ãƒ¢ãƒ‡ãƒ«](https://huggingface.co/models?sort=trending&search=swin2sr) ã®ã¿ãŒã“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pipe = pipeline(task=\"image-to-image\", model=\"caidas/swin2SR-lightweight-x2-64\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã§ã¯ã€ç”»åƒã‚’èª­ã¿è¾¼ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "print(image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# (532, 432)\n",
    "```\n",
    "<div class=\"flex justify-center\">\n",
    "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat.jpg\" alt=\"Photo of a cat\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "ã“ã‚Œã§ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚çŒ«ã®ç”»åƒã®æ‹¡å¤§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscaled = pipe(image)\n",
    "print(upscaled.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# (1072, 880)\n",
    "```\n",
    "\n",
    "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã›ãšã«è‡ªåˆ†ã§æ¨è«–ã‚’å®Ÿè¡Œã—ãŸã„å ´åˆã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã® `Swin2SRForImageSuperResolution` ã‚¯ãƒ©ã‚¹ã¨ `Swin2SRImageProcessor` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ã“ã‚Œã«ã¯åŒã˜ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’åˆæœŸåŒ–ã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor \n",
    "\n",
    "model = Swin2SRForImageSuperResolution.from_pretrained(\"caidas/swin2SR-lightweight-x2-64\").to(device)\n",
    "processor = Swin2SRImageProcessor(\"caidas/swin2SR-lightweight-x2-64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline`ã€ã¯ã€è‡ªåˆ†ã§è¡Œã†å¿…è¦ãŒã‚ã‚‹å‰å‡¦ç†ã¨å¾Œå‡¦ç†ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æŠ½è±¡åŒ–ã™ã‚‹ã®ã§ã€ç”»åƒã‚’å‰å‡¦ç†ã—ã¾ã—ã‚‡ã†ã€‚ç”»åƒã‚’ãƒ—ãƒ­ã‚»ãƒƒã‚µã«æ¸¡ã—ã¦ã‹ã‚‰ã€ãƒ”ã‚¯ã‚»ãƒ«å€¤ã‚’ GPU ã«ç§»å‹•ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "print(pixel_values.shape)\n",
    "\n",
    "pixel_values = pixel_values.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§ã€ãƒ”ã‚¯ã‚»ãƒ«å€¤ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ã“ã¨ã§ç”»åƒã‚’æ¨æ¸¬ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‡ºåŠ›ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãª `ImageSuperResolutionOutput` ã‚¿ã‚¤ãƒ—ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ ğŸ‘‡\n",
    "\n",
    "```\n",
    "(loss=None, reconstruction=tensor([[[[0.8270, 0.8269, 0.8275,  ..., 0.7463, 0.7446, 0.7453],\n",
    "          [0.8287, 0.8278, 0.8283,  ..., 0.7451, 0.7448, 0.7457],\n",
    "          [0.8280, 0.8273, 0.8269,  ..., 0.7447, 0.7446, 0.7452],\n",
    "          ...,\n",
    "          [0.5923, 0.5933, 0.5924,  ..., 0.0697, 0.0695, 0.0706],\n",
    "          [0.5926, 0.5932, 0.5926,  ..., 0.0673, 0.0687, 0.0705],\n",
    "          [0.5927, 0.5914, 0.5922,  ..., 0.0664, 0.0694, 0.0718]]]],\n",
    "       device='cuda:0'), hidden_states=None, attentions=None)\n",
    "```\n",
    "\n",
    "`reconstruction`ã‚’å–å¾—ã—ã€ãã‚Œã‚’è¦–è¦šåŒ–ã™ã‚‹ãŸã‚ã«å¾Œå‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã©ã®ã‚ˆã†ã«è¦‹ãˆã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.reconstruction.data.shape\n",
    "# torch.Size([1, 3, 880, 1072])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‡ºåŠ›ã‚’åœ§ç¸®ã—ã¦è»¸ 0 ã‚’å‰Šé™¤ã—ã€å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—ã—ã¦ã‹ã‚‰ã€ãã‚Œã‚’ numpy float ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æ¬¡ã«ã€è»¸ã‚’ [1072, 880] ã®å½¢çŠ¶ã«ãªã‚‹ã‚ˆã†ã«é…ç½®ã—ã€æœ€å¾Œã«å‡ºåŠ›ã‚’ç¯„å›² [0, 255] ã«æˆ»ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# squeeze, take to CPU and clip the values\n",
    "output = outputs.reconstruction.data.squeeze().cpu().clamp_(0, 1).numpy()\n",
    "# rearrange the axes\n",
    "output = np.moveaxis(output, source=0, destination=-1)\n",
    "# bring values back to pixel values range\n",
    "output = (output * 255.0).round().astype(np.uint8)\n",
    "Image.fromarray(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"flex justify-center\">\n",
    "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat_upscaled.png\" alt=\"Upscaled photo of a cat\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
